[{"pageNumber":1,"pageContent":"ARTICLE .23 Historical Simulations – Motivational, Ethical and Legal Issues Peter S. Jenkins* Attorney at Law USA Introduction The notion that the perceived world is an illusion or a simulation has arisen for centuries in the works of philosophers 1 , mathematicians 2 , and social scientists. 3 A recent variant on this theme, posited by Nick Bostrom of the University of Oxford, is that it is possible that we are forms of artificial intelligence in an ancestor, (i.e. historical) simulation created by a future society. 4 Moore's Law, which has held true for about 40 years, states that computer processing power doubles approximately Journal of Futures Studies, August 2006, 11(1): 23 - 42 Abstract A future society will very likely have the technological ability and the motivation to create large numbers of completely realistic historical simulations and be able to overcome any ethical and legal obstacles to doing so. It is thus highly probable that we are a form of artificial intelligence inhabiting one of these simulations. To avoid stacking (i.e. simulations within simulations), the termination of these simulations is likely to be the point in history when the technology to create them first became widely available, (estimated to be 2050). Long range planning beyond this date would therefore be futile. Key Words: historical simulation, virtual world, artificial intelligence, massively multiple online role- playing games, ethics of clinical research, Moore's Law. * Member of the State Bar of California. He would like to thank John Smart for his helpful comments at the very early stages of writing this paper, Greg Lastowka and some anonymous referees for some insights when it was nearing completion and Edward Castronova for his encouragement."},{"pageNumber":2,"pageContent":"Journal of Futures Studies 24 every 2 years. The prominent futur- ist, Ray Kurzweil, estimates that this is currently accelerating to doubling every year and that by approximate- ly the year 2050, for the then equiv- alent of $1,000, you will be able to purchase a computer with greater processing power than that of all the brains of all humans that have ever lived 5 . This means that by 2050 it would be feasible to have a com- pletely realistic historical simulation running on every desktop, and that these simulated worlds would out- number the real one by a factor of millions or even billions to one. This makes it almost certain that we live in one of the simulations if a future society has the motivation to create them and does not encounter any insurmountable ethical and legal obstacles to doing so. Bostrom's view is that a future society would likely not have the motivation to create these simula- tions or alternatively may not be able to overcome the ethical and legal impediments involved 6 . The purpose of this paper is to demon- strate that there is a high probability that a future society will have strong and extensive motivation to create these simulations, and will be able to overcome any ethical and legal impediments 7 . 2050 is only 44 years away, and the development of ethi- cal and legal systems tends to lag significantly behind technological progress 8 . Therefore, the same types of ethical and legal issues, as well as motivational ones, that we face today will likely be similar to the ones providing a framework for a future society's deliberations about implementing their ability to create and run historical simulations. Motivational Issues W ould future societies likely not have any motivation to create and run historical simulations, either because they have more efficient ways of amusing themselves (e.g. by directly stimulating the pleasure centers of the brain) or they deem the creation of these types of simu- lations to be frivolous and of no sci- entific, research or other practical value? 9 1. Nostalgia and the Rear View Mirror Effect The skeptical view is that, although many members of present day soci- ety would probably wish to create historical simulations if they were capable of doing so, members of a future society would be significantly different in this regard 10 . However, although this might be true in a future society whose members have fully evolved into machine form, it would likely not be true for a future society that can make historical sim- ulations but whose members have not yet fully shed their biological form. In an era of rapid change in the future, it is very probable that there may be a high degree of nos- talgia and interest in the past in the same manner that neo-classicism arose in Europe during the paradigm shift of the Industrial Revolution in the mid to late 18 th century 11 . There is also the \"rear view mir- ror\" effect noted by Marshall McLuhan, where new media technologies tend to use the content of the old media before developing content of their own, e.g. when television was first invented, the content was primarily the stage play 12 . Therefore, when a"},{"pageNumber":3,"pageContent":"Historical Simulations 25 future society develops advanced quantum computing, it is likely to first use it for historical simulations (the equivalent of stage plays on early television) prior to finding new uses and content for it, such as fine- tuning their universe themselves, or communicating with other universes. 2. Testing Ground for Artificial Intelligence (AI) Another possible motivation for the creation of historical simulations would be to use them as a safe environment in which to test newly manufactured artificially intelligent entities before releasing them into the real world. A future world could be composed entirely of artificially intelligent entities, or a combination of these entities and technologically enhanced human beings. In either case, the interaction of newly creat- ed AI units with their established AI counterparts or with technologically enhanced humans would be critical. High standards of ethical behavior would likely be expected of such newly created AI's and these stan- dards may not be susceptible of being easily programmed into these machines. Observing a simulated world in which the AI entities inhabit- ing it are not aware that they are in a simulation would be a safe, reli- able method of determining whether an AI's ethical programming requires adjustment or perhaps instead is fundamentally flawed and beyond repair. Such a simulation would be set in a historical period prior to the point when fine-grained simulation technology was first developed, oth- erwise the AI's inhabiting the simula- tion would suspect that the world they are inhabiting is not real, and their behavior would not be genuine. 3. Social and Economic Experiments As the economist Edward Castr- onova has noted in his ground- breaking book, Synthetic Worlds – the Business and Culture of Online Games , 13 these worlds, currently known as Massively Multiple Online Role-Playing Games (MMORPG's) can provide an excellent social sci- ence laboratory tool. He states that future generations of PhD students in anthropology, sociology, political science and economics will likely work with pairs of worlds (an experi- mental and a control) seeking to test various hypotheses by tweaking the parameters and observing the differ- ing results in each case. Versions or portions of online worlds such as SimCity and Second Life are cur- rently being used for policy and strategic analysis by think-tanks, universities and the U.S. Department of Defense. 14 It could be expected that this type of trend would contin- ue in a future society that is capable of producing fine-grained simula- tions that are indistinguishable from the real world. Simulated worlds cre- ated by a future society to solve pol- icy, strategic and research issues would most likely be retrospectives, i.e. historical simulations in which artificial intelligence would genuinely believe itself to be human, rather than merely playing the role of a human. These simulations could provide a rich source of information to a future society about how it arrived at its current stage of devel- opment as well as how it could avoid repeating the mistakes of the past."},{"pageNumber":4,"pageContent":"Journal of Futures Studies 26 4. Apocalypse In the event of apocalyptic events such as the release of malevolent genetically modified organisms or self-replicating nanobots, humans may want to completely download their consciousness into machines and forsake their physical bodies completely, in which case it would make sense to enhance the realism of the experience by erasing the memory of the download occur- ring in the first place as well as the horrific events that led up to it. Alternatively, emergency manage- ment or military strategists may wish to be proactive in terms of planning for apocalypse or Armageddon and create a simulation as a back-up system for civilization. It would be periodically updated but not activat- ed until the Doomsday Clock reach- es one minute before midnight. Such simulations would obviously be set in the period prior to when the disaster occurred and also before fine-grained simulations were developed. As such, they would not necessarily be historical simulations in the strict sense, but rather may be simulations of the lives of the partici- pants as they were a few short years earlier. However, they would be consistent with the notion that we live in a simulation without being aware of that fact, especially when one considers that it is very possible that malevolent self-replicating nanobots, genetically modified bio- organisms and thermonuclear war could destroy civilization within our lifetimes 15 . Ethical Issues Having determined that a future society on earth would very likely want to create historical simulations for various purposes (motivation), the next question that needs to be addressed is whether it would come to the conclusion that there is no valid reason why it should not create such simulations (ethics). A good framework for the dis- cussion of this issue can be readily found in the existing work done in the area of the ethics of medical research. The recent work of Emmanuel, Wendler and Grady 16 provides a useful summary of the generally recognized ethical require- ments of clinical research as set out in several prominent documents and codes. 17 This summary takes the form of seven requirements for the research protocol, which must: 1. have social, scientific or clini- cal value that justifies expos- ing subjects to potential harm; 2. be scientifically rigorous; 3. select subjects fairly on the basis of scientific objectives and not, for example, because of vulnerability or privilege; 4. minimize the risk to individual subjects, and have potential benefits to those subjects and/or society that outweigh or are proportionate to the risks; 5. be reviewed and approved prospectively by a committee of independent and qualified evaluators; 6. be conditioned, to the extent possible, on the voluntary and informed consent of its partici- pating subjects; and"},{"pageNumber":5,"pageContent":"Historical Simulations 27 7. ensure that enrolled subjects are shown respect, which includes protecting their priva- cy, monitoring their well- being, and providing opportu- nities to withdraw. 1. Is the historical simulation essentially just an entertainment? There is no doubt that some potential harm and pain would be inflicted on the artificial intelligences inhabiting the simulation, and that it would not be a Garden of Eden. The first of the seven requirements, i.e. that the protocol have social, scien- tific or clinical value that justifies exposing subjects to potential harm is useful in evaluating the prospects of a future society creating historical simulations for purely nostalgic and entertainment purposes. On first impression, it would seem that a simulation created for such purpos- es would have no social, scientific or clinical value, and would be purely a pleasant diversion. However, the issue warrants further examination. In her insightful book, The Future of Nostalgia 18 , Professor Svetlana Boym of Harvard University indi- cates that there are two types of nostalgia – restorative and reflec- tive. Restorative nostalgia empha- sizes the Greek nostos root, i.e. the return home as in Homer's Odyssey . It seeks to rebuild the lost home and fill in gaps in memory. Reflective nostalgia, on the other hand, comes from the algia part of the Greek root, i.e. the feeling of longing and loss due to the imperfect nature of remembrance. As Boym points out, what is needed following historical cataclysms and periods of rapid change, is not to literally recreate the monuments of home (restorative nostalgia) but rather to mourn the loss of the space of shared cultural experience within which one elected pursuits according to one's own affinities (reflective nostalgia). This mourning facilitates the existence of the space between the individual and the environment that is formed in early childhood and is integral to human nature. If a historical simulation could support the therapeutic, reflective form of nostalgia, then it would have social or even clinical value during the period of rapid upheaval as soci- ety transitions to its next stage. However, arguably a fine-grained historical simulation that recreates a past environment in every detail so that it is indistinguishable from the former reality would actually support the literal and somewhat frivolous restorative form of nostalgia rather than the therapeutic reflective type. V iewed in this light, the historical simulation would be nothing more than an über-Disneyland having entertainment but no real social value. On the other hand, it is possible that the historical simulation could be structured so as to permit reflec- tive nostalgia by creating an envi- ronment in which the loss of shared former cultural space could be prop- erly mourned, e.g. by simulating a mythical city containing actual his- torical fragments and cultural arti- facts in such a way that each per- son's experience of it would be evocative and subjective, rather than literal. Alternatively, a historical simulation where the landscape altered according to one's own inter- nal reflections, or a form of embed-"},{"pageNumber":6,"pageContent":"Journal of Futures Studies 28 ded personalized augmented reali- ty 19 overlaying the simulation would accomplish the same purpose. Therefore, even a simulation creat- ed for nostalgic purposes could have important social or clinical value. 2. Informed consent – motivation and method A significant issue in the study of the ethics of medical research is whether the social, scientific or clini- cal value of the medical protocol need accrue to the participants specifically in order to outweigh potential harms or whether, on the other hand, the value could accrue to society as a whole, or parts there- of. The recent consensus on this issue 20 seems to be that the benefits need not accrue to the participant specifically and that they can accrue to the population at large. The rationale is that a competent adult does not need to be protected from choosing to assume reasonable risks when there are important sub- jective reasons to do so that are unrelated to that individual's person- al health. Some participants in med- ical research do so, for example, to help conquer a disease that a loved one died from, in honor of their memory. They might also do so because they benefited from past medical research concerning an unrelated illness and as a gesture of gratitude they wish to help others with different illnesses. Would simi- lar sorts of altruistic motivations apply to artificial intelligence in con- nection with agreeing to participate in a historical simulation? Yes, they might apply, for example, in the case of an artificial intelligence that had personally benefited from informa- tion garnered from a previous exper- iment in which an AI had agreed to participate in a historical simulation. Such an experiment may have led to an improved form of AI of which the new proposed participant was a product. There may also be a form of inter-species reciprocal altruism 21 in the sense of AI wanting to benefit humans, e.g. by playing out various scenarios in the simulation to assist humans better understand their his- tory, while humans assist AI's evolu- tion. This may involve implicit or explicit promises by the researchers that the AI would receive future preferential treatment in terms of enhancement in programming or maintenance or other benefits in return for agreeing to participate in the historical simulation. This raises some vexing ethical concerns, since AI could be characterized as being in a class of vulnerable persons such as children, prisoners and expectant mothers for purposes of determining whether truly voluntary and informed consent had been given. As I will discuss in the next section of the paper, the general principle, as embodied in the U.S. federal regulations concerning experiments with human subjects 22 , is that vulnerable individuals such as prisoners must not be offered incentives for participating in the research that would compromise their ability to objectively evaluate the risks involved. There is also the question of the method of obtaining the consent. The premise of the historical simula- tion is that the artificial intelligences inhabiting it are not aware that they"},{"pageNumber":7,"pageContent":"Historical Simulations 29 are in such a simulation. From their perspective, they are living in the real world. How could they possibly be considered to have given informed consent generally in such a situa- tion, let alone informed consent specifically saying that they acknow- ledge that the benefits accrue not to them but to others, i.e. other AI's or the humans observing the simula- tion through their avatars? The answer is that the AI would be asked for this consent while they existed outside of the simulation and then, if they agreed, would have memories of the choice wiped clean. When asked to give consent, the AI would not be informed what sort of life it would lead in the simulation, otherwise most AI's would not give consent unless the proposed life was somehow exceptional, e.g. Albert Einstein, Elvis Presley, Queen Elizabeth etc. However, it would be necessary to inform the AI that it would be guaranteed not to be put into a life of extreme pain, suffering or hard- ship, e.g. a peasant living in grinding poverty. These sorts of roles in the simulation could feasibly be played by automated programs (\"bots\") that are not self-aware. This implies that if you are a conscious individual who is experiencing extreme dis- tress, then you probably exist in the real world of the early 21 st century rather than in a simulation run by a future society. Conversely, if you are an individual whose life generally involves happiness, freedom and self-fulfillment, then you probably exist in a simulation, (although there is one real version of you in the early 21 st century). This raises some difficult ethical questions vis-à-vis the treatment of the severely disad- vantaged by those who are better off. If the members of the latter group view members of the former as merely simulated, non-conscious entities, then they may have an incentive not to accord them the respect and dignity to which they are entitled. However, if there are any conscious AI's which have the extraordinary perceptiveness to realize that they exist in a simula- tion, they would probably also be cognizant that one of the reasons they are there is to evaluate their ethical standards prior to being allowed to leave the simulation and enter the future paradise outside it. Therefore, these AI's would not be lacking an incentive to treat the dis- advantaged bots in the simulation with proper care and concern. As Edward Castronova has pointed out 23 , in MMORPG's the human players conform to the patterns of the non-player character (NPC) bots. Similarly, in a simulation, the AI's may tailor their behavior to the needs of the disadvantaged bots to demonstrate ethical behavior. An analogy may be drawn be- tween the approach of asking the AI to consent to a random assignment to a role in the simulation (except one involving extreme suffering or hardship) and the practice of using a randomized placebo-controlled drug trial where a participant is informed that he or she will be bypassing standard medical care and there is a 50% chance that he or she may receive a placebo (i.e. a dummy pill) instead of the experimental drug. This practice is somewhat contro- versial in that some physicians are of the view that there is no scientific"},{"pageNumber":8,"pageContent":"Journal of Futures Studies 30 need for placebos in an experiment. This is in distinction to non-placebo protocols which are generally not controversial such as where the patient undergoes some risks with- out any compensating medical ben- efit, for example, agreeing to under- go a lumbar puncture procedure as part of a clinical study where the participant does not medically require such a procedure. However, there is a strong school of thought that, assuming placebos are neces- sary for the integrity of the experi- ment's results, then, if there is no serious, life-threatening risk to the patient, and all the seven require- ments listed above are met, there is no ethical difference between the placebo type of protocol and the non-placebo type, such as the lum- bar puncture 24 . In both situations, the participant gives consent after being informed that he or she will be exposed to a limited and reasonable risk possibly without any compen- sating medical benefit to him or her, in order to assist in the advance- ment of medical science. It is also significant to point out that requirement #6 as set out above states that the protocol must be conditioned on the informed con- sent of the participants, to the extent possible , in recognition of the fact that it is not always possible to pro- vide the participants with all the information concerning the experi- ment, such as for example in psy- chological experiments. A notable example is the Milgram experiment 25 designed to measure the willingness to blindly follow instructions even if they result in inflicting pain on another human being. This experi- ment involves a test administrator A asking B, who is the real subject, to assist in giving a series of progres- sively stronger electrical shocks to a \"subject\" C, who is actually a con- federate of A. C then feigns increas- ing distress as B, following instruc- tions from A, turns up the dial on the mock electrical device. The decep- tive nature of the experiment is not without its ethical critics, but it also has supporters who note that most participants were reportedly glad to have served in Milgram experiment, despite experiencing some psycho- logical distress during it. Similar reactions have been obtained from many other persons who were deceived during psychological experiments. 26 Furthermore, most of the criticism of the Milgram experi- ment relates to the total lack of informed consent, which would be distinguishable from historical simu- lations where there would be in- formed consent as indicated previ- ously. 3. Opportunities to withdraw There is one more significant ethical issue that can be unpacked from the list of seven requirements set out above. This relates to requirement #7, which is that the participants' well-being should be monitored during the experiment and that they should be given the opportunity to withdraw from the experiment. The only fair and practi- cal way to give a meaningful oppor- tunity of withdrawal for the AI would be for the creator to periodically inform it that it exists in a simulation and that, if it wishes, it can end its presence in the simulation. There"},{"pageNumber":9,"pageContent":"Historical Simulations 31 might be a penalty system to dis- courage excessive use of the \"re- start\" option, such as an increased chance of being worse-off. For those AI's that chose not to end their lives in the AI after having received the information from the creator, their memories would be wiped clean of the information and they would con- tinue to live their simulated lives, unaware that they were not human. The fact that many players of MMORPG's currently view them- selves as citizens of the synthetic world that they play in 27 and that some games have to periodically remind players after a certain amount of hours of play per week not to neglect their real world activities 28 indicates that there may be appeal for AI's in continuing to play the \"game\" of the historical simulation. 29 Although, as in MMORPG's, this process in historical simulations would tend to weed out AI's that do not have a substantial amount of curiosity, stamina and competitive- ness, this would not be much differ- ent from the process of natural selection in the real world and would therefore not significantly skew the results or interfere with the realism of the simulation. Furthermore, some recent research in medical ethics has indicated that participants in experiments should not automati- cally have an absolute or uncondi- tional right to withdraw from the experiment and that reasonable conditions can be attached to the right to withdraw, such as a require- ment to enter into a dialogue or negotiation with the researcher prior to making a decision. 30 Finally, it is interesting to note that, despite the fact that one of the basic tenets of the Islamic religion is the firm and widespread belief that martyrdom by suicide will involve instant and guaranteed transporta- tion to paradise, only an extremely small proportion (less than 5 in a million) of members of that religion actually act on that belief. This indi- cates that there is a basic human tendency, due to the endowment effect 31 , or status quo bias, to remain immersed in life despite being offered appealing exit options. It is certainly reasonable to expect that this trait would carry over to AI's in a histori- cal simulation in terms of how they would respond to an offer to with- draw, especially if the offer were to be presented to them inside the sim- ulation, and characterized as a deci- sion that is permanent and irrevoca- ble. Legal Issues 1. Would AI have the legal status of a person? The question of whether AI would have the legal status of a per- son has been considered by many lawyers, legal scholars and comput- er scientists to date, although not in the context of a historical simulation. Most of these individuals have come to the conclusion that AI would meet the definition of personhood on the basis of the having the attributes of reasoning, self-awareness, commu- nication, a sense of the past and the future, and the ability to experience pain and pleasure 32 . Of course, the absence of any one or more of these is not necessarily critical to the issue of personhood status, e.g. in the case of the profoundly retard- ed, the comatose, the brain dead,"},{"pageNumber":10,"pageContent":"Journal of Futures Studies 32 third trimester fetuses, and newly born infants. However, the fact that AI would likely possess all of the attributes on the list, and more, would definitely indicate that it should be granted personhood sta- tus. The fact that the AI's conscious- ness resides in a different substrate, e.g. silicon, carbon nanotubes, quantum dots etc., than human con- sciousness is not a valid reason to deny it equal status 33 . The implica- tions of this equality for AI and soci- ety generally would be extensive and profound. AI would have the right to life, (i.e. not to be unplugged) and the right not to be subject to intentional infliction of emotional dis- tress, (i.e. the right not to be exposed to threats that it will be unplugged) and the right to receive critical medical care. 34 2. Would AI be considered as a vulnerable class of person? A sub-issue of the personhood one is the question as to whether AI would be considered to be a mem- ber of a vulnerable class of persons such as, for example, children, pris- oners, the poor and the disabled. If that is the case, then as discussed previously, special considerations would likely apply for purposes of the informed consent issue. An AI would likely have superior reasoning powers to most humans and yet it would be in the process of learning how to apply that knowledge in an appropriate manner and would likely not have fully developed that ability in the early stages. An AI would like- ly have little or no freedom of move- ment in the early stages of its devel- opment. It would probably either be confined to a computer lab like a prisoner or else, if it were housed in a mobile robot, it would be limited to a small area of exploration, or re- quired to be accompanied at all times by a human handler. It would also be dependent on the researchers who built it for new information, pro- gramming enhancements, electrical power and maintenance, like an indigent person who is dependent on welfare for the basic necessities of life. Therefore, special precau- tions would have to be undertaken to ensure that an AI gives truly vol- untary, informed consent to partici- pate in a historical simulation if it is at an early stage of its development. By way of analogy, the U.S. federal regulations on Protection of Human Research Subjects require that, in the case of prisoners 35 , there be some additional steps taken, includ- ing the following: 1. the majority of the Institutional Review Board (IRB), aside from prisoner members, should have no association with the prison; (in this case, it would be the computer lab) 2. at least one member of the IRB should be a prisoner or a prisoner representative/advo- cate with an appropriate back- ground (in this case, another AI in the lab or a fully devel- oped AI outside the lab); 3. parole boards should not take into account the fact that the prisoner participated in the research in determining whe- ther to grant early release (in this case, the computer lab should arguably not take it into account in determining whether the AI should be released early from the lab"},{"pageNumber":11,"pageContent":"Historical Simulations 33 into the world); 4. the prisoner should not be tempted by possible advan- tages of participating in the experiment such as improved living conditions, opportunity for earnings, medical care etc. such that he or she can- not objectively assess the risks involved (in this case, the computer lab should not offer enhanced programming as an incentive to take part in the historical simulation); 5. the research must concern conditions particularly affect- ing prisoners as a class, or have the intent and reason- able probability of improving their health and well-being (in this case, the historical simu- lation would have to assist the AI in its development, such as perhaps by enabling it to bet- ter relate to humans). Therefore, by analogizing from the regulations concerning experi- ments on prisoners, it is evident that, even for AI's that remain con- fined to the computer lab and have not been released into the world, it is possible, with certain safeguards in place, to ensure that truly volun- tary, informed consent has been obtained for the AI's participation in the historical simulation. Having AI's participate in historical simulations while still in the lab would be benefi- cial in that the lab could use these trials to weed out and re-program \"bad seeds\" and also to train AI's in human empathy and emotional intel- ligence, since the AI while in the simulation would genuinely believe itself to be human. A recent development in the case law regarding medical experi- ments involving human subjects, which may be relevant to the issue of informed consent in historical simulations, is the concept of harm to dignity. This doctrine indicates that there is a legally and constitu- tionally protectable interest in med- ical choice for human subjects, regardless of whether or not there is actual injury to the subject. In the case Diaz v. Hillsborough County Hospital Authority , 36 the plaintiff was a sixteen year old Hispanic girl in her first pregnancy who attended the Tampa General Hospital's high risk clinic. Upon admission to the hospital, she was given sedatives to arrest her preterm labor and, while in a drowsy state from the drugs, she signed a complicated, three page, English language consent form document to participate in a drug study for fetal lung immaturity. Diaz, along with 384 other women, participated in the study with no adverse physical effects. However, they felt that their consent was obtained through coercion, and insti- tuted a class action against the hos- pital for harm to human dignity. Ultimately, the case was settled for $3.8 million, covering a class of about 5,000 pregnant women who had been subject to various medical experiments. What is significant about this case is that it indicates that there is a compensable harm resulting from injury to human digni- ty in medical experiments, even where there is no actual injury or harm to the patient 37 . Even if the AI has not suffered harm as a result of participating in the experiment, it may have a cause of action for harm to its dignity on the basis that, like a"},{"pageNumber":12,"pageContent":"Journal of Futures Studies 34 16 year old pregnant Hispanic girl who cannot understand the com- plexities of a three page English lan- guage consent form, a newly creat- ed AI cannot fully fathom the depths of human suffering when it has never been inside a historical simu- lation, and hence cannot properly give informed consent to enter one. This argument is, of course, a highly speculative one, and should not cre- ate an insurmountable obstacle to would-be creators of historical simu- lations. It will likely, at best, be one of the many factors tending to lead them in the direction of caution and discretion in the process of obtain- ing informed consent for these simu- lations. 3. Intentional Infliction of Emotional Distress In the past few years, there have been several lawsuits for intentional infliction of emotional dis- tress 38 launched by participants in Reality TV shows 39 , which are popu- lar programs in which participants are put into embarrassing, humiliat- ing and/or stressful competitions or situations. Often, to enhance the drama, conflict and suspense fac- tors, the participants have been deceived by the producers of the program as to the real circum- stances involved in the program. 40 W ould the litigation concerning Reality TV provide a precedent for possible lawsuits concerning histori- cal simulations or is it distinguish- able? A good defense that could be established by the creators of histor- ical simulations in response to law- suits for emotional distress would be that when the AI exited the simula- tion, the simulated human life that it had been living ended, and there- fore any pain and suffering that it endured in the simulation should not form a cause of action by the AI. This would be similar to the general principle in many jurisdictions that a cause of action for pain and suffer- ing cannot be continued by the estate of a deceased person. 41 The notion is that the pain and suffering that the deceased experienced dur- ing his or her lifetime does not actu- ally diminish the value of the de- ceased's estate, and therefore the executor of that estate cannot com- mence or continue an action for such pain and suffering. In the con- text of an experiment on AI in a his- torical simulation, the life played by the AI has terminated, and the AI is analogous to the executor. The creators of the simulation would be bolstered in their argument by the fact that the AI's direct mem- ory of the events in the historical simulation could be wiped clean. There would be some drawbacks to this memory erasure, in that the AI would not be able to learn from the experience of life in the simulation, but the erasure remedy would not used in every case, of course, only in ones where there was a serious risk of a lawsuit for intentional inflic- tion of emotional distress, due to the circumstances of the simulation. Even in situations where erasure was used, it would still be possible to utilize the AI's experiences in the simulation for purposes of weeding out manufacturing or programming defects in the AI. It may also be pos- sible to use the AI's experiences for purposes of social and economic experiments generally that extended"},{"pageNumber":13,"pageContent":"Historical Simulations 35 beyond the individual learning of the AI. However, where the AI endured exceptional pain and suffering, per- haps due to an unpredictable chain of events rapidly flowing from the unique nature of the simulation, it would be wise to apply an exclu- sionary rule to prohibit the use of any data arising from such pain and suffering, so as not to encourage the creators of the simulation or others to run similar experiments in the future 42 . 4. Publicity, privacy and copyright issues   There is also the issue of whether historical simulations would involve the consideration of the publicity or privacy rights, or both, of the individ- uals whose lives are being simulat- ed. Although publicity rights, as property rights that are part of the deceased's estate, generally contin- ue after the death of the individual, they are subject to overarching free- dom of speech concerns that allow the use of the individual's image for informational and parody/satire pur- poses, but not for commercial/adver- tis ing purposes. Historical simula- tions would probably fall into the for- mer category. More problematic then would be the issue of privacy rights, which have been traditionally asserted by non-celebrities. Although historically, privacy rights, as a tort like intentional infliction of emotional distress, have tended not to contin- ue after an individual's death, there has been a trend in the U.S. and other countries such as Germany, for example, towards recognizing that privacy rights in some circum- stances continue after death. 43 These rights are sometimes referred to as a right of anonymity. To the extent that this legal trend continues and is adopted by a future society, there may be some constraints on the creation and running of historical simulations. However, the more time that elapses between the death of the deceased and the piercing of the veil of privacy, the less danger there is of a successful lawsuit for viola- tion of privacy rights. Finally, there are copyright issues associated with illegal prac- tices such as file-sharing, as well as other issues which may be in more of a gray area such as bricolage, or \"mashups\" where individuals take pieces of videos or songs that they have purchased and combine them in unique ways for personal and other non-commercial uses. 44 The likelihood that we live in a simulation would be greatly increased by the extent to which these simulations are copied and distributed, which might be by future file sharing sys- tems. Assuming that the current copyright prohibitions on file-sharing and on technologies that actively encourage users to file-share are continued and adopted by a future society 45 , then would there be a role for bricolage or \"mashups\" instead in a future society running historical simulations? A person in a future society may be interested in taking a simulation of the Soviet Revolution of 1917, for example, and altering it to remove Lenin and substituting a simulated Russian ancestor from his own family tree and then observing what happens. Would the creation of a unique new version for the per- sonal non-commercial use of the bricoleur and his or her friends con-"},{"pageNumber":14,"pageContent":"Journal of Futures Studies 36 stitute fair use or would it be a viola- tion of intellectual property laws? Lawrence Lessig 46 would probably lean towards concluding that it is not or should not be a violation of intel- lectual property rights, but in my view this may be a bit over-opti- mistic 47 , and therefore there may be an additional constraint, at least in theory, on the numbers of historical simulations that are run. Nonetheless, even a future soci- ety would likely have some practical difficulties in suppressing the dis- semination of large numbers of per- sonalized historical simulations cre- ated by bricoleurs of the future. Furthermore, given that technology has recently been developed to allow individuals to create their own personalized MMORPG without a significant investment 48 , it may become possible for future individu- als to create their own historical sim- ulations from scratch in the same manner, in which case copyright issues due to bricolage would not arise. Conclusion Historical simulations can be created, modified and copied with relative rapidity compared to the bil- lions of years it takes for a real civi- lization to evolve. Furthermore, it is highly probable that a future society would have extensive motivation to create and run historical simulations and not encounter any insurmount- able ethical and legal obstacles to doing so. Therefore, any presump- tion that we live in a real world is rebutted and it is very likely that we live in a simulated world created by a future society. What do we do? Nick Bostrom suggests that, if we do live in a historical simulation, then it is possible that our behavior is being evaluated by the creators of the sim- ulation to determine our suitability for admission into a future afterlife that exists outside of the simulation and so we may have an incentive to act ethically. 49 Robin Hanson sug- gests that we should endeavor to continue to lead an interesting life or else associate with interesting peo- ple, so that the creators of our simu- lation will not unplug it out of bore- dom. 50 These are both possible answers to the question of what we should do upon realizing that we probably live in a simulation. Hanson's concept of endeavoring to lead interesting lives and associate with interesting people to ensure the continued existence of the simula- tion touches on a key issue, which is that of when the simulation can be expected to end. The creators of the simulation would likely not continue it past the point in history when the technology to create and run these simulations on a widespread basis was first developed. The reason for stopping it at this point is that, as noted previously, a historical simula- tion that is set in a period when the necessary simulation technology already exists would tend to stymie any efforts to make the simulated entities unaware that they exist in a simulation. This lack of awareness is necessary for the simulation to run effectively, otherwise the behav- ior of the AI's inhabiting the simula- tion would not be genuine and the basic purpose of the simulation would not be accomplished. Another reason is to avoid stacking of simu- lations, i.e. simulations within simu-"},{"pageNumber":15,"pageContent":"Historical Simulations 37 lations, which would inevitably at some point overload the base machine on which all of the simula- tions are running, thereby causing all of the worlds to disappear. This is illustrated by the fact that, as Seth Lloyd of MIT has noted in his recent book, Programming the Universe , if every single elementary particle in the real universe were devoted to quantum computation, it would be able to perform 10 122 operations per second on 10 92 bits of information 51 . In a stacked simulation scenario, where 10 6 simulations are progres- sively stacked, after only 16 genera- tions, the number of simulations would exceed by a factor of 10 4 the total number of bits of information available for computation in the real universe. Therefore, the end of history can be anticipated as arriving when the technology necessary to create and run historical simulations on a wide- spread basis becomes feasible. This is estimated to be 2050, based on Ray Kurzweil's projections of Moore's Law. When I say, the \"end of histo- ry\", I mean it both in the Hegel /Fukuyama 52 sense, i.e. when humankind reaches its ultimate end state, and in the technological sin- gularity sense 53 , i.e. where the world as we know it winks out of existence and we are transported to a collec- tive consciousness in the form of a generalized artificial intelligence of which, as individualized AI's or as technologically enhanced humans, we would each constitute a part. In terms of how we should behave in light of this knowledge, we should recognize that long range planning beyond 2050 would be futile. Correspondence Peter S. Jenkins 11 Ellsworth Avenue Toronto, Ontario Canada MCG 2K 4 Personal weblog: http://petabytes.typepad.com/ Email: peterjenkins@rogers.com Notes 1. Plato. 2. Descartes 1664. 3. Baudrillard 1994. 4. Bostrom 2003. 5. Kurzweil 2005, p. 70. 6. Bostrom 2006, Q.2. 7. Due to his \"bland indifference princi- ple\" (Bostrom 2003), p. 7, Bostrom assumes that one could be located anywhere in the universe that is compatible with life, whereas I examine the issue in the context of earth since, as Kurzweil states, the complete absence of evidence of extraterrestrial life indicates that we should assume (until proven other- wise) that we are the first civilization in the universe (Kurzweil 2005), p.357. Furthermore, Bostrom does not make any assumptions about when the necessary simulation technology would become available (Bostrom 2003), p.3. Due to the lack of a clear context of time and place, Bostrom's approach understandably results in his providing a very brief, general discussion of the motiva- tional, ethical and legal issues, whereas my approach lays the foun- dation for a detailed, pragmatic- analysis. 8. Center for Democracy and Technology 2006, p.2. 9. Bostrom 2003, p. 9. 10. Id."},{"pageNumber":16,"pageContent":"Journal of Futures Studies 38 1 1. Krupa 1996. 12. McLuhan 1964. 13. Castronova 2005, p. 252. 14. For example, the Serious Games Project at the Woodrow Wilson International Center for Scholars which has modified Maxis' Simcity , the Democracy Design Workshop at New York Law School which has leased an island in Second Life , and America's Army which the U.S. Defense Dept. created from \"middleware\" which is a tem- plate sold by developers of online games. 15. Rees 2003. 16. Emmanuel, Wendler and Grady 2000, p. 2701. 17. e.g. the Nuremberg Code and the Declaration of Helsinki. 18. Boym 2001, p.41. 19. An example of augmented reality would be eyeglasses containing a heads-up display (HUD) which superimposes personalized infor- mation over the view of the real world, to assist the wearer to bet- ter understand that world. This system could be transposed into a historical simulation, as it already is in some MMORPG's, e.g. Second Life. http://secondlife.com/ newsletter/2006_02_15/hud.php. 20. e.g. Litton and Miller 2005,p.571. 21. Dawkins 1989, p.186. 22. U.S. Code of Federal Regulations 2005, Title 45, s.46.3. 23. Castronova 2005, p. 97. 24. Litton and Miller 2005, p.572. 25. Milgram 1963, p.371. 26. Herrera 2001, n.17. 27. Castronova 2001. Tables 1, 3. Jenkins 2004, p.11. 28. On October 1, 2005, the Chinese government instituted fatigue requirements for online games such as World of Warcraft and Legend of Mir2, to reduce the amount of time gamers spend playing these games. (Subsequently, in January, 2006, these require- ments were modified to exempt adults.) The system only awards players full experience points for the first three hours of each day, half experience for the next two hours, and no experience after five hours. This was introduced in response to reports of actual, real- life deaths of players due to physi- cal exhaustion, which obviously is not a concern for AI. 29. Exiting a MMORPG permanently after hundreds or thousands of cumulative hours of play has been compared to a real world suicide, indicating that such a step would be even more difficult to take in a totally immersive historical simula- tion. See www.joystiq.com/2006/ 03/09/drakedogs-suicide-now-on- google-video/. 30. Edwards 2005, p. 114. 31. Sunstein 2000, p. 19. 32. Rothblatt 2003, Gray 2002, p.23. (McNally and Inayatullah 1988) . 33. Kurzweil 2005, p.375. 34. Rothblatt 2003. 35. U.S. Code of Federal Regulations 2005, Title 45, sec. 46.3. 36. Diaz v. Hillsborough County Hospital Authority, 1996 U.S. Dist. LEXIS 1 1913. 37. Hanlon and Shapiro 2002, p.3. 38. The elements of a prima facie case for the tort of intentional infliction of emotional distress are as follows: 1) extreme and outrageous con- duct by the defendant with the intention of causing, or reckless disregard of the probability of causing, emotional distress; 2) the"},{"pageNumber":17,"pageContent":"Historical Simulations 39 plaintiff's suffering severe or extreme emotional distress; and 3) actual and proximate causation of the emotional distress by the defendant's outrageous conduct. Generally, the courts have found that for the defendant to be liable, its conduct must have been so outrageous in character, and so extreme in degree, as to go beyond all possible bounds of decency, and to be regarded as atrocious, and utterly intolerable in a civilized community. Flynn v. Higham, 149 Cal. App. 3d 977 (1983). 39. Deleese Williams v. American Broadcasting Corporation et al, Superior Court of the State of California (Statement of Claim dated Sept. 21, 2005). 40. e.g. \"Wife Swap\" where two hus- bands exchange wives for a tem- porary period. The producers of this show were sued by a husband participant who was upset that his wife was exchanged for one of the male partners in a gay couple. 41. County of Los Angeles v. Superior Court of Los Angeles County (Kim A. Schonert) Supreme Court of Los Angeles, No. BC0908848, Aug. 12, 1999. 42. Cohen 2003, p.32. 43. For example, a German court recent- ly ordered the shutdown of the German Wikipedia site on the grounds that it infringed on the pri- vacy and anonymity rights of a deceased hacker known as Tron, by mentioning his real name. Boris (Family name), Case-No. 209 C 1015/05, Berlin Municipal Court, Dec. 14, 2005. http://service. spiegel.de/cache/international/ 0,1518,396307,00.html. In National Archives and Records Administr- ation v. Favish, 124 U.S. 1570 (2004), the U.S. Supreme Court ruled that photographs of the dead body of Vince Foster, the White House Counsel who committed suicide, should not be disclosed, in order to protect the privacy of his family. 44. For example, Anime Music Videos http://www.animemusicvideos.org/ home/home.php. 45. Metro-Goldwyn-Mayer Studios v. Grokster Ltd. 545 U.S. 1 (2005). 46. Lessig 2005, p. 46. 47. But see the recent case on fair use, Blake A. Field v. Google Inc., U.S. Dist. Crt. (Nevada) Jan. 12, 2006, which may give some sup- port to Lessig's view. 48. http://multiverse.net/ . 49. Bostrom 2003, p.10. 50. Hanson 2001, p.3. 51. Lloyd 2006, p. 166. 52. Fukuyama 1992. 53. Kurzweil 2005, p.29. References Baudrillard, Jean.1994. Simulacra and Simulation . Ann Arbor: University of Michigan Press Blake A. Field v. Google Inc. 2006. Jan. 12. U.S. Dist. Crt. Nevada. Boris (Family name). Case-No. 209 C 1015/05. Berlin Municipal Court. Dec. 14, 2005. Bostrom, Nick. 2003. \"Are You Living in a Computer Simulation?\" Philosophical Quarterly . Vol. 53(211): 243-255 ____. 2006. \"The Simulation Argument FAQ, Version 1.1.\" Accessed at www.simulation- argument.com/faq.html . Boym, Svetlana. 2001. The Future of"},{"pageNumber":18,"pageContent":"Journal of Futures Studies 40 Nostalgia . New York: Basic Books. Castronova, Edward. 2001. \"Virtual W orlds: A First-Hand Account of Market and Society on the Cyberian Frontier.\" CESifo Work- ing Paper Series . No. 618. ____. 2005. Synthetic Worlds – the Business and Culture of Online Games . Chicago: University of Chicago Press Center for Democracy and Technology. 2006. \"Digital Search and Seizure: Updating Privacy Protections to Keep Pace with Technology.\" Accessed at http://www.cdt.org/ publications/articles.php . Cohen, Baruch C. 2003. \"The Ethics of Using Data from Nazi Medical Experiments.\" Jewish Law Articles . Accessed at http://www.jlaw. com/ Articles/NaziMedEx.html. Council for International Organizations of Medical Sciences. 2002. \"International Ethical Guidelines for Biomedical Research Involving Human Subjects.\" Accessed at http://www.cioms.ch/frame_ guidelines_nov_2002. htm . County of Los Angeles v. Superior Court of Los Angeles County (Kim A. Schonert). 1999. Aug. 12. Supreme Court of Los Angeles. No. BC0908848. Dawkins, Richard. 1989. The Selfish Gene . Oxford: Oxford University Press. Deleese Williams v. American Broad- casting Corporation et al. 2005. Sept. 21. Superior Court of the State of California. Descartes, Rene.1644. Meditations on First Philosophy . Cambridge: Cambridge University Press. Diaz v. Hillsborough County Hospital Authority. 1996. U.S. Dist. LEXIS 1 1913. Edwards, Sarah J.L. 2005. \"Research Participation and the Right to Withdraw.\" Bioethics . Vol. 19(2): 1 12. Emmanuel, E.J., Wendler, D., and C. Grady. 2000. \"What makes clini- cal research ethical?\" Journal of the American Medical Association . V ol. 283: 2701-2711. Flynn v. Higham. 1983. 149 Cal. App. 3d 977. Fukuyama, Francis. 1992. The End of History and the Last Man . New Y ork: Free Press. Gray, Chris. 2002. Cyborg Citizen . New York: Routledge. Hanlon, Stephen and Shapiro, Robyn.2002. \"Ethical Issues in Biomedical Research: Diaz v. Hillsborough County Hospital Authority.\" ABA Human Rights Magazine . Winter 2002. Hanson, Robin. 2001. \"How to Live in a Simulation\" Journal of Evolution and Technology . Vol. 7(1). Herrera, C.D. 2001. \"Informed Consent vs. Incomplete Disclosure in Social Science.\" USC Bioethics Club . Accessed at http://www- hsc.usc.edu/~mbernste/BJC.4.ht ml . Jenkins, Peter S. 2004. \"The Virtual W orld as a Company Town – Freedom of Speech in Massively Multiple Online Role-Playing Games.\" Journal of Internet Law . V ol. 8(1). Krupa, Frederique. 1996. \"The History of Design from the Enlightenment to the Industrial Revolution.\" Parsons School of Design . Accessed at www.translucency. com/frede/hod.html."},{"pageNumber":19,"pageContent":"Historical Simulations 41 Kurzweil, Ray. 1999. The Age of Spiritual Machines . New York: Penguin ____. 2005. The Singularity is Near . New York: Penguin. Lessig, Lawrence. 2005. Free Culture . New York: Penguin. Litton, Paul and Miller, Franklin. 2005. \"A Normative Justification for Distinguishing the Ethics of Clinical Research from the Ethics of Medical Care\" The Journal of Law, Medicine and Ethics . No.4: 566-573. Lloyd, Seth. 2006. Programming the Universe . New York: Knopf. McLuhan, Marshall. 1964. Under- standing Media: The Extensions of Man . New York: Signet. McNally, Phil and Inayatullah, Sohail. 1988. \"The Rights of Robots: T echnology, Law and Culture in the 21 st Century.\" Futures . Vol. 20(2): 119-136. Accessed at http://www.kurzweilai.net/meme/ frame.html?main=/articles/art 0266.html . Metro-Goldwyn-Mayer Studios v. Grokster Ltd. 2005. 545 U.S. 1. Milgram, Stanley. 1963.\"Behavioral Study of Obedience.\" Journal of Abnormal and Social Psychology . V ol. 67: 371-378. National Archives and Records Administration v. Favish. 2004. 124 U.S. 1570. Plato. The Republic . New York: Dover Publications. Rees, Martin. 2003. Our Final Hour - A Scientist's Warning How Terror, Error and Environmental Disaster Threaten Humankind's Future in This Century – On Earth and Beyond . New York: Basic Books. Rothblatt, Martine. 2003. \"Biocy- berethics – should we stop a company from unplugging an intelligent computer?\" Intern- ational Bar Association Confer- ence. San Francisco. September 16, 2003. Accessed at http:// www.kurzweilai.net/meme/ frame.html?main=/articles/ art0594.html . Sunstein, Cass (ed.). 2000. Behavioral Law & Economics . New York: Cambridge University Press. \"The Nuremberg Code.\" 1947. Reprinted in the British Medical Journal . Vol. 7070 (313): 1448. U.S. Code of Federal Regulations. 2005. Title 45 Public Welfare, Department of Health and Human Services. Part 46 – Protection of Human Subjects. W orld Medical Association. 2000. \"Declaration of Helsinki: Ethical Principles for Medical Research Involving Human Subjects.\" Reprinted in the Journal of the American Medical Association . V ol. 284: 3043-45."},{"pageNumber":20,"pageContent":"Journal of Futures Studies 42"}]
