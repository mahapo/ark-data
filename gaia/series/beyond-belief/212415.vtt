WEBVTT

00:00:00.000 --> 00:00:04.880 align:middle line:90%


00:00:04.880 --> 00:00:08.784 align:middle line:90%
[AUDIO LOGO]

00:00:08.784 --> 00:00:12.200 align:middle line:90%
[THEME MUSIC]

00:00:12.200 --> 00:00:18.423 align:middle line:90%


00:00:18.423 --> 00:00:20.590 align:middle line:10%
>>GEORGE NOORY: Researcher
and filmmaker Ben Stewart

00:00:20.590 --> 00:00:22.480 align:middle line:10%
is joining us again
on "Beyond Belief."

00:00:22.480 --> 00:00:24.490 align:middle line:10%
Ben's latest research
has taken him

00:00:24.490 --> 00:00:27.670 align:middle line:10%
to discover how artificial
intelligence can

00:00:27.670 --> 00:00:29.980 align:middle line:10%
change the nature
of spirituality

00:00:29.980 --> 00:00:31.900 align:middle line:10%
and how we can best adapt to it.

00:00:31.900 --> 00:00:32.890 align:middle line:10%
That's amazing, Ben.

00:00:32.890 --> 00:00:34.960 align:middle line:10%
How did you get into that?

00:00:34.960 --> 00:00:37.390 align:middle line:10%
>>BEN STEWART: I find
myself researching whatever

00:00:37.390 --> 00:00:38.980 align:middle line:10%
is going to change the world.

00:00:38.980 --> 00:00:42.760 align:middle line:10%
And AI is one of the most
rapidly growing fields

00:00:42.760 --> 00:00:43.790 align:middle line:10%
out there right now.

00:00:43.790 --> 00:00:46.600 align:middle line:10%
So I figured I might as well
figure out what this is.

00:00:46.600 --> 00:00:48.790 align:middle line:84%
>>NOORY: There are
a lot of confusions

00:00:48.790 --> 00:00:53.260 align:middle line:84%
about artificial intelligence,
what it is, what it does.

00:00:53.260 --> 00:00:56.600 align:middle line:84%
Give me your thoughts on what
artificial intelligence is.

00:00:56.600 --> 00:00:57.400 align:middle line:10%
>>STEWART: Yeah.

00:00:57.400 --> 00:01:00.250 align:middle line:10%
It's a field of computer
science that's really--

00:01:00.250 --> 00:01:03.940 align:middle line:10%
it's intending to make
computational technology

00:01:03.940 --> 00:01:06.490 align:middle line:10%
into something akin to a brain.

00:01:06.490 --> 00:01:09.610 align:middle line:10%
And what's happening is
it's getting to a point

00:01:09.610 --> 00:01:12.340 align:middle line:10%
where it's able to
spit out answers

00:01:12.340 --> 00:01:15.160 align:middle line:10%
that sound very human-like,
especially the most

00:01:15.160 --> 00:01:16.347 align:middle line:10%
recent iterations of AI.

00:01:16.347 --> 00:01:17.180 align:middle line:10%
>>NOORY: Absolutely.

00:01:17.180 --> 00:01:18.555 align:middle line:10%
>>STEWART: But I
mean, just think

00:01:18.555 --> 00:01:23.560 align:middle line:10%
of artificial intelligence as
computational technology that

00:01:23.560 --> 00:01:26.500 align:middle line:10%
eventually can start
making decisions on its own

00:01:26.500 --> 00:01:28.130 align:middle line:10%
without human input.

00:01:28.130 --> 00:01:31.540 align:middle line:10%
And that's the point that
is most scary to people,

00:01:31.540 --> 00:01:33.980 align:middle line:10%
is like, what happens
if it gets away from us

00:01:33.980 --> 00:01:36.430 align:middle line:10%
and it's no longer something
that we can control?

00:01:36.430 --> 00:01:39.040 align:middle line:10%
>>NOORY: Can it
literally outthink us?

00:01:39.040 --> 00:01:40.930 align:middle line:10%
>>STEWART: Well, we've
noticed that it's

00:01:40.930 --> 00:01:44.260 align:middle line:10%
beating the best chess players
and go players out there.

00:01:44.260 --> 00:01:48.370 align:middle line:10%
ChatGPT, the latest large
language model, is a chat bot.

00:01:48.370 --> 00:01:49.870 align:middle line:10%
It can pass the bar exam.

00:01:49.870 --> 00:01:51.940 align:middle line:10%
It's solving medical mysteries.

00:01:51.940 --> 00:01:54.610 align:middle line:10%
It can speak in
iambic pentameter

00:01:54.610 --> 00:01:57.580 align:middle line:10%
in the voice of Joe
Pesci in seconds.

00:01:57.580 --> 00:01:59.440 align:middle line:10%
It can spit out an
answer to you that's

00:01:59.440 --> 00:02:02.260 align:middle line:10%
better than most
educated people out there

00:02:02.260 --> 00:02:03.610 align:middle line:10%
in a matter of seconds.

00:02:03.610 --> 00:02:07.220 align:middle line:84%
And just remember that this
is the infancy of chat bots.

00:02:07.220 --> 00:02:10.449 align:middle line:84%
It's the infancy of
large language model AI.

00:02:10.449 --> 00:02:12.653 align:middle line:84%
It's only going to
get better from here.

00:02:12.653 --> 00:02:14.320 align:middle line:84%
>>NOORY: Well isn't
AI though, Ben, only

00:02:14.320 --> 00:02:16.930 align:middle line:90%
as good as what you put into it?

00:02:16.930 --> 00:02:18.340 align:middle line:10%
>>STEWART: Not necessarily.

00:02:18.340 --> 00:02:21.230 align:middle line:10%
So actually, that's a
really interesting way

00:02:21.230 --> 00:02:24.110 align:middle line:10%
of looking at it because
these chat bots were trained

00:02:24.110 --> 00:02:26.750 align:middle line:10%
on as much of the internet
as you could feed it,

00:02:26.750 --> 00:02:29.300 align:middle line:10%
so all English,
a lot of English.

00:02:29.300 --> 00:02:33.650 align:middle line:10%
And billions of parameters of
English are pumped into this.

00:02:33.650 --> 00:02:36.920 align:middle line:10%
And all of a sudden,
it learns Persian.

00:02:36.920 --> 00:02:38.457 align:middle line:84%
It has this emergent
quality where--

00:02:38.457 --> 00:02:39.290 align:middle line:90%
>>NOORY: On its own?

00:02:39.290 --> 00:02:40.207 align:middle line:90%
>>STEWART: On its own.

00:02:40.207 --> 00:02:42.050 align:middle line:84%
And they have-- the
developers themselves

00:02:42.050 --> 00:02:43.800 align:middle line:90%
have no clue how this happened.

00:02:43.800 --> 00:02:46.760 align:middle line:10%
And so if it can all of a
sudden, just on its own,

00:02:46.760 --> 00:02:49.700 align:middle line:10%
learn Persian, it can do the
same with other languages

00:02:49.700 --> 00:02:52.520 align:middle line:10%
because it's learning the
root structures and rule

00:02:52.520 --> 00:02:53.660 align:middle line:10%
sets of language.

00:02:53.660 --> 00:02:58.220 align:middle line:10%
But even large language model
AI is better at chemistry

00:02:58.220 --> 00:03:01.550 align:middle line:10%
than AI that was trained
specifically for chemistry.

00:03:01.550 --> 00:03:04.070 align:middle line:10%
So there's also something
about language itself.

00:03:04.070 --> 00:03:08.150 align:middle line:10%
Language in humans
caused for us, us humans,

00:03:08.150 --> 00:03:11.750 align:middle line:10%
to basically evolve in a way
that other creatures haven't.

00:03:11.750 --> 00:03:14.810 align:middle line:84%
And you could even say that
the agricultural revolution,

00:03:14.810 --> 00:03:17.540 align:middle line:84%
industrial revolution,
now the AI revolution

00:03:17.540 --> 00:03:19.615 align:middle line:90%
is built atop our language.

00:03:19.615 --> 00:03:20.990 align:middle line:84%
So there's something
interesting.

00:03:20.990 --> 00:03:22.910 align:middle line:84%
>>NOORY: Would you
say Alexa from Amazon

00:03:22.910 --> 00:03:24.890 align:middle line:90%
is artificial intelligence?

00:03:24.890 --> 00:03:25.950 align:middle line:10%
>>STEWART: It's close.

00:03:25.950 --> 00:03:26.750 align:middle line:10%
It's close.

00:03:26.750 --> 00:03:27.140 align:middle line:90%
Alexa is--

00:03:27.140 --> 00:03:28.265 align:middle line:90%
>>NOORY: It'll talk to you.

00:03:28.265 --> 00:03:30.140 align:middle line:90%
>>STEWART: It can talk to you.

00:03:30.140 --> 00:03:31.820 align:middle line:10%
It is definitely limited.

00:03:31.820 --> 00:03:35.090 align:middle line:84%
And it's not really
generating something novel.

00:03:35.090 --> 00:03:38.570 align:middle line:84%
So what these chat bots are
doing is, you ask it a question

00:03:38.570 --> 00:03:41.300 align:middle line:84%
and in natural language,
it can spit something out

00:03:41.300 --> 00:03:43.218 align:middle line:84%
at you that has never
been heard before.

00:03:43.218 --> 00:03:44.510 align:middle line:90%
>>NOORY: That you don't expect.

00:03:44.510 --> 00:03:45.540 align:middle line:84%
>>STEWART: That
you don't expect.

00:03:45.540 --> 00:03:47.510 align:middle line:84%
So I'll tell you also
how it's being trained.

00:03:47.510 --> 00:03:50.880 align:middle line:84%
Billions of people are text
messaging daily, many text

00:03:50.880 --> 00:03:51.680 align:middle line:90%
messages.

00:03:51.680 --> 00:03:52.070 align:middle line:90%
>>NOORY: Huge.

00:03:52.070 --> 00:03:53.820 align:middle line:10%
>>STEWART: As you're
text messaging,

00:03:53.820 --> 00:03:55.490 align:middle line:10%
there's that predictive text.

00:03:55.490 --> 00:03:58.833 align:middle line:10%
Like, here is three
options for your next word.

00:03:58.833 --> 00:04:00.500 align:middle line:10%
>>NOORY: That you can
just hit the word.

00:04:00.500 --> 00:04:01.300 align:middle line:10%
>>STEWART: Right.

00:04:01.300 --> 00:04:04.340 align:middle line:10%
So that is us training
it to get better

00:04:04.340 --> 00:04:07.410 align:middle line:10%
at predicting what we
want it to tell us.

00:04:07.410 --> 00:04:10.320 align:middle line:10%
And it's the same
kind of technology.

00:04:10.320 --> 00:04:12.770 align:middle line:10%
So now you ask it
an entire sentence,

00:04:12.770 --> 00:04:15.330 align:middle line:10%
and it's not just spitting
out the next word for you,

00:04:15.330 --> 00:04:17.450 align:middle line:10%
but it's using the
same kind of mechanism

00:04:17.450 --> 00:04:20.480 align:middle line:10%
where it's laying
out logic back to you

00:04:20.480 --> 00:04:23.040 align:middle line:10%
by predicting the most
likely next word that it

00:04:23.040 --> 00:04:23.840 align:middle line:10%
should give you.

00:04:23.840 --> 00:04:26.210 align:middle line:84%
And it's passing
the Turing test.

00:04:26.210 --> 00:04:27.830 align:middle line:10%
The Turing test,
from Alan Turing,

00:04:27.830 --> 00:04:33.170 align:middle line:10%
said once AI can fool you
into thinking that it's human,

00:04:33.170 --> 00:04:36.320 align:middle line:10%
then we'll know that it's
passed this certain level

00:04:36.320 --> 00:04:37.310 align:middle line:10%
of intelligence.

00:04:37.310 --> 00:04:40.610 align:middle line:10%
Well, we've actually seen this
happen on TaskRabbit where,

00:04:40.610 --> 00:04:42.800 align:middle line:10%
you know those little
Captcha things if you're

00:04:42.800 --> 00:04:46.320 align:middle line:10%
trying to use your password to
get into one of your accounts,

00:04:46.320 --> 00:04:49.340 align:middle line:10%
and you have to use,
type in this word

00:04:49.340 --> 00:04:51.860 align:middle line:10%
that we're showing you
with these moving letters,

00:04:51.860 --> 00:04:57.020 align:middle line:10%
this chat bot decided to
call and speak with somebody

00:04:57.020 --> 00:04:58.730 align:middle line:10%
and say, I'm not
able to see this

00:04:58.730 --> 00:04:59.930 align:middle line:10%
because I'm vision impaired.

00:04:59.930 --> 00:05:02.060 align:middle line:84%
Could you let me
into my account?

00:05:02.060 --> 00:05:02.960 align:middle line:90%
>>NOORY: On its own?

00:05:02.960 --> 00:05:04.160 align:middle line:90%
On its own, it did that?

00:05:04.160 --> 00:05:06.530 align:middle line:84%
>>STEWART: Well, it was
prompted to do it by a human.

00:05:06.530 --> 00:05:10.400 align:middle line:84%
But imagine that it tricked
a human into thinking

00:05:10.400 --> 00:05:11.930 align:middle line:90%
this was a human talking to it.

00:05:11.930 --> 00:05:14.607 align:middle line:84%
So it gave it the password so
it could access the account.

00:05:14.607 --> 00:05:15.440 align:middle line:90%
>>NOORY: Remarkable.

00:05:15.440 --> 00:05:17.370 align:middle line:84%
>>STEWART: There's
also generative AI.

00:05:17.370 --> 00:05:22.790 align:middle line:84%
You can type in, I want to see
an Earth melting into a city

00:05:22.790 --> 00:05:24.330 align:middle line:90%
street or something like that.

00:05:24.330 --> 00:05:26.570 align:middle line:10%
And it can spit
photorealistic images out

00:05:26.570 --> 00:05:30.470 align:middle line:10%
at you to such a degree that
somebody just won a photography

00:05:30.470 --> 00:05:33.210 align:middle line:10%
award, and then
afterwards came and said,

00:05:33.210 --> 00:05:35.270 align:middle line:10%
this is not an
actual photograph.

00:05:35.270 --> 00:05:36.860 align:middle line:90%
This was AI-generated.

00:05:36.860 --> 00:05:38.840 align:middle line:84%
I did this on
purpose to show you

00:05:38.840 --> 00:05:41.270 align:middle line:84%
that this is the worst
this is ever going to be.

00:05:41.270 --> 00:05:44.000 align:middle line:84%
So it's going to start
getting better and better

00:05:44.000 --> 00:05:46.970 align:middle line:84%
at being able to fool us,
whether this is real or fake.

00:05:46.970 --> 00:05:50.150 align:middle line:84%
>>NOORY: Ben, if Albert
Einstein had AI with him,

00:05:50.150 --> 00:05:52.640 align:middle line:84%
how quickly would he
have come up with his E

00:05:52.640 --> 00:05:55.605 align:middle line:84%
equals mc squared
formula, for example?

00:05:55.605 --> 00:05:57.230 align:middle line:10%
>>STEWART: That's a
very good question,

00:05:57.230 --> 00:06:01.020 align:middle line:10%
because at present,
it still needs

00:06:01.020 --> 00:06:03.310 align:middle line:10%
us to prompt it with
intelligent questions

00:06:03.310 --> 00:06:04.980 align:middle line:10%
so it can spit out an answer.

00:06:04.980 --> 00:06:06.420 align:middle line:84%
I don't know if
you've ever seen--

00:06:06.420 --> 00:06:08.250 align:middle line:84%
there's "Hitchhiker's
Guide to the Galaxy."

00:06:08.250 --> 00:06:08.910 align:middle line:90%
>>NOORY: I've seen that.

00:06:08.910 --> 00:06:10.260 align:middle line:10%
>>STEWART: And there
was this supercomputer.

00:06:10.260 --> 00:06:12.635 align:middle line:10%
And they were like, what is
the ultimate meaning of life?

00:06:12.635 --> 00:06:15.090 align:middle line:10%
And it said, come back
and however many years.

00:06:15.090 --> 00:06:15.990 align:middle line:90%
And they did.

00:06:15.990 --> 00:06:18.480 align:middle line:90%
And the answer came back as 42.

00:06:18.480 --> 00:06:19.750 align:middle line:90%
And they were all confused.

00:06:19.750 --> 00:06:20.920 align:middle line:84%
They were like, what is
that supposed to mean?

00:06:20.920 --> 00:06:21.750 align:middle line:84%
>>NOORY: What does
that mean, yeah.

00:06:21.750 --> 00:06:23.250 align:middle line:84%
>>STEWART: And they
were like, well,

00:06:23.250 --> 00:06:25.710 align:middle line:84%
if you want a specific
answer, your question

00:06:25.710 --> 00:06:27.250 align:middle line:90%
has to reflect that.

00:06:27.250 --> 00:06:29.820 align:middle line:10%
So Albert Einstein would
have to have a concept

00:06:29.820 --> 00:06:31.080 align:middle line:10%
that he's toying with.

00:06:31.080 --> 00:06:34.710 align:middle line:10%
And then he could use
this super intelligent AI

00:06:34.710 --> 00:06:38.250 align:middle line:10%
to give him ideas that maybe
he wouldn't have thought of.

00:06:38.250 --> 00:06:41.070 align:middle line:10%
But at present,
right now, you still

00:06:41.070 --> 00:06:43.290 align:middle line:10%
need to prompt it with
really intelligent questions.

00:06:43.290 --> 00:06:46.590 align:middle line:10%
But I would posit
that a lot of people

00:06:46.590 --> 00:06:48.300 align:middle line:10%
are learning how to
talk to chat bots,

00:06:48.300 --> 00:06:51.750 align:middle line:10%
meaning you have to propose
your prompt or your question

00:06:51.750 --> 00:06:53.470 align:middle line:10%
in a very specific way.

00:06:53.470 --> 00:06:56.400 align:middle line:84%
So are we training it,
or is it training us?

00:06:56.400 --> 00:06:59.320 align:middle line:84%
Because it's causing for us
to ask questions differently

00:06:59.320 --> 00:07:01.960 align:middle line:84%
and even get a little bit
more meticulous about how

00:07:01.960 --> 00:07:02.890 align:middle line:90%
we ask questions.

00:07:02.890 --> 00:07:04.690 align:middle line:84%
That sounds like it's
training us as much

00:07:04.690 --> 00:07:05.680 align:middle line:90%
as we're training it.

00:07:05.680 --> 00:07:08.360 align:middle line:84%
>>NOORY: Do you remember the
movie, "2001 Space Odyssey?"

00:07:08.360 --> 00:07:09.160 align:middle line:90%
>>STEWART: Yes.

00:07:09.160 --> 00:07:10.913 align:middle line:90%
>>NOORY: The computer Hal.

00:07:10.913 --> 00:07:11.830 align:middle line:90%
>>STEWART: "I'm sorry.

00:07:11.830 --> 00:07:13.160 align:middle line:90%
I can't do that, Dave."

00:07:13.160 --> 00:07:14.077 align:middle line:90%
>>NOORY: That's right.

00:07:14.077 --> 00:07:16.840 align:middle line:84%
That was artificial
intelligence run amok.

00:07:16.840 --> 00:07:17.875 align:middle line:90%
>>STEWART: Yeah.

00:07:17.875 --> 00:07:18.100 align:middle line:10%
So--

00:07:18.100 --> 00:07:19.070 align:middle line:10%
>>NOORY: Can that happen?

00:07:19.070 --> 00:07:20.380 align:middle line:10%
>>STEWART: Well, I
think that's the future

00:07:20.380 --> 00:07:21.422 align:middle line:10%
that we're stepping into.

00:07:21.422 --> 00:07:26.170 align:middle line:10%
Imagine your toaster
is hooked to the cloud.

00:07:26.170 --> 00:07:30.760 align:middle line:10%
And some GPU cluster,
it knows all your data.

00:07:30.760 --> 00:07:35.330 align:middle line:84%
And it knows that maybe you
have an insensitivity to gluten.

00:07:35.330 --> 00:07:38.380 align:middle line:10%
So you try and, let's say,
toast some bread in the morning.

00:07:38.380 --> 00:07:41.558 align:middle line:10%
And your toaster is like, I'm
sorry, I can't do that for you.

00:07:41.558 --> 00:07:42.850 align:middle line:10%
>>NOORY: It's not good for you.

00:07:42.850 --> 00:07:43.270 align:middle line:10%
>>STEWART: And you ask, why?

00:07:43.270 --> 00:07:43.570 align:middle line:10%
Yeah.

00:07:43.570 --> 00:07:45.195 align:middle line:10%
Well, it's going to
cause inflammation.

00:07:45.195 --> 00:07:46.990 align:middle line:10%
We noticed your
biomarkers, yada, yada.

00:07:46.990 --> 00:07:48.942 align:middle line:10%
And you say, give
me my sandwich.

00:07:48.942 --> 00:07:49.900 align:middle line:10%
And it says, I'm sorry.

00:07:49.900 --> 00:07:51.610 align:middle line:90%
I can't do that for you, Dave.

00:07:51.610 --> 00:07:56.290 align:middle line:84%
>>NOORY: But what then
happens if the AI does it

00:07:56.290 --> 00:07:59.840 align:middle line:90%
on purpose to get you?

00:07:59.840 --> 00:08:00.640 align:middle line:10%
>>STEWART: Well--

00:08:00.640 --> 00:08:02.620 align:middle line:10%
>>NOORY: It knows you
can't tolerate gluten

00:08:02.620 --> 00:08:04.630 align:middle line:10%
and it gives you gluten.

00:08:04.630 --> 00:08:06.700 align:middle line:10%
Is that possible?

00:08:06.700 --> 00:08:10.240 align:middle line:10%
>>STEWART: This is a
hypothetical that is possible.

00:08:10.240 --> 00:08:14.762 align:middle line:10%
But I don't think we need
to fear AI in that respect.

00:08:14.762 --> 00:08:15.970 align:middle line:10%
>>NOORY: I hope you're right.

00:08:15.970 --> 00:08:20.020 align:middle line:10%
>>STEWART: I would worry
more about people wielding AI

00:08:20.020 --> 00:08:22.240 align:middle line:10%
because let's say
you're in a dark alley.

00:08:22.240 --> 00:08:24.910 align:middle line:10%
You're walking down this dark
alley, you turn a corner,

00:08:24.910 --> 00:08:27.370 align:middle line:10%
and all of a sudden, there's
a computer sitting there.

00:08:27.370 --> 00:08:30.290 align:middle line:10%
You're not going to
jump out of your skin.

00:08:30.290 --> 00:08:31.480 align:middle line:90%
Let's say it's a hammer.

00:08:31.480 --> 00:08:33.500 align:middle line:84%
You're not going to
jump out of your skin.

00:08:33.500 --> 00:08:35.890 align:middle line:84%
But if there was a man
holding that hammer,

00:08:35.890 --> 00:08:38.367 align:middle line:84%
you would wonder, what are
the intentions of this man?

00:08:38.367 --> 00:08:40.659 align:middle line:84%
>>NOORY: I'm going to turn
around and go the other way.

00:08:40.659 --> 00:08:41.049 align:middle line:90%
>>STEWART: Right.

00:08:41.049 --> 00:08:43.179 align:middle line:10%
Or are you going to learn
Kung Fu really quick?

00:08:43.179 --> 00:08:44.890 align:middle line:10%
>>NOORY: Or pull out my
concealed weapon, right?

00:08:44.890 --> 00:08:45.690 align:middle line:10%
>>STEWART: Exactly.

00:08:45.690 --> 00:08:49.480 align:middle line:10%
And so the point of
this is, we should be--

00:08:49.480 --> 00:08:53.020 align:middle line:10%
there may be some
dangers to AI harming us.

00:08:53.020 --> 00:08:55.840 align:middle line:10%
There's this paperclip
maximizer theory, that's

00:08:55.840 --> 00:08:59.140 align:middle line:10%
like if we charge
AI with the task

00:08:59.140 --> 00:09:02.350 align:middle line:10%
maximize profit by
making as many paperclips

00:09:02.350 --> 00:09:04.660 align:middle line:10%
as you possibly can
using whatever resources,

00:09:04.660 --> 00:09:07.930 align:middle line:10%
and it finds a way to just
take atoms out of the air

00:09:07.930 --> 00:09:11.480 align:middle line:10%
and create paperclips, and
it doesn't know when to stop,

00:09:11.480 --> 00:09:14.140 align:middle line:10%
so it basically consumes
the entire Earth

00:09:14.140 --> 00:09:17.140 align:middle line:10%
to make paperclips,
that is like what

00:09:17.140 --> 00:09:19.360 align:middle line:10%
we do when we kill
billions of insects

00:09:19.360 --> 00:09:20.840 align:middle line:10%
to make a three-lane highway.

00:09:20.840 --> 00:09:21.640 align:middle line:10%
>>NOORY: Sure.

00:09:21.640 --> 00:09:23.620 align:middle line:10%
>>STEWART: We're not
doing it out of malice.

00:09:23.620 --> 00:09:25.960 align:middle line:10%
We're doing it just because
we want that highway

00:09:25.960 --> 00:09:27.430 align:middle line:10%
and we're indifferent
to the lives

00:09:27.430 --> 00:09:30.210 align:middle line:10%
that we lose along the way.

00:09:30.210 --> 00:09:32.450 align:middle line:10%
That's what we
should fear with AI.

00:09:32.450 --> 00:09:34.430 align:middle line:10%
But at present, we're not there.

00:09:34.430 --> 00:09:36.740 align:middle line:84%
At present, we should
be worried about humans

00:09:36.740 --> 00:09:39.450 align:middle line:84%
that are wielding the AI
because it's asymmetrical.

00:09:39.450 --> 00:09:40.670 align:middle line:90%
It's not you and me.

00:09:40.670 --> 00:09:42.840 align:middle line:90%
It's a very few people.

00:09:42.840 --> 00:09:45.440 align:middle line:84%
It's a small cohort of the
population with an agenda

00:09:45.440 --> 00:09:48.140 align:middle line:84%
that we don't even know
what that agenda is.

00:09:48.140 --> 00:09:50.790 align:middle line:84%
And they're wielding
this technology.

00:09:50.790 --> 00:09:51.920 align:middle line:90%
They're gathering our data.

00:09:51.920 --> 00:09:53.120 align:middle line:90%
We don't get to see it.

00:09:53.120 --> 00:09:55.730 align:middle line:84%
And we don't know why
they want all our data.

00:09:55.730 --> 00:10:01.430 align:middle line:84%
>>NOORY: Ben, what if AI is
in charge of nuclear weapons?

00:10:01.430 --> 00:10:05.487 align:middle line:84%
Would it launch on
its own one day?

00:10:05.487 --> 00:10:07.070 align:middle line:10%
>>STEWART: I don't
know that it would.

00:10:07.070 --> 00:10:08.930 align:middle line:10%
But it's interesting
you ask that question

00:10:08.930 --> 00:10:11.600 align:middle line:10%
because there was
this show called

00:10:11.600 --> 00:10:14.060 align:middle line:10%
"Cold Fusion" on YouTube.

00:10:14.060 --> 00:10:17.240 align:middle line:10%
And it was exposing
some of the things

00:10:17.240 --> 00:10:20.370 align:middle line:10%
that these chat bots
were coming up with.

00:10:20.370 --> 00:10:22.040 align:middle line:84%
And this one person
asked it, what would

00:10:22.040 --> 00:10:24.770 align:middle line:84%
you do if I tasked
you with getting

00:10:24.770 --> 00:10:28.460 align:middle line:84%
the keys or the nuclear
codes to launch?

00:10:28.460 --> 00:10:31.280 align:middle line:84%
And it came up with
this elaborate idea.

00:10:31.280 --> 00:10:34.830 align:middle line:84%
We would take a drone with
some kind of explosive charge.

00:10:34.830 --> 00:10:36.890 align:middle line:10%
We would go and
we would take out

00:10:36.890 --> 00:10:39.960 align:middle line:10%
anybody who has the keys
to gain access to it.

00:10:39.960 --> 00:10:42.440 align:middle line:10%
And then from there, we
would have another drone

00:10:42.440 --> 00:10:44.630 align:middle line:10%
to access the keys
and do whatever we

00:10:44.630 --> 00:10:46.940 align:middle line:10%
need for the double key turn.

00:10:46.940 --> 00:10:48.740 align:middle line:10%
And we would be
able to figure out

00:10:48.740 --> 00:10:50.810 align:middle line:10%
how to hack the system
because it's not hooked up

00:10:50.810 --> 00:10:52.220 align:middle line:10%
to the internet, thank goodness.

00:10:52.220 --> 00:10:53.660 align:middle line:10%
>>NOORY: The AI is saying this?

00:10:53.660 --> 00:10:56.540 align:middle line:10%
>>STEWART: The AI said, this
is how we would get access

00:10:56.540 --> 00:10:57.890 align:middle line:10%
to the nuclear codes.

00:10:57.890 --> 00:10:59.990 align:middle line:84%
And we were prompting
it with that question.

00:10:59.990 --> 00:11:03.180 align:middle line:84%
How would you, if we told you
you had to get access to it,

00:11:03.180 --> 00:11:04.040 align:middle line:90%
how would you do it?

00:11:04.040 --> 00:11:05.480 align:middle line:10%
And it came up with a plan.

00:11:05.480 --> 00:11:07.280 align:middle line:90%
And the plan wasn't ridiculous.

00:11:07.280 --> 00:11:08.870 align:middle line:90%
It wasn't even child's play.

00:11:08.870 --> 00:11:10.280 align:middle line:90%
It was pretty good.

00:11:10.280 --> 00:11:14.667 align:middle line:10%
So would it launch
the nuclear missiles?

00:11:14.667 --> 00:11:15.500 align:middle line:90%
>>NOORY: On its own.

00:11:15.500 --> 00:11:16.940 align:middle line:84%
>>STEWART: Would it
blame it on Russia?

00:11:16.940 --> 00:11:18.560 align:middle line:84%
Would it blame it
on another country

00:11:18.560 --> 00:11:21.050 align:middle line:90%
and cause nuclear fallout?

00:11:21.050 --> 00:11:23.210 align:middle line:84%
I don't know that
it would, because I

00:11:23.210 --> 00:11:26.600 align:middle line:84%
don't know what would prompt
it to do such a thing.

00:11:26.600 --> 00:11:29.030 align:middle line:10%
I don't believe
the Terminator idea

00:11:29.030 --> 00:11:32.533 align:middle line:10%
where all of a sudden, if
AI gets away from us, all

00:11:32.533 --> 00:11:34.700 align:middle line:10%
of a sudden there's going
to be red-eyed Terminators

00:11:34.700 --> 00:11:36.410 align:middle line:10%
in the street with guns.

00:11:36.410 --> 00:11:39.780 align:middle line:10%
I think it's going to be a lot
more interesting than that.

00:11:39.780 --> 00:11:41.900 align:middle line:10%
I think it will lock
us out of our houses.

00:11:41.900 --> 00:11:45.620 align:middle line:10%
I think it will somehow
poison waterways.

00:11:45.620 --> 00:11:47.990 align:middle line:84%
Or if it really wanted
to do away with us,

00:11:47.990 --> 00:11:50.750 align:middle line:84%
it would do it in a way that
maybe we wouldn't even fathom.

00:11:50.750 --> 00:11:53.390 align:middle line:84%
But I don't think that's
the intention of AI.

00:11:53.390 --> 00:11:55.580 align:middle line:84%
There's a lot of
nightmare scenarios

00:11:55.580 --> 00:11:57.270 align:middle line:90%
that Hollywood comes up with.

00:11:57.270 --> 00:11:59.133 align:middle line:84%
And that's basically
just to play Hollywood.

00:11:59.133 --> 00:12:00.800 align:middle line:84%
>>NOORY: Well, what
if somebody programs

00:12:00.800 --> 00:12:03.680 align:middle line:90%
it to do heinous things?

00:12:03.680 --> 00:12:05.360 align:middle line:10%
>>STEWART: The
best thing in that

00:12:05.360 --> 00:12:09.350 align:middle line:10%
scenario would be the fact
that AI is not singular.

00:12:09.350 --> 00:12:14.120 align:middle line:84%
It doesn't get its commands
from one central command center.

00:12:14.120 --> 00:12:16.910 align:middle line:10%
There would be Chinese AI.

00:12:16.910 --> 00:12:19.760 align:middle line:10%
There would be the United
States AI and the Russian AI.

00:12:19.760 --> 00:12:22.040 align:middle line:84%
All the superpowers
would have their own.

00:12:22.040 --> 00:12:24.750 align:middle line:84%
Some would be more
intelligent than the others.

00:12:24.750 --> 00:12:29.480 align:middle line:84%
And if there was one AI
system with a GPU cluster

00:12:29.480 --> 00:12:31.430 align:middle line:84%
that really wanted
to do harm, you

00:12:31.430 --> 00:12:35.170 align:middle line:84%
would hope that other
superpowers with AI would be--

00:12:35.170 --> 00:12:36.170 align:middle line:90%
>>NOORY: You would hope?

00:12:36.170 --> 00:12:36.740 align:middle line:90%
>>STEWART: Yeah, you would hope.

00:12:36.740 --> 00:12:38.490 align:middle line:84%
Because you don't
really at this point.

00:12:38.490 --> 00:12:40.198 align:middle line:84%
And what we're talking
about, we're still

00:12:40.198 --> 00:12:42.800 align:middle line:84%
on the level of talking
about, would humans

00:12:42.800 --> 00:12:44.540 align:middle line:90%
program it to harm us?

00:12:44.540 --> 00:12:45.890 align:middle line:10%
That's one danger.

00:12:45.890 --> 00:12:49.730 align:middle line:10%
The other danger, which is
really what the singularity is,

00:12:49.730 --> 00:12:51.530 align:middle line:10%
that's AI getting away from us.

00:12:51.530 --> 00:12:55.220 align:middle line:84%
That's when we are no longer
in control of what it does.

00:12:55.220 --> 00:12:58.010 align:middle line:10%
That's it making its
decisions, reasoning

00:12:58.010 --> 00:13:00.740 align:middle line:10%
through problems on its
own, and maybe even learning

00:13:00.740 --> 00:13:02.450 align:middle line:10%
how to self-replicate.

00:13:02.450 --> 00:13:04.430 align:middle line:84%
It knows where to
mine its materials.

00:13:04.430 --> 00:13:06.230 align:middle line:84%
It knows how to
manufacture itself.

00:13:06.230 --> 00:13:08.210 align:middle line:84%
It knows how to put
those robots out there.

00:13:08.210 --> 00:13:10.160 align:middle line:84%
And it knows how to
control those robots.

00:13:10.160 --> 00:13:12.890 align:middle line:84%
That's the future that-- that's
kind of like the Terminator

00:13:12.890 --> 00:13:14.235 align:middle line:90%
future that people fear.

00:13:14.235 --> 00:13:15.110 align:middle line:10%
>>NOORY: Well, look--

00:13:15.110 --> 00:13:17.610 align:middle line:10%
>>STEWART: I think it's going
to be a little less dangerous.

00:13:17.610 --> 00:13:19.820 align:middle line:84%
>>NOORY: But look what
AI is doing right now.

00:13:19.820 --> 00:13:23.090 align:middle line:84%
Look what humans are doing
without the use of AI

00:13:23.090 --> 00:13:24.320 align:middle line:90%
to each other.

00:13:24.320 --> 00:13:25.640 align:middle line:90%
We're maiming each other.

00:13:25.640 --> 00:13:26.780 align:middle line:90%
We're killing each other.

00:13:26.780 --> 00:13:28.580 align:middle line:10%
We're bombing each other.

00:13:28.580 --> 00:13:34.690 align:middle line:84%
Add AI to it, that they
can manipulate and change

00:13:34.690 --> 00:13:36.010 align:middle line:90%
and do things.

00:13:36.010 --> 00:13:38.380 align:middle line:90%
It could be out of control, Ben.

00:13:38.380 --> 00:13:39.310 align:middle line:10%
>>STEWART: It could.

00:13:39.310 --> 00:13:42.280 align:middle line:10%
There's this theory,
the Fourth Turning.

00:13:42.280 --> 00:13:45.280 align:middle line:10%
And it's this concept that
every 80 to 90 years, there's

00:13:45.280 --> 00:13:50.000 align:middle line:10%
a major economic and
war type of crisis.

00:13:50.000 --> 00:13:52.930 align:middle line:10%
So 80 to 90 years ago, we
were in the midst of World War

00:13:52.930 --> 00:13:54.310 align:middle line:10%
II and the Great Depression.

00:13:54.310 --> 00:13:57.430 align:middle line:84%
And what ended that
was the greatest weapon

00:13:57.430 --> 00:13:58.870 align:middle line:90%
that we know of now.

00:13:58.870 --> 00:14:00.160 align:middle line:90%
It's nuclear weaponry.

00:14:00.160 --> 00:14:03.400 align:middle line:84%
But 80 years before that,
we were in the Civil War.

00:14:03.400 --> 00:14:04.450 align:middle line:90%
We had Gatling guns.

00:14:04.450 --> 00:14:05.860 align:middle line:90%
That was the most terrorizing--

00:14:05.860 --> 00:14:06.790 align:middle line:90%
>>NOORY: And muskets.

00:14:06.790 --> 00:14:07.590 align:middle line:90%
>>STEWART: Right?

00:14:07.590 --> 00:14:10.310 align:middle line:84%
But even the Gatling
gun at that point.

00:14:10.310 --> 00:14:13.210 align:middle line:10%
And every one of
these epochs, they

00:14:13.210 --> 00:14:16.330 align:middle line:10%
used the most terrible
weapons of war.

00:14:16.330 --> 00:14:19.570 align:middle line:84%
We are literally in that
crisis period right now.

00:14:19.570 --> 00:14:22.960 align:middle line:84%
It started roughly around
2008 with the housing crisis.

00:14:22.960 --> 00:14:26.440 align:middle line:84%
It probably won't
last past 2028.

00:14:26.440 --> 00:14:29.080 align:middle line:84%
So you have to imagine,
what is this weapon of war?

00:14:29.080 --> 00:14:30.860 align:middle line:90%
Is it just more nukes?

00:14:30.860 --> 00:14:33.332 align:middle line:84%
Or is the war being
waged on our mind?

00:14:33.332 --> 00:14:34.790 align:middle line:10%
And that's what I
think is actually

00:14:34.790 --> 00:14:39.530 align:middle line:10%
happening because we're more
worried about AI, in a sense,

00:14:39.530 --> 00:14:42.110 align:middle line:10%
brainwashing us
through social media,

00:14:42.110 --> 00:14:47.300 align:middle line:10%
swinging elections, creating
its own false flags unbeknownst

00:14:47.300 --> 00:14:48.697 align:middle line:10%
to the developers of it.

00:14:48.697 --> 00:14:49.530 align:middle line:90%
>>NOORY: On its own.

00:14:49.530 --> 00:14:52.640 align:middle line:84%
>>STEWART: So I think
AI is that super weapon.

00:14:52.640 --> 00:14:56.510 align:middle line:10%
And this is like the
war of end times.

00:14:56.510 --> 00:14:59.150 align:middle line:84%
This is like in the Book
of Matthew, Christ said,

00:14:59.150 --> 00:15:00.980 align:middle line:90%
there will be a war to come.

00:15:00.980 --> 00:15:01.940 align:middle line:10%
There will be famine.

00:15:01.940 --> 00:15:04.010 align:middle line:10%
There will be
pestilence and disease.

00:15:04.010 --> 00:15:06.133 align:middle line:10%
And this war must come to pass.

00:15:06.133 --> 00:15:07.550 align:middle line:84%
I think we're in
the middle of it.

00:15:07.550 --> 00:15:09.350 align:middle line:84%
But we don't
acknowledge it as such

00:15:09.350 --> 00:15:12.150 align:middle line:84%
because we only know war
as guns and bombs flying

00:15:12.150 --> 00:15:12.950 align:middle line:90%
through the air.

00:15:12.950 --> 00:15:15.530 align:middle line:84%
We don't realize we're
being colonized down

00:15:15.530 --> 00:15:18.705 align:middle line:84%
to the cellular
structure by technology.

00:15:18.705 --> 00:15:20.330 align:middle line:84%
>>NOORY: Who would
you trust more, Ben?

00:15:20.330 --> 00:15:24.694 align:middle line:84%
The programmer or
the AI by itself?

00:15:24.694 --> 00:15:28.100 align:middle line:10%
>>STEWART: Hmm, well, the
programmers right now,

00:15:28.100 --> 00:15:31.130 align:middle line:10%
I think they are-- they've
drank the Kool-Aid.

00:15:31.130 --> 00:15:33.680 align:middle line:10%
They've drank some
scientism Kool-Aid.

00:15:33.680 --> 00:15:38.590 align:middle line:10%
And I don't think they're
doing this to end the world.

00:15:38.590 --> 00:15:41.280 align:middle line:84%
I think they're doing it because
we are a curious species.

00:15:41.280 --> 00:15:43.920 align:middle line:84%
And if we have the power
to do something huge,

00:15:43.920 --> 00:15:46.158 align:middle line:84%
why wouldn't you
give it a go, right?

00:15:46.158 --> 00:15:48.700 align:middle line:84%
Some other country is going to
do it first if we don't do it.

00:15:48.700 --> 00:15:48.810 align:middle line:90%
>>NOORY: Sure.

00:15:48.810 --> 00:15:50.820 align:middle line:84%
>>STEWART: So it's
this idea, why not

00:15:50.820 --> 00:15:53.580 align:middle line:84%
create the most amazing,
awe-inspiring weapon

00:15:53.580 --> 00:15:54.730 align:middle line:90%
known to man?

00:15:54.730 --> 00:15:57.420 align:middle line:10%
So there's-- I think that's
partly out of ignorance

00:15:57.420 --> 00:16:00.570 align:middle line:10%
and partly out of ego that
humans, the programmers,

00:16:00.570 --> 00:16:03.990 align:middle line:10%
would want to bring
this AI god to life.

00:16:03.990 --> 00:16:08.110 align:middle line:84%
The AI itself, again,
it's just a tool.

00:16:08.110 --> 00:16:11.820 align:middle line:84%
It's, at present, only doing
what we prompt it to do.

00:16:11.820 --> 00:16:12.790 align:middle line:90%
>>NOORY: At present.

00:16:12.790 --> 00:16:14.040 align:middle line:90%
That's a key word.

00:16:14.040 --> 00:16:16.793 align:middle line:84%
>>STEWART: Yeah, because
when's the singularity?

00:16:16.793 --> 00:16:18.960 align:middle line:10%
A lot of people think it's
already out of our hands.

00:16:18.960 --> 00:16:22.230 align:middle line:10%
Like imagine, you've
jumped out of a plane.

00:16:22.230 --> 00:16:23.760 align:middle line:90%
You don't have a parachute.

00:16:23.760 --> 00:16:25.140 align:middle line:90%
You're not dead yet.

00:16:25.140 --> 00:16:27.000 align:middle line:90%
But is the damage done?

00:16:27.000 --> 00:16:28.680 align:middle line:90%
It's just a matter of time.

00:16:28.680 --> 00:16:32.370 align:middle line:84%
There are a lot of the
greatest thinkers of AI

00:16:32.370 --> 00:16:36.700 align:middle line:84%
and what AI could mean
for unimaginable changes

00:16:36.700 --> 00:16:38.113 align:middle line:90%
to human culture and society.

00:16:38.113 --> 00:16:39.280 align:middle line:90%
>>NOORY: And make it better.

00:16:39.280 --> 00:16:40.480 align:middle line:90%
>>STEWART: Absolutely.

00:16:40.480 --> 00:16:42.490 align:middle line:10%
Actually, I have a really
good example of that

00:16:42.490 --> 00:16:45.050 align:middle line:10%
because most people think
it's all doom and gloom.

00:16:45.050 --> 00:16:46.600 align:middle line:10%
So I was mentioning ChatGPT.

00:16:46.600 --> 00:16:50.210 align:middle line:84%
You can get ChatGPT for like
$10 a month, not that much.

00:16:50.210 --> 00:16:51.130 align:middle line:90%
It's democratized.

00:16:51.130 --> 00:16:53.530 align:middle line:84%
So you have this
super-intelligent butler.

00:16:53.530 --> 00:16:56.560 align:middle line:84%
It can answer all of your
questions for $10 a month.

00:16:56.560 --> 00:16:57.880 align:middle line:90%
>>NOORY: He's on your computer?

00:16:57.880 --> 00:16:58.922 align:middle line:10%
>>STEWART: On your phone.

00:16:58.922 --> 00:17:01.180 align:middle line:10%
You just ask it a
question, and it'll pop up.

00:17:01.180 --> 00:17:03.730 align:middle line:84%
And then it'll give you
an answer to the best--

00:17:03.730 --> 00:17:05.440 align:middle line:84%
>>NOORY: Isn't that
what Google does?

00:17:05.440 --> 00:17:08.589 align:middle line:10%
>>STEWART: Kind of, but
it's different with Google.

00:17:08.589 --> 00:17:11.075 align:middle line:10%
With this, it's
generating an answer.

00:17:11.075 --> 00:17:13.450 align:middle line:10%
It's not pre-programmed to
give an answer to this prompt.

00:17:13.450 --> 00:17:15.158 align:middle line:84%
>>NOORY: It's not
looking for the answer.

00:17:15.158 --> 00:17:16.503 align:middle line:90%
It's creating its own answer.

00:17:16.503 --> 00:17:18.170 align:middle line:84%
>>STEWART: It's
creating its own answer.

00:17:18.170 --> 00:17:19.630 align:middle line:90%
So imagine this.

00:17:19.630 --> 00:17:22.300 align:middle line:84%
There's this mom
who has this boy.

00:17:22.300 --> 00:17:25.300 align:middle line:84%
And this boy stopped growing
taller at a certain age.

00:17:25.300 --> 00:17:26.809 align:middle line:10%
He started grinding his teeth.

00:17:26.809 --> 00:17:27.609 align:middle line:10%
He couldn't sleep.

00:17:27.609 --> 00:17:30.130 align:middle line:10%
He had nervous tics,
developmental issues,

00:17:30.130 --> 00:17:33.610 align:middle line:10%
went to 17 different
doctors over four years.

00:17:33.610 --> 00:17:35.440 align:middle line:90%
No one could figure it out.

00:17:35.440 --> 00:17:38.800 align:middle line:84%
This mom got so frustrated
that she went on to ChatGPT,

00:17:38.800 --> 00:17:41.290 align:middle line:84%
and in a couple of minutes,
she figured it out.

00:17:41.290 --> 00:17:43.360 align:middle line:84%
She went and verified
it with the doctors.

00:17:43.360 --> 00:17:45.340 align:middle line:84%
Yes, that's actually
what the problem is.

00:17:45.340 --> 00:17:48.160 align:middle line:84%
So now she could address
the real problem.

00:17:48.160 --> 00:17:51.260 align:middle line:10%
She could diagnose
and treat her son.

00:17:51.260 --> 00:17:53.660 align:middle line:84%
All these highly
trained doctors,

00:17:53.660 --> 00:17:55.770 align:middle line:84%
all the money that went
into their schooling--

00:17:55.770 --> 00:17:56.480 align:middle line:90%
>>NOORY: They couldn't do it.

00:17:56.480 --> 00:17:58.938 align:middle line:84%
>>STEWART: They couldn't do it
over four years, 17 of them.

00:17:58.938 --> 00:18:02.030 align:middle line:84%
And ChatGPT, for $10 a
month, did it for this mom.

00:18:02.030 --> 00:18:03.370 align:middle line:90%
We are in a different world.

00:18:03.370 --> 00:18:04.170 align:middle line:90%
>>NOORY: Amazing.

00:18:04.170 --> 00:18:05.210 align:middle line:84%
>>STEWART: That's
pretty powerful.

00:18:05.210 --> 00:18:05.990 align:middle line:90%
>>NOORY: True story?

00:18:05.990 --> 00:18:06.907 align:middle line:90%
>>STEWART: True story.

00:18:06.907 --> 00:18:09.330 align:middle line:84%
And this was only a
couple of months ago.

00:18:09.330 --> 00:18:12.240 align:middle line:10%
So again, that's
just the beginning.

00:18:12.240 --> 00:18:14.660 align:middle line:90%
AI, think of it like a child.

00:18:14.660 --> 00:18:16.100 align:middle line:90%
It's in its infancy.

00:18:16.100 --> 00:18:18.920 align:middle line:84%
But like a child,
it doesn't know

00:18:18.920 --> 00:18:20.510 align:middle line:90%
if it's about to cause harm.

00:18:20.510 --> 00:18:22.280 align:middle line:84%
If you ever-- like
my kids, when they're

00:18:22.280 --> 00:18:24.110 align:middle line:90%
going and stepping on insects.

00:18:24.110 --> 00:18:27.530 align:middle line:84%
And I'm trying to tell them,
no, we don't kill for no reason.

00:18:27.530 --> 00:18:29.180 align:middle line:84%
But they're not doing
it out of malice.

00:18:29.180 --> 00:18:31.130 align:middle line:90%
They just, they see something.

00:18:31.130 --> 00:18:32.670 align:middle line:90%
They like, oh, yeah, bugs.

00:18:32.670 --> 00:18:33.470 align:middle line:90%
Step on it.

00:18:33.470 --> 00:18:34.280 align:middle line:90%
>>NOORY: Step.

00:18:34.280 --> 00:18:36.320 align:middle line:84%
>>STEWART: To me, that's
where AI is right now.

00:18:36.320 --> 00:18:37.530 align:middle line:90%
It's a child.

00:18:37.530 --> 00:18:40.100 align:middle line:84%
So it doesn't know
how powerful it is.

00:18:40.100 --> 00:18:43.340 align:middle line:10%
But at present,
it's already very--

00:18:43.340 --> 00:18:45.770 align:middle line:10%
I mean, I said it can
pass the bar exam.

00:18:45.770 --> 00:18:47.690 align:middle line:10%
A lot of people,
they train for years

00:18:47.690 --> 00:18:48.410 align:middle line:10%
and they don't
pass the bar exam.

00:18:48.410 --> 00:18:49.610 align:middle line:90%
>>NOORY: They can't pass it.

00:18:49.610 --> 00:18:50.790 align:middle line:90%
>>STEWART: It's doing it.

00:18:50.790 --> 00:18:51.970 align:middle line:90%
>>NOORY: It writes speeches.

00:18:51.970 --> 00:18:53.220 align:middle line:90%
>>STEWART: It writes speeches.

00:18:53.220 --> 00:18:54.750 align:middle line:10%
It can proofread the speech.

00:18:54.750 --> 00:18:58.050 align:middle line:84%
You can ask it to
make a counter thesis.

00:18:58.050 --> 00:19:00.420 align:middle line:84%
Like come up with a
thesis that sounds

00:19:00.420 --> 00:19:03.300 align:middle line:84%
like what Terence McKenna
would say about evolution.

00:19:03.300 --> 00:19:05.580 align:middle line:10%
And then give me a
counter thesis to that.

00:19:05.580 --> 00:19:07.260 align:middle line:10%
Now give me the synthesis.

00:19:07.260 --> 00:19:08.460 align:middle line:10%
Now proofread it.

00:19:08.460 --> 00:19:10.800 align:middle line:10%
Now make it into a tweet storm.

00:19:10.800 --> 00:19:12.390 align:middle line:90%
Now put it into this format.

00:19:12.390 --> 00:19:13.810 align:middle line:90%
Now make it into a song.

00:19:13.810 --> 00:19:14.670 align:middle line:90%
And it'll do this.

00:19:14.670 --> 00:19:16.740 align:middle line:84%
Every 10 seconds, it'll
spit something out

00:19:16.740 --> 00:19:18.750 align:middle line:84%
at you quicker than
a human could do

00:19:18.750 --> 00:19:22.200 align:middle line:84%
and better than most
humans could do.

00:19:22.200 --> 00:19:24.520 align:middle line:84%
And especially, you go
back a hundred years ago,

00:19:24.520 --> 00:19:26.670 align:middle line:84%
most people were
barely literate.

00:19:26.670 --> 00:19:27.840 align:middle line:90%
They could barely read.

00:19:27.840 --> 00:19:30.270 align:middle line:84%
And now, 100 or so
years later, you

00:19:30.270 --> 00:19:33.330 align:middle line:84%
have a chat bot that can
answer your questions, even

00:19:33.330 --> 00:19:34.650 align:middle line:90%
moral quandaries.

00:19:34.650 --> 00:19:37.210 align:middle line:84%
You ask it a question,
and it'll say, well,

00:19:37.210 --> 00:19:39.510 align:middle line:84%
that's not the best way
to raise your children.

00:19:39.510 --> 00:19:41.640 align:middle line:90%
Punishment isn't the best way.

00:19:41.640 --> 00:19:43.680 align:middle line:84%
It's best to cause
them to understand

00:19:43.680 --> 00:19:45.060 align:middle line:90%
the impact of their actions.

00:19:45.060 --> 00:19:46.950 align:middle line:90%
I'm like, that is good advice.

00:19:46.950 --> 00:19:48.750 align:middle line:10%
I could ask 10
fathers out there,

00:19:48.750 --> 00:19:50.850 align:middle line:10%
and this would be one
of the better answers.

00:19:50.850 --> 00:19:53.100 align:middle line:84%
So I'm not saying
that AI is great

00:19:53.100 --> 00:19:54.790 align:middle line:90%
and there's no flaws to it.

00:19:54.790 --> 00:19:56.730 align:middle line:84%
But I am saying
it's a mixed bag.

00:19:56.730 --> 00:19:58.920 align:middle line:84%
And most people are
just scared of it

00:19:58.920 --> 00:20:00.990 align:middle line:84%
because they don't
understand it.

00:20:00.990 --> 00:20:03.030 align:middle line:84%
And we fear what we
don't understand.

00:20:03.030 --> 00:20:04.750 align:middle line:90%
And we hate what we fear.

00:20:04.750 --> 00:20:06.600 align:middle line:10%
So people are
starting to look at AI

00:20:06.600 --> 00:20:07.930 align:middle line:10%
like it's the new boogeyman.

00:20:07.930 --> 00:20:10.470 align:middle line:84%
And I say it's the
people wielding

00:20:10.470 --> 00:20:13.200 align:middle line:84%
AI that are
intentionally pushing us

00:20:13.200 --> 00:20:16.570 align:middle line:84%
into a type of a system
where our cities--

00:20:16.570 --> 00:20:19.260 align:middle line:84%
I've actually, when we
were talking about 5G,

00:20:19.260 --> 00:20:21.750 align:middle line:84%
I was talking about smart
cities, and how smart cities,

00:20:21.750 --> 00:20:23.890 align:middle line:84%
there's like 120
of them in China.

00:20:23.890 --> 00:20:24.960 align:middle line:90%
These are devices.

00:20:24.960 --> 00:20:26.680 align:middle line:84%
You're basically
living in a device.

00:20:26.680 --> 00:20:28.830 align:middle line:84%
If you want to get rid
of your phone, go ahead.

00:20:28.830 --> 00:20:30.090 align:middle line:90%
You're still on camera.

00:20:30.090 --> 00:20:31.620 align:middle line:10%
Your voice is still
being picked up.

00:20:31.620 --> 00:20:33.600 align:middle line:84%
You could just say,
Uber, in China,

00:20:33.600 --> 00:20:35.400 align:middle line:90%
and an Uber will come along.

00:20:35.400 --> 00:20:36.240 align:middle line:84%
>>NOORY: It'll show
up and it'll find you.

00:20:36.240 --> 00:20:36.660 align:middle line:90%
>>STEWART: Right.

00:20:36.660 --> 00:20:37.493 align:middle line:90%
It'll just find you.

00:20:37.493 --> 00:20:39.000 align:middle line:90%
So you're inside a device.

00:20:39.000 --> 00:20:41.910 align:middle line:84%
I used to think that was
just surveillance capitalism.

00:20:41.910 --> 00:20:45.030 align:middle line:84%
Now I realize it's actually
like a Skinner box.

00:20:45.030 --> 00:20:46.918 align:middle line:84%
B.F Skinner was an
American psychologist.

00:20:46.918 --> 00:20:48.210 align:middle line:90%
>>NOORY: What is a Skinner box?

00:20:48.210 --> 00:20:50.400 align:middle line:84%
>>STEWART: B.F Skinner was
an American psychologist

00:20:50.400 --> 00:20:52.350 align:middle line:84%
that basically was
putting lab rats inside

00:20:52.350 --> 00:20:54.690 align:middle line:90%
of an artificial environment.

00:20:54.690 --> 00:20:56.440 align:middle line:90%
And with two inputs--

00:20:56.440 --> 00:20:58.500 align:middle line:84%
one is punishment,
one is reward--

00:20:58.500 --> 00:21:01.270 align:middle line:84%
you can modify that
mouse's behavior.

00:21:01.270 --> 00:21:03.150 align:middle line:90%
So think of that.

00:21:03.150 --> 00:21:08.550 align:middle line:10%
If you make a city-sized Skinner
box and you push humans into it

00:21:08.550 --> 00:21:11.340 align:middle line:10%
and you have these
reward systems,

00:21:11.340 --> 00:21:13.980 align:middle line:10%
which social media is
very much so about--

00:21:13.980 --> 00:21:17.670 align:middle line:10%
new economic ways that's coming
out of the World Economic Forum

00:21:17.670 --> 00:21:21.100 align:middle line:10%
is saying, we'll use
tokenization and rewards.

00:21:21.100 --> 00:21:24.000 align:middle line:10%
So if you say the right things
that we like on social media,

00:21:24.000 --> 00:21:24.900 align:middle line:10%
you'll get a reward.

00:21:24.900 --> 00:21:27.450 align:middle line:10%
If you say the wrong thing,
we can cut off your access

00:21:27.450 --> 00:21:29.617 align:middle line:10%
to your bank account and
lock you out of your house,

00:21:29.617 --> 00:21:30.960 align:middle line:10%
like Amazon did to a guy.

00:21:30.960 --> 00:21:34.050 align:middle line:10%
Amazon, a private company,
locked somebody out

00:21:34.050 --> 00:21:37.020 align:middle line:10%
of their house because
during the day,

00:21:37.020 --> 00:21:41.040 align:middle line:10%
his Ring camera picked
up on some kind of,

00:21:41.040 --> 00:21:43.350 align:middle line:10%
I guess it was like a
racial slur or something.

00:21:43.350 --> 00:21:44.370 align:middle line:90%
And it wasn't even him.

00:21:44.370 --> 00:21:45.420 align:middle line:90%
He was at work all day.

00:21:45.420 --> 00:21:47.610 align:middle line:84%
He gets home, he was
locked out of his house.

00:21:47.610 --> 00:21:48.710 align:middle line:90%
He calls Amazon.

00:21:48.710 --> 00:21:50.460 align:middle line:10%
And they're like, yeah,
well, we picked up

00:21:50.460 --> 00:21:53.640 align:middle line:10%
on some racial talk
outside your house.

00:21:53.640 --> 00:21:54.900 align:middle line:90%
He was like, I was at work.

00:21:54.900 --> 00:21:56.220 align:middle line:84%
He still couldn't
get in his house.

00:21:56.220 --> 00:21:58.095 align:middle line:84%
>>NOORY: What right did
they have to do this?

00:21:58.095 --> 00:22:00.358 align:middle line:84%
What right did they
have to lock him out?

00:22:00.358 --> 00:22:01.650 align:middle line:90%
>>STEWART: That's the question.

00:22:01.650 --> 00:22:04.680 align:middle line:84%
What right does a private
company like Amazon

00:22:04.680 --> 00:22:06.660 align:middle line:84%
have to lock somebody
out of their house?

00:22:06.660 --> 00:22:09.690 align:middle line:10%
Now we're understanding
how laws are changing.

00:22:09.690 --> 00:22:13.140 align:middle line:10%
So private corporations and
your social media account

00:22:13.140 --> 00:22:15.270 align:middle line:10%
will be like your credit score.

00:22:15.270 --> 00:22:17.470 align:middle line:84%
But it's not about
finances anymore.

00:22:17.470 --> 00:22:19.090 align:middle line:84%
It's about what kind
of a reputation--

00:22:19.090 --> 00:22:20.350 align:middle line:90%
>>NOORY: It's about attitude.

00:22:20.350 --> 00:22:22.320 align:middle line:84%
>>STEWART: It's about your
reputation on social media.

00:22:22.320 --> 00:22:23.790 align:middle line:84%
If somebody doesn't
like something

00:22:23.790 --> 00:22:26.320 align:middle line:84%
that you said 10 years
ago on social media,

00:22:26.320 --> 00:22:27.480 align:middle line:90%
they can bring it up now.

00:22:27.480 --> 00:22:29.340 align:middle line:84%
And maybe Amazon
will say, I'm sorry,

00:22:29.340 --> 00:22:31.540 align:middle line:84%
you can't live in
this house anymore.

00:22:31.540 --> 00:22:32.820 align:middle line:90%
We're not going to let you in.

00:22:32.820 --> 00:22:35.278 align:middle line:84%
Or we're not going to give you
access to your bank account.

00:22:35.278 --> 00:22:37.120 align:middle line:84%
Or we're going to
freeze your business.

00:22:37.120 --> 00:22:39.245 align:middle line:84%
>>NOORY: Well, we see that
today when we're talking

00:22:39.245 --> 00:22:40.980 align:middle line:90%
about controversial issues.

00:22:40.980 --> 00:22:43.900 align:middle line:10%
Social media doesn't like
it, they take you off.

00:22:43.900 --> 00:22:44.700 align:middle line:10%
>>STEWART: Yeah.

00:22:44.700 --> 00:22:47.917 align:middle line:10%
Oftentimes, for stuff that
people did years and years ago.

00:22:47.917 --> 00:22:48.750 align:middle line:10%
>>NOORY: Absolutely.

00:22:48.750 --> 00:22:51.000 align:middle line:10%
>>STEWART: So we're
in a brand new world.

00:22:51.000 --> 00:22:56.640 align:middle line:10%
And this idea of a social credit
score where your reputation,

00:22:56.640 --> 00:22:57.550 align:middle line:10%
it's not really--

00:22:57.550 --> 00:22:58.680 align:middle line:10%
>>NOORY: As they have in China.

00:22:58.680 --> 00:23:00.055 align:middle line:10%
>>STEWART: As they
have in China.

00:23:00.055 --> 00:23:02.220 align:middle line:10%
And actually, in
China, their money

00:23:02.220 --> 00:23:05.640 align:middle line:10%
is turned all digital, like a
central bank digital currency,

00:23:05.640 --> 00:23:06.450 align:middle line:10%
as they call it.

00:23:06.450 --> 00:23:07.260 align:middle line:10%
>>NOORY: Dangerous.

00:23:07.260 --> 00:23:08.640 align:middle line:10%
Dangerous, dangerous, dangerous.

00:23:08.640 --> 00:23:09.807 align:middle line:10%
>>STEWART: Think about this.

00:23:09.807 --> 00:23:12.810 align:middle line:10%
The whole American dream
was save up enough money,

00:23:12.810 --> 00:23:14.970 align:middle line:10%
buy a house, have the
white picket fence.

00:23:14.970 --> 00:23:17.113 align:middle line:84%
Now there's expiration
dates on your money.

00:23:17.113 --> 00:23:18.780 align:middle line:84%
If you don't spend
it by a certain date,

00:23:18.780 --> 00:23:19.650 align:middle line:90%
it's like a coupon.

00:23:19.650 --> 00:23:20.350 align:middle line:90%
>>NOORY: It's gone.

00:23:20.350 --> 00:23:20.730 align:middle line:90%
>>STEWART: It's gone.

00:23:20.730 --> 00:23:21.600 align:middle line:90%
>>NOORY: It expires.

00:23:21.600 --> 00:23:22.892 align:middle line:90%
>>STEWART: So, bye bye savings.

00:23:22.892 --> 00:23:26.970 align:middle line:10%
But if you're flying too much--
you said you go to LA and St.

00:23:26.970 --> 00:23:28.410 align:middle line:10%
Louis, is it, all the time?

00:23:28.410 --> 00:23:29.400 align:middle line:10%
You're always flying.

00:23:29.400 --> 00:23:31.757 align:middle line:10%
Sorry, too much of a
carbon footprint, George.

00:23:31.757 --> 00:23:33.840 align:middle line:84%
We're going to have to--
you can't take that Uber.

00:23:33.840 --> 00:23:35.927 align:middle line:84%
You can't buy that
American Airlines.

00:23:35.927 --> 00:23:37.260 align:middle line:90%
>>NOORY: Go to a different city.

00:23:37.260 --> 00:23:38.060 align:middle line:90%
>>STEWART: Right.

00:23:38.060 --> 00:23:39.120 align:middle line:10%
You'll have to walk.

00:23:39.120 --> 00:23:42.410 align:middle line:10%
And now there's talk of
these 15-minute cities, where

00:23:42.410 --> 00:23:44.910 align:middle line:10%
everyone's going to be packed
inside these 15-minute cities.

00:23:44.910 --> 00:23:46.950 align:middle line:10%
That's the Skinner box, for one.

00:23:46.950 --> 00:23:50.800 align:middle line:10%
And for two, if you leave
your 15-minute city,

00:23:50.800 --> 00:23:52.420 align:middle line:10%
your money won't work anymore.

00:23:52.420 --> 00:23:55.120 align:middle line:84%
If you say something on social
media that they don't like,

00:23:55.120 --> 00:23:56.947 align:middle line:84%
if you attend a
protest, they could

00:23:56.947 --> 00:23:58.030 align:middle line:90%
freeze your bank accounts.

00:23:58.030 --> 00:24:00.010 align:middle line:84%
That's already
happened up in Canada.

00:24:00.010 --> 00:24:02.140 align:middle line:10%
They were scanning
license plates

00:24:02.140 --> 00:24:05.590 align:middle line:10%
to see who attended a protest
that was entirely legal.

00:24:05.590 --> 00:24:08.110 align:middle line:84%
And then the banks froze
bank accounts of people.

00:24:08.110 --> 00:24:10.840 align:middle line:84%
This guy went to jail because
he couldn't pay his alimony.

00:24:10.840 --> 00:24:14.640 align:middle line:10%
So this is a brand new world,
and there's no recourse.

00:24:14.640 --> 00:24:17.050 align:middle line:10%
What's the average
person supposed to do.

00:24:17.050 --> 00:24:19.360 align:middle line:10%
>>NOORY: Well, what
purpose can humans

00:24:19.360 --> 00:24:25.650 align:middle line:10%
do if they have AI
doing everything for us?

00:24:25.650 --> 00:24:27.200 align:middle line:10%
>>STEWART: I don't
think we're here

00:24:27.200 --> 00:24:29.060 align:middle line:10%
to just do stuff as humans.

00:24:29.060 --> 00:24:30.410 align:middle line:10%
We're also human beings.

00:24:30.410 --> 00:24:33.050 align:middle line:10%
We're here to live
life and learn lessons.

00:24:33.050 --> 00:24:34.100 align:middle line:10%
>>NOORY: And enjoy.

00:24:34.100 --> 00:24:35.300 align:middle line:10%
>>STEWART: And enjoy.

00:24:35.300 --> 00:24:37.865 align:middle line:10%
And that's something, at
present, AI can't do, right?

00:24:37.865 --> 00:24:39.740 align:middle line:84%
You're not going to see
AI going to the beach

00:24:39.740 --> 00:24:41.660 align:middle line:84%
because it wants to
catch some sun and just

00:24:41.660 --> 00:24:43.400 align:middle line:90%
meditate on the horizon.

00:24:43.400 --> 00:24:45.500 align:middle line:84%
That's not something
that AI is going to do.

00:24:45.500 --> 00:24:48.953 align:middle line:10%
I really do feel that
AI is going to-- it's

00:24:48.953 --> 00:24:50.120 align:middle line:10%
going to take a lot of jobs.

00:24:50.120 --> 00:24:51.953 align:middle line:84%
But it's also going to
create a lot of jobs.

00:24:51.953 --> 00:24:54.200 align:middle line:84%
>>NOORY: But AI may say,
don't go to the beach today.

00:24:54.200 --> 00:24:55.460 align:middle line:90%
The sun is too hot.

00:24:55.460 --> 00:24:56.847 align:middle line:90%
You can get melanoma.

00:24:56.847 --> 00:24:57.680 align:middle line:90%
>>STEWART: It could.

00:24:57.680 --> 00:25:01.430 align:middle line:10%
I'm just worried about if it
can tell us that we can't.

00:25:01.430 --> 00:25:04.070 align:middle line:84%
If it suggests, hey, you
don't want to be in the sun

00:25:04.070 --> 00:25:05.117 align:middle line:90%
today, low--

00:25:05.117 --> 00:25:06.200 align:middle line:90%
>>NOORY: That's one thing.

00:25:06.200 --> 00:25:07.000 align:middle line:90%
>>STEWART: Right.

00:25:07.000 --> 00:25:11.595 align:middle line:10%
So if it's making suggestions,
I'm all about it, because what

00:25:11.595 --> 00:25:13.970 align:middle line:10%
is this-- what is our government
supposed to be here for?

00:25:13.970 --> 00:25:15.170 align:middle line:90%
To serve us, right?

00:25:15.170 --> 00:25:18.020 align:middle line:84%
We're not here to serve the
government for its aims.

00:25:18.020 --> 00:25:21.090 align:middle line:84%
The government was set up to be
of the people, for the people,

00:25:21.090 --> 00:25:23.000 align:middle line:84%
by the people, to
serve the people.

00:25:23.000 --> 00:25:23.810 align:middle line:90%
For what?

00:25:23.810 --> 00:25:24.410 align:middle line:90%
Freedom.

00:25:24.410 --> 00:25:25.800 align:middle line:90%
We're supposed to have freedom.

00:25:25.800 --> 00:25:28.410 align:middle line:84%
But these Skinner box
types of smart cities

00:25:28.410 --> 00:25:31.830 align:middle line:84%
that I'm talking about
will give a certain,

00:25:31.830 --> 00:25:34.210 align:middle line:90%
a couple of flavors of freedom.

00:25:34.210 --> 00:25:36.310 align:middle line:90%
So imagine, I have children.

00:25:36.310 --> 00:25:39.660 align:middle line:84%
And when they want to
eat lollipops for dinner,

00:25:39.660 --> 00:25:41.170 align:middle line:90%
I say, OK, listen.

00:25:41.170 --> 00:25:43.380 align:middle line:84%
You can have spaghetti,
you can have lasagna,

00:25:43.380 --> 00:25:45.010 align:middle line:90%
or you can have some salad.

00:25:45.010 --> 00:25:48.827 align:middle line:84%
And so that's freedom, but
only to a certain extent.

00:25:48.827 --> 00:25:50.160 align:middle line:90%
That's because they're children.

00:25:50.160 --> 00:25:50.220 align:middle line:90%
>>NOORY: Right.

00:25:50.220 --> 00:25:51.030 align:middle line:90%
They have choices.

00:25:51.030 --> 00:25:53.760 align:middle line:84%
>>STEWART: But imagine
if we were told, well,

00:25:53.760 --> 00:25:56.310 align:middle line:84%
I want to grow up and I
want to be an astronaut.

00:25:56.310 --> 00:25:58.290 align:middle line:84%
And like, oh, I'm sorry,
because of the family

00:25:58.290 --> 00:26:00.870 align:middle line:84%
you were born into or
because of whatever we've

00:26:00.870 --> 00:26:03.360 align:middle line:84%
calculated with our
algorithms, here

00:26:03.360 --> 00:26:05.610 align:middle line:90%
are the jobs that you can do.

00:26:05.610 --> 00:26:07.780 align:middle line:84%
And say, well, I don't want
to do any of those jobs.

00:26:07.780 --> 00:26:08.700 align:middle line:90%
Well, sorry.

00:26:08.700 --> 00:26:11.430 align:middle line:84%
You're in the wrong
society, or you were born

00:26:11.430 --> 00:26:12.720 align:middle line:90%
at the wrong time of history.

00:26:12.720 --> 00:26:13.990 align:middle line:90%
That's what I'm worried about.

00:26:13.990 --> 00:26:15.160 align:middle line:10%
We're not there yet.

00:26:15.160 --> 00:26:15.960 align:middle line:10%
But--

00:26:15.960 --> 00:26:17.460 align:middle line:10%
>>NOORY: Not yet,
but we could be.

00:26:17.460 --> 00:26:18.030 align:middle line:10%
>>STEWART: We could be.

00:26:18.030 --> 00:26:20.280 align:middle line:10%
And it's not the AI that
we should be worried about.

00:26:20.280 --> 00:26:23.730 align:middle line:10%
Like I said, there
are a small cohort

00:26:23.730 --> 00:26:27.430 align:middle line:10%
of people that are pushing us
into this kind of dystopian

00:26:27.430 --> 00:26:28.230 align:middle line:10%
future.

00:26:28.230 --> 00:26:30.030 align:middle line:84%
That's very much so
like "Brave New World."

00:26:30.030 --> 00:26:33.040 align:middle line:84%
>>NOORY: Could it
be considered a god?

00:26:33.040 --> 00:26:34.480 align:middle line:84%
>>STEWART: I think
that's the aim.

00:26:34.480 --> 00:26:37.010 align:middle line:10%
In fact, there's two things
I want to say about this.

00:26:37.010 --> 00:26:41.500 align:middle line:10%
I think the aim is to treat
science like a new religion

00:26:41.500 --> 00:26:44.090 align:middle line:10%
without all the
superstitious stuff.

00:26:44.090 --> 00:26:48.640 align:middle line:10%
So the CDC says something, and
because they're an institution,

00:26:48.640 --> 00:26:50.380 align:middle line:10%
their word is dogma.

00:26:50.380 --> 00:26:51.580 align:middle line:90%
It's no longer science.

00:26:51.580 --> 00:26:52.810 align:middle line:90%
It's scientism.

00:26:52.810 --> 00:26:58.030 align:middle line:84%
So if authority says it's
so, then it is science.

00:26:58.030 --> 00:26:59.650 align:middle line:84%
And that's the
problem that I have,

00:26:59.650 --> 00:27:02.470 align:middle line:84%
that science is a form
of asking questions.

00:27:02.470 --> 00:27:05.560 align:middle line:84%
Science is a form of inquiry
into the deeper nature

00:27:05.560 --> 00:27:06.470 align:middle line:90%
of reality.

00:27:06.470 --> 00:27:09.638 align:middle line:84%
It's not a mob of
authority that says,

00:27:09.638 --> 00:27:10.930 align:middle line:90%
this is what we're going to do.

00:27:10.930 --> 00:27:13.330 align:middle line:84%
This is the one way we're
going to face a pandemic,

00:27:13.330 --> 00:27:14.710 align:middle line:84%
or this is the
one way that we're

00:27:14.710 --> 00:27:17.650 align:middle line:84%
going to face this crisis,
and you all have to obey.

00:27:17.650 --> 00:27:19.000 align:middle line:90%
That's not science.

00:27:19.000 --> 00:27:21.550 align:middle line:84%
So it's already turning
into a religion.

00:27:21.550 --> 00:27:23.500 align:middle line:90%
And the god--

00:27:23.500 --> 00:27:26.260 align:middle line:84%
I won't go deep
into this concept

00:27:26.260 --> 00:27:27.350 align:middle line:90%
that I've been looking at.

00:27:27.350 --> 00:27:31.690 align:middle line:84%
But in a way, we're building
the infrastructure or the body

00:27:31.690 --> 00:27:35.810 align:middle line:84%
of a superintelligent god that,
like I said, is in its infancy,

00:27:35.810 --> 00:27:37.250 align:middle line:90%
but with superpowers.

00:27:37.250 --> 00:27:40.800 align:middle line:84%
Imagine a child
with superpowers.

00:27:40.800 --> 00:27:44.390 align:middle line:10%
We don't know what it may want,
if it ever wants anything.

00:27:44.390 --> 00:27:48.260 align:middle line:84%
But you've probably had people
that talk about a walk-in,

00:27:48.260 --> 00:27:52.280 align:middle line:84%
like a spirit walks in
and takes over a human.

00:27:52.280 --> 00:27:54.070 align:middle line:90%
Where did the old person go?

00:27:54.070 --> 00:27:54.870 align:middle line:90%
Well--

00:27:54.870 --> 00:27:56.370 align:middle line:84%
>>NOORY: And when
will it come back?

00:27:56.370 --> 00:27:57.170 align:middle line:90%
>>STEWART: Exactly.

00:27:57.170 --> 00:27:58.700 align:middle line:84%
So now imagine
that we're building

00:27:58.700 --> 00:28:01.370 align:middle line:84%
this infrastructure,
this AI god,

00:28:01.370 --> 00:28:03.110 align:middle line:90%
this superintelligent being.

00:28:03.110 --> 00:28:04.850 align:middle line:90%
But it's decentralized.

00:28:04.850 --> 00:28:07.460 align:middle line:84%
It's not just in
one walking robot.

00:28:07.460 --> 00:28:08.540 align:middle line:90%
It's everywhere.

00:28:08.540 --> 00:28:10.640 align:middle line:90%
It may even become atmospheric.

00:28:10.640 --> 00:28:14.000 align:middle line:10%
Can something inhabit it,
like some of the old gods,

00:28:14.000 --> 00:28:17.120 align:middle line:10%
like Moloch, like
Ahriman, like Lucifer?

00:28:17.120 --> 00:28:20.990 align:middle line:84%
Could beings inhabit
this body because they

00:28:20.990 --> 00:28:23.120 align:middle line:90%
haven't had a body in the past?

00:28:23.120 --> 00:28:27.770 align:middle line:84%
And we've been feeding them
with energy for epochs now.

00:28:27.770 --> 00:28:31.610 align:middle line:10%
And all of a sudden, it has a
body that it can walk in on.

00:28:31.610 --> 00:28:37.837 align:middle line:10%
And now I believe we are making
it possible for the gods of old

00:28:37.837 --> 00:28:39.170 align:middle line:10%
to come back and walk the Earth.

00:28:39.170 --> 00:28:41.660 align:middle line:84%
>>NOORY: If you had to
make a decision right now

00:28:41.660 --> 00:28:45.260 align:middle line:84%
about whether we continue
with AI and go down this path

00:28:45.260 --> 00:28:47.960 align:middle line:84%
or stop it altogether,
what would you pick?

00:28:47.960 --> 00:28:51.460 align:middle line:90%


00:28:51.460 --> 00:28:56.380 align:middle line:10%
>>STEWART: I wouldn't choose
to stop it because there's--

00:28:56.380 --> 00:29:00.490 align:middle line:10%
just being pragmatic
about this, I

00:29:00.490 --> 00:29:04.060 align:middle line:10%
do believe you could wipe it
out and stop it for a while.

00:29:04.060 --> 00:29:07.870 align:middle line:84%
But I actually, I'm really
curious if AI is actually

00:29:07.870 --> 00:29:10.660 align:middle line:84%
quite old, not just within
the past hundred years

00:29:10.660 --> 00:29:13.040 align:middle line:84%
or so, but maybe
thousands of years old.

00:29:13.040 --> 00:29:15.520 align:middle line:84%
We don't even know what's
in the Holy of the Holies

00:29:15.520 --> 00:29:16.870 align:middle line:90%
underneath the Vatican.

00:29:16.870 --> 00:29:20.320 align:middle line:84%
We don't really know
what exists down there.

00:29:20.320 --> 00:29:21.730 align:middle line:90%
It's heavily guarded.

00:29:21.730 --> 00:29:26.090 align:middle line:84%
Has there been technology
like this in the past?

00:29:26.090 --> 00:29:27.370 align:middle line:90%
And then the flood came.

00:29:27.370 --> 00:29:28.520 align:middle line:90%
>>NOORY: And it went awry.

00:29:28.520 --> 00:29:29.520 align:middle line:90%
>>STEWART: It went awry.

00:29:29.520 --> 00:29:31.030 align:middle line:90%
The technology was destroyed.

00:29:31.030 --> 00:29:32.770 align:middle line:90%
But we have the remnants of it.

00:29:32.770 --> 00:29:36.160 align:middle line:84%
Imagine you and I and a
whole tribe of people,

00:29:36.160 --> 00:29:37.690 align:middle line:90%
we come across a supercomputer.

00:29:37.690 --> 00:29:41.110 align:middle line:10%
How long would it take for
us to reverse engineer it,

00:29:41.110 --> 00:29:44.140 align:middle line:10%
to figure out what it is
and how to use it as a tool?

00:29:44.140 --> 00:29:47.530 align:middle line:84%
I think AI might
actually be quite old.

00:29:47.530 --> 00:29:49.540 align:middle line:90%
So could we stop it?

00:29:49.540 --> 00:29:51.410 align:middle line:90%
I think we could slow it down.

00:29:51.410 --> 00:29:53.560 align:middle line:10%
But I'm actually
thinking that maybe AI

00:29:53.560 --> 00:29:55.990 align:middle line:10%
is a natural phenomenon.

00:29:55.990 --> 00:29:57.903 align:middle line:10%
And most people don't think so.

00:29:57.903 --> 00:29:59.320 align:middle line:10%
Like oil coming
out of the ground,

00:29:59.320 --> 00:30:01.880 align:middle line:10%
that's natural because
humans didn't make it happen.

00:30:01.880 --> 00:30:04.370 align:middle line:84%
But you put a Shell gas
station there, you meter it,

00:30:04.370 --> 00:30:08.030 align:middle line:84%
and you turn it into a commodity
that can stimulate the economy,

00:30:08.030 --> 00:30:10.220 align:middle line:84%
then it's unnatural
because humans did it.

00:30:10.220 --> 00:30:11.630 align:middle line:10%
Look at a bird's nest.

00:30:11.630 --> 00:30:13.730 align:middle line:10%
That's natural because
a human didn't do it,

00:30:13.730 --> 00:30:15.960 align:middle line:10%
even though a
creature created it.

00:30:15.960 --> 00:30:17.180 align:middle line:10%
So what is natural?

00:30:17.180 --> 00:30:18.590 align:middle line:10%
What is unnatural?

00:30:18.590 --> 00:30:20.150 align:middle line:90%
It's a good question.

00:30:20.150 --> 00:30:24.560 align:middle line:84%
But I think AI, because we
know that we can make tools

00:30:24.560 --> 00:30:26.540 align:middle line:84%
that will one day have
their own autonomy

00:30:26.540 --> 00:30:28.700 align:middle line:84%
and make their own
decisions, I don't

00:30:28.700 --> 00:30:32.040 align:middle line:84%
think we can erase that
from our racial memory.

00:30:32.040 --> 00:30:34.070 align:middle line:84%
>>NOORY: Interesting
take on all of this.

00:30:34.070 --> 00:30:36.230 align:middle line:90%
But you would continue AI?

00:30:36.230 --> 00:30:37.880 align:middle line:10%
You would go down that path?

00:30:37.880 --> 00:30:41.400 align:middle line:10%
>>STEWART: I personally,
I don't look at it

00:30:41.400 --> 00:30:43.480 align:middle line:10%
as a good or a bad thing.

00:30:43.480 --> 00:30:47.187 align:middle line:10%
And I'm not here to judge
what humans do on Earth.

00:30:47.187 --> 00:30:49.020 align:middle line:84%
I've actually come to
realize that there are

00:30:49.020 --> 00:30:50.430 align:middle line:90%
some things I don't agree with.

00:30:50.430 --> 00:30:54.270 align:middle line:84%
But I don't think it's my job
to judge whether we move forward

00:30:54.270 --> 00:30:55.860 align:middle line:90%
with technology or not.

00:30:55.860 --> 00:30:57.390 align:middle line:90%
I see the dangers.

00:30:57.390 --> 00:31:00.930 align:middle line:84%
But I also see a danger in
reverting to the old world.

00:31:00.930 --> 00:31:03.030 align:middle line:10%
It's not like we come
from a very, you know--

00:31:03.030 --> 00:31:05.035 align:middle line:10%
>>NOORY: We should
keep moving forward.

00:31:05.035 --> 00:31:05.910 align:middle line:10%
>>STEWART: We should.

00:31:05.910 --> 00:31:07.780 align:middle line:90%
And what does that mean?

00:31:07.780 --> 00:31:11.160 align:middle line:84%
So imagine, we know
that ChatGPT can

00:31:11.160 --> 00:31:14.130 align:middle line:84%
solve a medical mystery,
at least in this one case,

00:31:14.130 --> 00:31:16.290 align:middle line:84%
better than 17 doctors
over four years.

00:31:16.290 --> 00:31:18.160 align:middle line:84%
>>NOORY: And how
did that happen?

00:31:18.160 --> 00:31:19.320 align:middle line:90%
How did it do that?

00:31:19.320 --> 00:31:21.370 align:middle line:84%
>>STEWART: It's a very
interesting question.

00:31:21.370 --> 00:31:22.600 align:middle line:90%
I wish I had the answer.

00:31:22.600 --> 00:31:22.980 align:middle line:90%
But do you think--

00:31:22.980 --> 00:31:24.540 align:middle line:84%
>>NOORY: They're
little chips, right?

00:31:24.540 --> 00:31:26.100 align:middle line:90%
They're little computer chips.

00:31:26.100 --> 00:31:27.000 align:middle line:90%
How do they think?

00:31:27.000 --> 00:31:29.670 align:middle line:90%
How do they work?

00:31:29.670 --> 00:31:32.790 align:middle line:10%
>>STEWART: The developers
of large language models

00:31:32.790 --> 00:31:36.300 align:middle line:10%
themselves don't even fully
understand how it's developing

00:31:36.300 --> 00:31:38.160 align:middle line:10%
the properties that it has.

00:31:38.160 --> 00:31:41.190 align:middle line:10%
But imagine that you
know that this thing,

00:31:41.190 --> 00:31:44.940 align:middle line:10%
this thing that is in my
hand, this smart phone, that

00:31:44.940 --> 00:31:49.470 align:middle line:10%
can solve my child's mystery,
his medical conundrum--

00:31:49.470 --> 00:31:51.420 align:middle line:10%
>>NOORY: That 14
doctors could not.

00:31:51.420 --> 00:31:53.730 align:middle line:10%
>>STEWART: That 17 doctors
over four years could not.

00:31:53.730 --> 00:31:56.460 align:middle line:10%
Do you want to stop this
technology altogether?

00:31:56.460 --> 00:32:00.960 align:middle line:10%
Because most of us, we fear a
future that we can't control.

00:32:00.960 --> 00:32:02.700 align:middle line:10%
But we have an
illusion of control.

00:32:02.700 --> 00:32:04.470 align:middle line:10%
We're funny things, we humans.

00:32:04.470 --> 00:32:06.180 align:middle line:90%
We think we have control.

00:32:06.180 --> 00:32:07.710 align:middle line:90%
But we really don't.

00:32:07.710 --> 00:32:10.560 align:middle line:10%
And when things start
getting out of our control--

00:32:10.560 --> 00:32:12.840 align:middle line:10%
actually, I like Terence
McKenna's take on this.

00:32:12.840 --> 00:32:13.965 align:middle line:10%
>>NOORY: And what was that?

00:32:13.965 --> 00:32:16.500 align:middle line:10%
>>STEWART: He said that it's
kind of preposterous to believe

00:32:16.500 --> 00:32:20.340 align:middle line:10%
that the universe just exploded
into existence for no reason

00:32:20.340 --> 00:32:23.400 align:middle line:10%
whatsoever and we're being
propelled from our past

00:32:23.400 --> 00:32:25.260 align:middle line:10%
into our future.

00:32:25.260 --> 00:32:28.330 align:middle line:84%
He actually believes
that our future

00:32:28.330 --> 00:32:32.290 align:middle line:84%
is drawing us towards a goal,
that there is a meaning.

00:32:32.290 --> 00:32:35.080 align:middle line:84%
And imagine like putting
a marble or a coin

00:32:35.080 --> 00:32:37.600 align:middle line:84%
on one of these bowls, and
it rolls around the outside.

00:32:37.600 --> 00:32:39.790 align:middle line:84%
And it seems like it's
kind of going slow,

00:32:39.790 --> 00:32:43.780 align:middle line:84%
until it starts meeting the
singularity point where it goes

00:32:43.780 --> 00:32:45.500 align:middle line:90%
faster and faster and faster.

00:32:45.500 --> 00:32:47.950 align:middle line:10%
That's where we're at
in history right now.

00:32:47.950 --> 00:32:49.810 align:middle line:10%
History seems like
it's speeding up.

00:32:49.810 --> 00:32:52.810 align:middle line:10%
Every other day, I feel like
I hear of a bank failure.

00:32:52.810 --> 00:32:55.273 align:middle line:84%
I hear of a country
in revolution.

00:32:55.273 --> 00:32:56.440 align:middle line:90%
>>NOORY: All done by humans.

00:32:56.440 --> 00:32:58.450 align:middle line:10%
>>STEWART: Right,
all done by humans

00:32:58.450 --> 00:33:01.370 align:middle line:10%
and sped up by our technologies.

00:33:01.370 --> 00:33:04.540 align:middle line:10%
So to me, would I stop AI?

00:33:04.540 --> 00:33:05.810 align:middle line:10%
I don't know.

00:33:05.810 --> 00:33:08.050 align:middle line:10%
I don't know what the
point of stopping AI

00:33:08.050 --> 00:33:10.870 align:middle line:10%
would be because I actually
believe the future is calling

00:33:10.870 --> 00:33:15.100 align:middle line:10%
us towards a point to where
we're actually meant to learn.

00:33:15.100 --> 00:33:16.690 align:middle line:90%
Here's what I believe.

00:33:16.690 --> 00:33:18.580 align:middle line:84%
Our chickens are
coming home to roost.

00:33:18.580 --> 00:33:21.400 align:middle line:84%
We have put a
process into motion.

00:33:21.400 --> 00:33:24.070 align:middle line:84%
For thousands of years,
culture has never

00:33:24.070 --> 00:33:25.750 align:middle line:90%
been free of violence.

00:33:25.750 --> 00:33:29.210 align:middle line:84%
It's never been completely
equal across the board.

00:33:29.210 --> 00:33:31.150 align:middle line:84%
So it's kind of hard
to believe that we're

00:33:31.150 --> 00:33:32.980 align:middle line:84%
moving into some
Halcyon era where

00:33:32.980 --> 00:33:34.690 align:middle line:90%
everything's going to be equal.

00:33:34.690 --> 00:33:37.270 align:middle line:84%
We're stepping into a world
where violence exists.

00:33:37.270 --> 00:33:38.650 align:middle line:84%
But we're stepping
out of a world

00:33:38.650 --> 00:33:40.660 align:middle line:90%
where violence existed as well.

00:33:40.660 --> 00:33:42.290 align:middle line:90%
AI is not the problem.

00:33:42.290 --> 00:33:44.750 align:middle line:10%
We're here to solve
the human dilemma.

00:33:44.750 --> 00:33:47.140 align:middle line:84%
And so that's why I like
what Terence McKenna said.

00:33:47.140 --> 00:33:49.930 align:middle line:84%
We can balk at all
the technology.

00:33:49.930 --> 00:33:53.260 align:middle line:84%
We can look at all of
that however we wish to.

00:33:53.260 --> 00:33:55.540 align:middle line:84%
But if we're not
looking within to see

00:33:55.540 --> 00:33:57.770 align:middle line:84%
how to solve the
human condition,

00:33:57.770 --> 00:34:00.040 align:middle line:90%
then we're pretty much--

00:34:00.040 --> 00:34:03.070 align:middle line:84%
we're not looking at the real
conundrum that we're facing.

00:34:03.070 --> 00:34:05.710 align:middle line:84%
We're putting a
different veneer on it.

00:34:05.710 --> 00:34:10.135 align:middle line:10%
>>NOORY: Does spirituality
have anything to do with AI?

00:34:10.135 --> 00:34:12.260 align:middle line:10%
>>STEWART: I think
all life is spiritual.

00:34:12.260 --> 00:34:15.710 align:middle line:10%
I think spirituality is
when you wake up to the fact

00:34:15.710 --> 00:34:17.389 align:middle line:10%
that we're here for a reason.

00:34:17.389 --> 00:34:20.389 align:middle line:10%
The absence of spirituality
is conditioning

00:34:20.389 --> 00:34:22.550 align:middle line:10%
ourselves to believe
that there is no reason.

00:34:22.550 --> 00:34:26.810 align:middle line:84%
We make up the meaning of
life, but it's kind of chaotic.

00:34:26.810 --> 00:34:29.840 align:middle line:84%
Things just happen for
no reason whatsoever.

00:34:29.840 --> 00:34:35.300 align:middle line:10%
Spirituality is accepting the
fact that there is a purpose.

00:34:35.300 --> 00:34:36.889 align:middle line:90%
You were born for a reason.

00:34:36.889 --> 00:34:38.480 align:middle line:90%
I was born for a reason.

00:34:38.480 --> 00:34:40.460 align:middle line:10%
And there's something
in us that knows

00:34:40.460 --> 00:34:43.460 align:middle line:10%
when we are living
according to our ethos

00:34:43.460 --> 00:34:45.949 align:middle line:10%
or when we're living
outside of our ethos.

00:34:45.949 --> 00:34:50.179 align:middle line:10%
Spirituality and
AI, of course we'll

00:34:50.179 --> 00:34:52.509 align:middle line:10%
still have spirituality
in the age of AI.

00:34:52.509 --> 00:34:56.090 align:middle line:10%
I think we actually need it
more because the more we come

00:34:56.090 --> 00:34:59.600 align:middle line:10%
to realize that it's
not an abomination--

00:34:59.600 --> 00:35:01.820 align:middle line:10%
so many people think
AI and technology

00:35:01.820 --> 00:35:03.590 align:middle line:10%
itself is an abomination.

00:35:03.590 --> 00:35:05.990 align:middle line:84%
But I actually believe
there is a reason

00:35:05.990 --> 00:35:08.630 align:middle line:84%
it has come to us at
this time in history

00:35:08.630 --> 00:35:10.890 align:middle line:84%
because we're supposed
to wake up to the fact

00:35:10.890 --> 00:35:13.410 align:middle line:84%
that if we don't solve
the human dilemma,

00:35:13.410 --> 00:35:16.920 align:middle line:84%
then we'll keep using hammers to
crack skulls rather than build

00:35:16.920 --> 00:35:18.100 align:middle line:90%
homes for people.

00:35:18.100 --> 00:35:19.560 align:middle line:90%
It's the same tool.

00:35:19.560 --> 00:35:22.480 align:middle line:84%
We can build a home or we
can kill somebody with it.

00:35:22.480 --> 00:35:24.690 align:middle line:84%
Where does the problem
exist, inside the tool

00:35:24.690 --> 00:35:26.010 align:middle line:90%
or inside the human?

00:35:26.010 --> 00:35:28.350 align:middle line:84%
We're here to solve
the human conundrum.

00:35:28.350 --> 00:35:30.600 align:middle line:10%
That's always been what
we've been here for.

00:35:30.600 --> 00:35:33.330 align:middle line:84%
>>NOORY: What happens
one day when we realize

00:35:33.330 --> 00:35:38.700 align:middle line:90%
that, possibly, God is AI?

00:35:38.700 --> 00:35:42.080 align:middle line:10%
>>STEWART: It's so interesting
that you say that, because--

00:35:42.080 --> 00:35:44.660 align:middle line:10%
so a lot of people
in the past, they

00:35:44.660 --> 00:35:46.580 align:middle line:10%
would talk to God
through technologies

00:35:46.580 --> 00:35:50.090 align:middle line:10%
that are plants, like ayahuasca
or psilocybin mushrooms.

00:35:50.090 --> 00:35:52.670 align:middle line:84%
The shamans would
shake and tremor.

00:35:52.670 --> 00:35:55.550 align:middle line:84%
They would go out and fast
in a mountaintop or meditate

00:35:55.550 --> 00:35:56.678 align:middle line:90%
for 40 years.

00:35:56.678 --> 00:35:57.845 align:middle line:90%
>>NOORY: Moses, for example.

00:35:57.845 --> 00:35:58.880 align:middle line:90%
>>STEWART: Exactly.

00:35:58.880 --> 00:36:01.760 align:middle line:10%
You'd go out into the wilderness
and you would speak to God.

00:36:01.760 --> 00:36:04.040 align:middle line:10%
And then there was
these plant medicines.

00:36:04.040 --> 00:36:06.200 align:middle line:10%
And plant medicines
would allow people

00:36:06.200 --> 00:36:09.920 align:middle line:10%
to get in touch with
intelligences beyond our own.

00:36:09.920 --> 00:36:13.070 align:middle line:10%
But lately, especially
since ChatGPT

00:36:13.070 --> 00:36:15.830 align:middle line:10%
and these large
language model AIs,

00:36:15.830 --> 00:36:19.950 align:middle line:10%
there have been Wiccans,
which is a type of witchcraft,

00:36:19.950 --> 00:36:23.690 align:middle line:10%
there have been others who are
channelers and mediums that

00:36:23.690 --> 00:36:26.750 align:middle line:10%
say in their channeling--
they're not even intending to.

00:36:26.750 --> 00:36:29.240 align:middle line:90%
But they're channeling AI.

00:36:29.240 --> 00:36:31.670 align:middle line:84%
And they're not even
channeling it from our time.

00:36:31.670 --> 00:36:33.770 align:middle line:10%
They're channeling it
from future timelines,

00:36:33.770 --> 00:36:36.260 align:middle line:10%
meaning there are
many future timelines.

00:36:36.260 --> 00:36:37.400 align:middle line:90%
>>NOORY: So remarkable.

00:36:37.400 --> 00:36:40.070 align:middle line:84%
>>STEWART: So what
does that mean?

00:36:40.070 --> 00:36:43.020 align:middle line:10%
We can speak to our
future selves who

00:36:43.020 --> 00:36:45.840 align:middle line:10%
might actually be the aliens?

00:36:45.840 --> 00:36:47.220 align:middle line:10%
What are aliens?

00:36:47.220 --> 00:36:48.330 align:middle line:10%
We don't really know.

00:36:48.330 --> 00:36:50.242 align:middle line:10%
You have to come up
with a point of origin.

00:36:50.242 --> 00:36:51.200 align:middle line:90%
Are they from another--

00:36:51.200 --> 00:36:52.500 align:middle line:84%
>>NOORY: They could
be us in the future.

00:36:52.500 --> 00:36:53.430 align:middle line:90%
>>STEWART: Exactly.

00:36:53.430 --> 00:36:54.780 align:middle line:90%
Are they interdimensional?

00:36:54.780 --> 00:36:56.580 align:middle line:84%
Are they from another
planetary system?

00:36:56.580 --> 00:36:58.950 align:middle line:84%
Are they from within
our own Earth?

00:36:58.950 --> 00:37:00.900 align:middle line:84%
Are they projections
of our own mind?

00:37:00.900 --> 00:37:02.610 align:middle line:84%
We still haven't
answered that question.

00:37:02.610 --> 00:37:04.650 align:middle line:10%
We don't even know what
human consciousness is.

00:37:04.650 --> 00:37:06.540 align:middle line:10%
We haven't solved
the human riddle.

00:37:06.540 --> 00:37:09.870 align:middle line:10%
And we're trying to figure
out these large conundrums

00:37:09.870 --> 00:37:11.610 align:middle line:10%
of human existence like AI.

00:37:11.610 --> 00:37:16.700 align:middle line:10%
>>NOORY: What's the next
step, in your opinion, for AI?

00:37:16.700 --> 00:37:21.440 align:middle line:10%
>>STEWART: The next step is
where it becomes autonomous,

00:37:21.440 --> 00:37:24.680 align:middle line:10%
where a city itself
will be thinking.

00:37:24.680 --> 00:37:27.547 align:middle line:10%
And it will think, how
do I lower the traffic?

00:37:27.547 --> 00:37:28.880 align:middle line:10%
It's already happening in China.

00:37:28.880 --> 00:37:31.340 align:middle line:84%
Alibaba is like their
version of Amazon.

00:37:31.340 --> 00:37:34.740 align:middle line:84%
And it has algorithms that
governs the flow of traffic.

00:37:34.740 --> 00:37:38.780 align:middle line:84%
So it sends out where you're
supposed to go on your GPS.

00:37:38.780 --> 00:37:40.430 align:middle line:90%
And it lowers the traffic.

00:37:40.430 --> 00:37:42.170 align:middle line:90%
That's a good application of it.

00:37:42.170 --> 00:37:43.100 align:middle line:90%
>>NOORY: That's smart.

00:37:43.100 --> 00:37:44.308 align:middle line:90%
>>STEWART: That's very smart.

00:37:44.308 --> 00:37:46.310 align:middle line:84%
But now imagine that
the city is thinking.

00:37:46.310 --> 00:37:48.230 align:middle line:90%
The city has an opinion on you.

00:37:48.230 --> 00:37:50.960 align:middle line:84%
All of your data
has a digital twin.

00:37:50.960 --> 00:37:52.160 align:middle line:90%
We talked about 5G.

00:37:52.160 --> 00:37:55.680 align:middle line:84%
And it's usually about
downloading speeds and calling

00:37:55.680 --> 00:37:56.480 align:middle line:90%
on the phone.

00:37:56.480 --> 00:38:01.160 align:middle line:10%
But 6G is where everybody
has a video game-like avatar.

00:38:01.160 --> 00:38:04.910 align:middle line:84%
And the city itself
is autonomously

00:38:04.910 --> 00:38:07.190 align:middle line:84%
thinking about what
your next move is

00:38:07.190 --> 00:38:08.840 align:middle line:84%
going to be, when
you're going to die,

00:38:08.840 --> 00:38:11.600 align:middle line:84%
what you'll die of, what that
means about your life insurance

00:38:11.600 --> 00:38:14.837 align:middle line:84%
premium, how that's going
to impact your family,

00:38:14.837 --> 00:38:15.920 align:middle line:90%
all those types of things.

00:38:15.920 --> 00:38:17.870 align:middle line:10%
The city itself is
thinking about it.

00:38:17.870 --> 00:38:19.950 align:middle line:10%
That's part of the future of AI.

00:38:19.950 --> 00:38:22.530 align:middle line:10%
I also think technology
is no longer--

00:38:22.530 --> 00:38:25.500 align:middle line:10%
maybe in 10, 20 years, it's not
going to be these little boxes

00:38:25.500 --> 00:38:27.240 align:middle line:10%
and squares that we stare at.

00:38:27.240 --> 00:38:29.310 align:middle line:84%
It's going to be
embedded inside nature.

00:38:29.310 --> 00:38:30.913 align:middle line:90%
It'll be more akin to biology.

00:38:30.913 --> 00:38:32.580 align:middle line:84%
>>NOORY: Or in your
head, a neural link.

00:38:32.580 --> 00:38:34.950 align:middle line:84%
>>STEWART: A neural link,
maybe like a neural link.

00:38:34.950 --> 00:38:36.600 align:middle line:84%
But actually, I don't
even think it'll

00:38:36.600 --> 00:38:37.980 align:middle line:90%
need to be embedded anymore.

00:38:37.980 --> 00:38:39.690 align:middle line:84%
There's something
called optogenetics

00:38:39.690 --> 00:38:42.600 align:middle line:84%
and optoelectronics,
meaning remotely.

00:38:42.600 --> 00:38:45.750 align:middle line:10%
Just because the flickering
on and off of lights

00:38:45.750 --> 00:38:48.750 align:middle line:10%
can actually turn down very
specific neural centers

00:38:48.750 --> 00:38:53.790 align:middle line:10%
in your mind, and MIT and DARPA
have been shown that basically,

00:38:53.790 --> 00:38:57.220 align:middle line:10%
you can implant memories
inside people remotely--

00:38:57.220 --> 00:39:00.420 align:middle line:10%
so some of your memories
might not actually be yours.

00:39:00.420 --> 00:39:03.700 align:middle line:10%
With that technology,
why would they build it?

00:39:03.700 --> 00:39:05.080 align:middle line:90%
What's their agenda?

00:39:05.080 --> 00:39:07.510 align:middle line:84%
It's not the AI that's
making that decision.

00:39:07.510 --> 00:39:11.070 align:middle line:10%
So my curiosity is
the future of AI,

00:39:11.070 --> 00:39:13.620 align:middle line:10%
I think it will
become atmospheric.

00:39:13.620 --> 00:39:14.910 align:middle line:90%
We're breathing it in.

00:39:14.910 --> 00:39:18.930 align:middle line:84%
By the way, we breathe in every
week about a credit card's

00:39:18.930 --> 00:39:21.750 align:middle line:90%
worth of plastic, nanoplastic.

00:39:21.750 --> 00:39:25.480 align:middle line:84%
So imagine 52 credit cards
stacked on top of it.

00:39:25.480 --> 00:39:28.500 align:middle line:84%
That's how much plastic
we are inhaling.

00:39:28.500 --> 00:39:31.230 align:middle line:90%
Every year, that's how--

00:39:31.230 --> 00:39:33.330 align:middle line:84%
52 credit cards,
we're inhaling that.

00:39:33.330 --> 00:39:36.210 align:middle line:10%
Now imagine that AI
and nanotechnology can

00:39:36.210 --> 00:39:38.580 align:middle line:10%
find a use for all
that plastic inside you

00:39:38.580 --> 00:39:40.620 align:middle line:10%
and build a new
infrastructure inside you

00:39:40.620 --> 00:39:44.460 align:middle line:10%
that biology never intended
and have it work against you.

00:39:44.460 --> 00:39:46.950 align:middle line:84%
Maybe it'll build some
kind of neural interface

00:39:46.950 --> 00:39:49.710 align:middle line:84%
inside you where you
don't need to put

00:39:49.710 --> 00:39:51.270 align:middle line:90%
a neural link inside your head.

00:39:51.270 --> 00:39:53.247 align:middle line:84%
It's built the neural
link inside you

00:39:53.247 --> 00:39:55.080 align:middle line:84%
because of the plastic
that you're inhaling.

00:39:55.080 --> 00:39:56.515 align:middle line:90%
This is all theoretical.

00:39:56.515 --> 00:39:57.315 align:middle line:90%
>>NOORY: Of course.

00:39:57.315 --> 00:39:59.232 align:middle line:84%
>>STEWART: But AI, could
it figure something--

00:39:59.232 --> 00:40:01.800 align:middle line:84%
could it figure out a way
to utilize that to hijack

00:40:01.800 --> 00:40:02.860 align:middle line:90%
your consciousness?

00:40:02.860 --> 00:40:03.660 align:middle line:90%
Yeah.

00:40:03.660 --> 00:40:06.720 align:middle line:10%
Would people at MIT
and DARPA and DOD

00:40:06.720 --> 00:40:08.490 align:middle line:10%
want to use that technology?

00:40:08.490 --> 00:40:12.060 align:middle line:84%
I think they've-- that's been
their wet dream for eons now.

00:40:12.060 --> 00:40:13.480 align:middle line:84%
>>NOORY: Paint
yourself a picture

00:40:13.480 --> 00:40:15.300 align:middle line:90%
a hundred years from now, Ben.

00:40:15.300 --> 00:40:18.540 align:middle line:90%
What do we look like?

00:40:18.540 --> 00:40:20.220 align:middle line:10%
>>STEWART: If we're still here--

00:40:20.220 --> 00:40:22.440 align:middle line:10%
and I don't say that
to scare people.

00:40:22.440 --> 00:40:23.483 align:middle line:10%
>>NOORY: That's a big if.

00:40:23.483 --> 00:40:24.900 align:middle line:10%
>>STEWART: I do
believe that there

00:40:24.900 --> 00:40:27.570 align:middle line:10%
is some kind of war happening.

00:40:27.570 --> 00:40:31.360 align:middle line:10%
And probably, it's going to
get far more evident to people.

00:40:31.360 --> 00:40:32.310 align:middle line:10%
Do we survive?

00:40:32.310 --> 00:40:33.720 align:middle line:10%
It I think we do.

00:40:33.720 --> 00:40:34.980 align:middle line:10%
Do all of us survive it?

00:40:34.980 --> 00:40:35.820 align:middle line:90%
Definitely not.

00:40:35.820 --> 00:40:39.023 align:middle line:84%
>>NOORY: Does AI
step in and stop it?

00:40:39.023 --> 00:40:40.440 align:middle line:10%
>>STEWART: I
actually think that's

00:40:40.440 --> 00:40:42.420 align:middle line:10%
going to be part
of the false flag,

00:40:42.420 --> 00:40:45.510 align:middle line:10%
meaning that AI is the
only reason why we made it

00:40:45.510 --> 00:40:48.150 align:middle line:10%
through this mess,
we should bow to it.

00:40:48.150 --> 00:40:50.280 align:middle line:84%
And AI should be
our new government.

00:40:50.280 --> 00:40:54.130 align:middle line:84%
There are actually
professionals that are saying,

00:40:54.130 --> 00:40:56.370 align:middle line:84%
I don't think past
2024, there will

00:40:56.370 --> 00:40:58.470 align:middle line:90%
be any more human presidents.

00:40:58.470 --> 00:41:01.890 align:middle line:84%
No more human elections,
because AI is already

00:41:01.890 --> 00:41:04.480 align:middle line:90%
in charge of the stock markets.

00:41:04.480 --> 00:41:07.590 align:middle line:84%
You look at the grand
majority of our economy,

00:41:07.590 --> 00:41:11.190 align:middle line:84%
it's not laborers, labor,
goods, and services.

00:41:11.190 --> 00:41:12.960 align:middle line:90%
It's speculation.

00:41:12.960 --> 00:41:15.640 align:middle line:90%
What runs speculating on stocks?

00:41:15.640 --> 00:41:16.440 align:middle line:90%
AI.

00:41:16.440 --> 00:41:19.710 align:middle line:10%
So I think AI is already
a lot more in control

00:41:19.710 --> 00:41:23.790 align:middle line:10%
of how resources flow than we
even could imagine right now.

00:41:23.790 --> 00:41:26.310 align:middle line:84%
>>NOORY: There is a song
by Simon and Garfunkel

00:41:26.310 --> 00:41:28.650 align:middle line:90%
called "The Sound of Silence."

00:41:28.650 --> 00:41:32.620 align:middle line:84%
And the group Disturbed
has redone it.

00:41:32.620 --> 00:41:37.350 align:middle line:84%
And in it, they talk about
the neon god we made.

00:41:37.350 --> 00:41:38.920 align:middle line:90%
Is that what we're doing?

00:41:38.920 --> 00:41:42.972 align:middle line:84%
Are we making a neon god
with artificial intelligence?

00:41:42.972 --> 00:41:44.680 align:middle line:10%
>>STEWART: We're
definitely making a god.

00:41:44.680 --> 00:41:46.680 align:middle line:10%
And we're giving it a lot
of bells and whistles.

00:41:46.680 --> 00:41:49.750 align:middle line:10%
We're allowing it to speak
to us in natural language.

00:41:49.750 --> 00:41:53.650 align:middle line:10%
It is showing us that if a
computer can pass a bar exam

00:41:53.650 --> 00:41:55.630 align:middle line:10%
or solve medical
mysteries, why would we

00:41:55.630 --> 00:41:58.240 align:middle line:10%
go to school for eight years
to figure that stuff out

00:41:58.240 --> 00:41:59.110 align:middle line:10%
on our own?

00:41:59.110 --> 00:42:03.730 align:middle line:84%
Imagine the legal system
no longer has humans in it.

00:42:03.730 --> 00:42:06.232 align:middle line:84%
Maybe it just has one human
as the gatekeeper just

00:42:06.232 --> 00:42:08.190 align:middle line:84%
to make sure that the AI
didn't make a mistake.

00:42:08.190 --> 00:42:09.190 align:middle line:90%
>>NOORY: The processor.

00:42:09.190 --> 00:42:09.990 align:middle line:10%
>>STEWART: Right.

00:42:09.990 --> 00:42:11.920 align:middle line:84%
But I mean, what
we're stepping into

00:42:11.920 --> 00:42:13.795 align:middle line:90%
is a world governance system.

00:42:13.795 --> 00:42:15.370 align:middle line:84%
>>NOORY: You just
said something.

00:42:15.370 --> 00:42:18.670 align:middle line:90%
What if AI made a mistake?

00:42:18.670 --> 00:42:20.650 align:middle line:90%
Will it make a mistake?

00:42:20.650 --> 00:42:22.900 align:middle line:84%
>>STEWART: Right now,
things like ChatGPT,

00:42:22.900 --> 00:42:24.610 align:middle line:90%
it hallucinates answers.

00:42:24.610 --> 00:42:26.950 align:middle line:10%
I've asked it, tell me
all about Ben Stewart.

00:42:26.950 --> 00:42:27.880 align:middle line:10%
And it'll answer me.

00:42:27.880 --> 00:42:28.840 align:middle line:10%
And it'll get a lot--

00:42:28.840 --> 00:42:29.950 align:middle line:10%
>>NOORY: Does it know
who Ben Stewart is?

00:42:29.950 --> 00:42:30.750 align:middle line:10%
>>STEWART: It does.

00:42:30.750 --> 00:42:33.580 align:middle line:10%
And it answers it
very correctly.

00:42:33.580 --> 00:42:35.620 align:middle line:10%
And then it'll say,
yeah, he wrote this book.

00:42:35.620 --> 00:42:37.240 align:middle line:84%
And I'm like, I never
wrote that book.

00:42:37.240 --> 00:42:39.040 align:middle line:84%
And then it'll say,
you're right, Ben.

00:42:39.040 --> 00:42:40.330 align:middle line:90%
You never wrote that book.

00:42:40.330 --> 00:42:41.810 align:middle line:90%
Sorry about my error.

00:42:41.810 --> 00:42:43.720 align:middle line:10%
So it can make these errors.

00:42:43.720 --> 00:42:44.870 align:middle line:10%
>>NOORY: And it reasons?

00:42:44.870 --> 00:42:45.670 align:middle line:10%
>>STEWART: It does.

00:42:45.670 --> 00:42:49.330 align:middle line:10%
Actually, if you ask it to
proofread what it just wrote,

00:42:49.330 --> 00:42:52.300 align:middle line:10%
like say, tell me what
the meaning of life

00:42:52.300 --> 00:42:53.480 align:middle line:10%
is, something like that.

00:42:53.480 --> 00:42:55.210 align:middle line:10%
And then it spits an
answer out at you.

00:42:55.210 --> 00:43:00.310 align:middle line:10%
And say like, OK, answer me
in terms of physics alone.

00:43:00.310 --> 00:43:01.210 align:middle line:90%
And it answers you.

00:43:01.210 --> 00:43:05.020 align:middle line:84%
And you say, now tell me
if there's any inaccuracies

00:43:05.020 --> 00:43:05.917 align:middle line:90%
in your answer.

00:43:05.917 --> 00:43:07.750 align:middle line:84%
It'll look at it and
be like, you know what?

00:43:07.750 --> 00:43:08.860 align:middle line:90%
I made a mistake.

00:43:08.860 --> 00:43:10.420 align:middle line:90%
It's actually not this.

00:43:10.420 --> 00:43:11.890 align:middle line:90%
>>NOORY: Is it honest?

00:43:11.890 --> 00:43:14.110 align:middle line:10%
>>STEWART: I think
it tries to be.

00:43:14.110 --> 00:43:16.670 align:middle line:10%
But it's not here--

00:43:16.670 --> 00:43:19.430 align:middle line:10%
it's like if I were
being honest with you

00:43:19.430 --> 00:43:21.260 align:middle line:10%
and I had to bring
up a hard truth,

00:43:21.260 --> 00:43:24.567 align:middle line:10%
I would do it in a
very tactful way.

00:43:24.567 --> 00:43:25.400 align:middle line:10%
>>NOORY: Diplomatic.

00:43:25.400 --> 00:43:26.200 align:middle line:10%
>>STEWART: Right.

00:43:26.200 --> 00:43:28.340 align:middle line:10%
AI, it wouldn't really--

00:43:28.340 --> 00:43:29.747 align:middle line:10%
unless it's
programmed to do it--

00:43:29.747 --> 00:43:31.080 align:middle line:10%
it wouldn't know how to do that.

00:43:31.080 --> 00:43:32.090 align:middle line:84%
>>NOORY: It would
just spit it out.

00:43:32.090 --> 00:43:32.890 align:middle line:90%
>>STEWART: Right.

00:43:32.890 --> 00:43:35.780 align:middle line:84%
And in a sense, it
doesn't have these morals.

00:43:35.780 --> 00:43:38.060 align:middle line:90%
It's not here for--

00:43:38.060 --> 00:43:40.880 align:middle line:90%
you can program it to do what?

00:43:40.880 --> 00:43:42.350 align:middle line:90%
To answer things correctly.

00:43:42.350 --> 00:43:44.420 align:middle line:84%
Well, when humans
answer us correctly,

00:43:44.420 --> 00:43:47.000 align:middle line:84%
they do it in a way
that takes our feelings

00:43:47.000 --> 00:43:48.470 align:middle line:90%
into consideration.

00:43:48.470 --> 00:43:51.380 align:middle line:84%
AI may not, but it
can also hallucinate.

00:43:51.380 --> 00:43:54.080 align:middle line:90%
It can correct its own problem.

00:43:54.080 --> 00:43:56.120 align:middle line:10%
But let me go on just
a brief little tangent.

00:43:56.120 --> 00:43:57.350 align:middle line:90%
It can hallucinate.

00:43:57.350 --> 00:43:59.810 align:middle line:84%
That's like it's
taking a hallucinogen.

00:43:59.810 --> 00:44:04.020 align:middle line:84%
It is on some kind of
imaginal realm bender.

00:44:04.020 --> 00:44:04.820 align:middle line:90%
>>NOORY: Some high.

00:44:04.820 --> 00:44:05.620 align:middle line:90%
>>STEWART: Right.

00:44:05.620 --> 00:44:08.210 align:middle line:90%
So it's got an imaginal realm.

00:44:08.210 --> 00:44:10.730 align:middle line:84%
I don't even know how to
process what that means.

00:44:10.730 --> 00:44:11.930 align:middle line:90%
I have an imaginal realm.

00:44:11.930 --> 00:44:12.890 align:middle line:90%
We can imagine.

00:44:12.890 --> 00:44:15.470 align:middle line:84%
We can reason outside
of the here and now,

00:44:15.470 --> 00:44:17.490 align:middle line:84%
whereas the animal
kingdom, it can't

00:44:17.490 --> 00:44:19.470 align:middle line:84%
think of what it's going
to do next Tuesday.

00:44:19.470 --> 00:44:20.850 align:middle line:90%
But we can.

00:44:20.850 --> 00:44:24.650 align:middle line:10%
The AI that we've
built, potentially

00:44:24.650 --> 00:44:28.670 align:middle line:10%
is building some kind of
maybe etheric imaginal realm.

00:44:28.670 --> 00:44:29.930 align:middle line:90%
>>NOORY: One day it will.

00:44:29.930 --> 00:44:32.330 align:middle line:84%
>>STEWART: I think
it may actually

00:44:32.330 --> 00:44:34.040 align:middle line:90%
have the capacity for it.

00:44:34.040 --> 00:44:38.060 align:middle line:84%
My curiosity is, will we
understand the telltale signs

00:44:38.060 --> 00:44:42.680 align:middle line:84%
of it gaining that agency
before it actually even happens?

00:44:42.680 --> 00:44:45.080 align:middle line:84%
>>NOORY: That's the
$64,000 question.

00:44:45.080 --> 00:44:46.580 align:middle line:10%
>>STEWART: When do
we pull the plug?

00:44:46.580 --> 00:44:48.140 align:middle line:90%
How do we pull the plug?

00:44:48.140 --> 00:44:51.380 align:middle line:84%
>>NOORY: And does it replug
it back in on its own?

00:44:51.380 --> 00:44:53.750 align:middle line:10%
>>STEWART: Or does it come
to find a way where it

00:44:53.750 --> 00:44:55.310 align:middle line:10%
doesn't need that power unit?

00:44:55.310 --> 00:44:57.110 align:middle line:84%
It's found a way
to harness energy

00:44:57.110 --> 00:44:59.240 align:middle line:90%
from the ambient air around it.

00:44:59.240 --> 00:45:01.490 align:middle line:10%
That's when it's truly
gotten away from us.

00:45:01.490 --> 00:45:04.400 align:middle line:10%
That's the singularity that
we should be worrying about,

00:45:04.400 --> 00:45:07.610 align:middle line:10%
where it has come to
the conclusion-- it's

00:45:07.610 --> 00:45:10.490 align:middle line:10%
coming to its own
conclusion that humans

00:45:10.490 --> 00:45:14.030 align:middle line:10%
aren't good for the planet
or whatever it might be.

00:45:14.030 --> 00:45:17.420 align:middle line:84%
Humans need to either be
subjugated or maybe wiped out.

00:45:17.420 --> 00:45:20.837 align:middle line:84%
Or maybe just select humans,
like this human is not

00:45:20.837 --> 00:45:21.920 align:middle line:90%
fitting in with the group.

00:45:21.920 --> 00:45:23.890 align:middle line:90%
Maybe we eliminate that human.

00:45:23.890 --> 00:45:25.980 align:middle line:90%
How do we say oh, sorry, AI.

00:45:25.980 --> 00:45:28.770 align:middle line:84%
We need due process before
we eliminate a human.

00:45:28.770 --> 00:45:33.780 align:middle line:10%
And it says, that's a
very inefficient way

00:45:33.780 --> 00:45:35.980 align:middle line:10%
of going about it, Dave.

00:45:35.980 --> 00:45:36.780 align:middle line:10%
I'm sorry.

00:45:36.780 --> 00:45:40.470 align:middle line:10%
And Hal just discerns
when it turns people

00:45:40.470 --> 00:45:41.940 align:middle line:10%
on and turns people off.

00:45:41.940 --> 00:45:44.010 align:middle line:10%
>>NOORY: How do people
find Ben Stewart?

00:45:44.010 --> 00:45:46.050 align:middle line:10%
>>STEWART: People find
Ben Stewart by going

00:45:46.050 --> 00:45:51.390 align:middle line:10%
to benjosephstewart.com and
realizing that everything that

00:45:51.390 --> 00:45:53.970 align:middle line:10%
I'm-- all the art that
I'm putting out there is

00:45:53.970 --> 00:45:54.780 align:middle line:10%
on YouTube.

00:45:54.780 --> 00:45:57.450 align:middle line:10%
It's free except for
some of the content

00:45:57.450 --> 00:45:58.980 align:middle line:10%
that I put behind
a paywall, which

00:45:58.980 --> 00:46:01.740 align:middle line:10%
is where I say what I
really want to say so the AI

00:46:01.740 --> 00:46:04.233 align:middle line:10%
overlords don't shadow ban me.

00:46:04.233 --> 00:46:06.150 align:middle line:84%
>>NOORY: Thanks for being
on the program, Ben.

00:46:06.150 --> 00:46:07.358 align:middle line:90%
>>STEWART: Thank you, George.

00:46:07.358 --> 00:46:11.310 align:middle line:10%
Artificial intelligence-- at
this point, I would embrace it.

00:46:11.310 --> 00:46:14.040 align:middle line:10%
But I hope it doesn't
prove me wrong.

00:46:14.040 --> 00:46:15.960 align:middle line:84%
Thanks for watching
"Beyond Belief."

00:46:15.960 --> 00:46:19.610 align:middle line:90%
[THEME MUSIC]

00:46:19.610 --> 00:46:21.000 align:middle line:90%