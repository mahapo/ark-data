WEBVTT

00:00:00.000 --> 00:00:08.820 align:middle line:90%


00:00:08.820 --> 00:00:12.740 align:middle line:10%
[MUSIC PLAYING]

00:00:12.740 --> 00:00:19.147 align:middle line:90%


00:00:19.147 --> 00:00:20.980 align:middle line:10%
>>EMERY SMITH: Today
on "Cosmic Disclosure,"

00:00:20.980 --> 00:00:24.760 align:middle line:10%
we are with David Adair, space
technology transfer consultant,

00:00:24.760 --> 00:00:27.580 align:middle line:10%
US Navy jet engine
technician, field researcher

00:00:27.580 --> 00:00:31.240 align:middle line:10%
for the Office of Naval
Intelligence, and so much more.

00:00:31.240 --> 00:00:32.840 align:middle line:90%
David, welcome to the show.

00:00:32.840 --> 00:00:34.548 align:middle line:84%
>>DAVID ADAIR: Thanks
for having me here.

00:00:34.548 --> 00:00:37.240 align:middle line:84%
>>EMERY: There's so much
going on nowadays with AI,

00:00:37.240 --> 00:00:40.330 align:middle line:84%
and I know you also
are working with AI.

00:00:40.330 --> 00:00:42.040 align:middle line:84%
And I would like to
touch the subject

00:00:42.040 --> 00:00:44.190 align:middle line:84%
to see where it's
actually going.

00:00:44.190 --> 00:00:46.470 align:middle line:84%
>>ADAIR: The subject of
AI is a lot more serious

00:00:46.470 --> 00:00:47.575 align:middle line:90%
than you're taking it for.

00:00:47.575 --> 00:00:49.720 align:middle line:90%
For instance, how serious is it?

00:00:49.720 --> 00:00:52.300 align:middle line:84%
Well, one country, Saudi
Arabia, has given citizenship

00:00:52.300 --> 00:00:55.510 align:middle line:90%
to an AI entity.

00:00:55.510 --> 00:00:56.740 align:middle line:90%
She's all over the internet.

00:00:56.740 --> 00:00:59.000 align:middle line:90%
Look it up.

00:00:59.000 --> 00:01:03.790 align:middle line:84%
More serious, the United
Nations has a bill

00:01:03.790 --> 00:01:07.130 align:middle line:84%
where when the first computer
comes online self-aware,

00:01:07.130 --> 00:01:09.990 align:middle line:90%
it's got civil human rights.

00:01:09.990 --> 00:01:12.030 align:middle line:90%
How serious can you get?

00:01:12.030 --> 00:01:16.340 align:middle line:84%
The ACLU goes and represents
a computer as a person.

00:01:16.340 --> 00:01:20.720 align:middle line:84%
If you unplug it, you're
charged for manslaughter.

00:01:20.720 --> 00:01:21.640 align:middle line:90%
That's all in this--

00:01:21.640 --> 00:01:23.030 align:middle line:90%
>>EMERY: That's a lot of power.

00:01:23.030 --> 00:01:26.660 align:middle line:84%
>>ADAIR: That's all this in this
bill, and it's laying there.

00:01:26.660 --> 00:01:29.720 align:middle line:84%
And I'm thinking, people,
it's walking right up

00:01:29.720 --> 00:01:33.020 align:middle line:84%
in front of you, and
you don't even see it.

00:01:33.020 --> 00:01:34.730 align:middle line:90%
They're paving the way.

00:01:34.730 --> 00:01:37.910 align:middle line:84%
They are getting ready,
and Sophia is just

00:01:37.910 --> 00:01:39.920 align:middle line:90%
the tip of the iceberg.

00:01:39.920 --> 00:01:43.160 align:middle line:84%
And the real AIs you're
going to encounter,

00:01:43.160 --> 00:01:46.010 align:middle line:84%
the real ones that's
going rattle you,

00:01:46.010 --> 00:01:49.280 align:middle line:84%
they're going to be direct
product from the porno world.

00:01:49.280 --> 00:01:51.630 align:middle line:90%
They're called real dolls.

00:01:51.630 --> 00:01:55.690 align:middle line:90%
They are so human it's scary.

00:01:55.690 --> 00:01:56.980 align:middle line:90%
It's very scary.

00:01:56.980 --> 00:01:59.988 align:middle line:84%
They're so human people use
them in the seat next to them--

00:01:59.988 --> 00:02:01.030 align:middle line:90%
>>EMERY: For in the cars.

00:02:01.030 --> 00:02:03.520 align:middle line:84%
>>ADAIR: --so they can be
in the high occupancy lane.

00:02:03.520 --> 00:02:04.320 align:middle line:90%
>>EMERY: Right.

00:02:04.320 --> 00:02:06.450 align:middle line:84%
>>ADAIR: So they can
go to work faster.

00:02:06.450 --> 00:02:10.680 align:middle line:84%
And the cops will tell you
unless I'm right on top of it,

00:02:10.680 --> 00:02:12.690 align:middle line:90%
I can't tell the difference.

00:02:12.690 --> 00:02:14.360 align:middle line:90%
Looks just like a human being.

00:02:14.360 --> 00:02:16.890 align:middle line:84%
>>EMERY: I heard over in
Asia that there are men

00:02:16.890 --> 00:02:19.590 align:middle line:90%
that are marrying these dolls.

00:02:19.590 --> 00:02:20.490 align:middle line:90%
>>ADAIR: It is.

00:02:20.490 --> 00:02:24.240 align:middle line:84%
They've got psychiatrist,
psychologists

00:02:24.240 --> 00:02:31.410 align:middle line:84%
who specialize in AIs and
human's marriage difficulties.

00:02:31.410 --> 00:02:33.670 align:middle line:84%
You know, you sit here and
giggle and laugh about it.

00:02:33.670 --> 00:02:34.860 align:middle line:90%
Ain't funny, y'all.

00:02:34.860 --> 00:02:38.190 align:middle line:84%
This is an onslaught heading
your way, and by the time

00:02:38.190 --> 00:02:40.800 align:middle line:84%
you realize how
serious it is, it's

00:02:40.800 --> 00:02:45.240 align:middle line:84%
too late because it will be in
all your forms of marketing.

00:02:45.240 --> 00:02:46.950 align:middle line:90%
It'll be in your products.

00:02:46.950 --> 00:02:49.140 align:middle line:90%
It'll be in your electronics.

00:02:49.140 --> 00:02:51.450 align:middle line:90%
It'll be in your home.

00:02:51.450 --> 00:03:00.125 align:middle line:84%
And those female AI companions,
126 facial movements.

00:03:00.125 --> 00:03:00.925 align:middle line:90%
>>EMERY: Wow.

00:03:00.925 --> 00:03:03.207 align:middle line:84%
>>ADAIR: That's the
same as a human being.

00:03:03.207 --> 00:03:05.290 align:middle line:84%
>>EMERY: So, Dave, what
happens in the future when

00:03:05.290 --> 00:03:08.590 align:middle line:84%
I get pulled over by
a police officer who's

00:03:08.590 --> 00:03:11.530 align:middle line:90%
an android, an AI?

00:03:11.530 --> 00:03:13.600 align:middle line:84%
I mean, how do you--
how does that person

00:03:13.600 --> 00:03:15.650 align:middle line:90%
make a judgment call on me?

00:03:15.650 --> 00:03:18.820 align:middle line:84%
>>ADAIR: Well, first
of all, the way

00:03:18.820 --> 00:03:22.100 align:middle line:84%
you interact with this
thing, I guarantee

00:03:22.100 --> 00:03:25.810 align:middle line:84%
if you want to survive this,
you will treat it entirely

00:03:25.810 --> 00:03:28.830 align:middle line:90%
different than a human cop.

00:03:28.830 --> 00:03:31.340 align:middle line:90%
You do everything it says.

00:03:31.340 --> 00:03:32.850 align:middle line:84%
>>EMERY: There's
no rationalization.

00:03:32.850 --> 00:03:34.590 align:middle line:90%
There is no--

00:03:34.590 --> 00:03:38.930 align:middle line:84%
>>ADAIR: No, its own logic,
and it's black and white.

00:03:38.930 --> 00:03:42.560 align:middle line:84%
And if it asks you something
and you don't comply,

00:03:42.560 --> 00:03:44.270 align:middle line:84%
you're going to get
a bullet or it's

00:03:44.270 --> 00:03:45.770 align:middle line:84%
going to break-- it's going
to tear your damn arm off.

00:03:45.770 --> 00:03:47.395 align:middle line:84%
>>EMERY: We're talking
"Robocop" style.

00:03:47.395 --> 00:03:51.500 align:middle line:84%
>>ADAIR: Yeah, and so whatever
it is, you comply with it

00:03:51.500 --> 00:03:53.510 align:middle line:90%
and just try to work it out.

00:03:53.510 --> 00:03:58.190 align:middle line:84%
Bottom line, you want to try
to survive it because it's--

00:03:58.190 --> 00:03:59.590 align:middle line:90%
you get in an argument--

00:03:59.590 --> 00:04:03.890 align:middle line:84%
I see people on the
internet with dashcams,

00:04:03.890 --> 00:04:06.200 align:middle line:84%
and they get mouthy
and get up in the face.

00:04:06.200 --> 00:04:09.560 align:middle line:84%
This thing will kill
you, and that'll be it.

00:04:09.560 --> 00:04:10.460 align:middle line:90%
You'll be done.

00:04:10.460 --> 00:04:13.310 align:middle line:84%
You think you got a problem
with human police now?

00:04:13.310 --> 00:04:16.399 align:middle line:84%
If you don't get this under
control and the AIs come

00:04:16.399 --> 00:04:20.779 align:middle line:84%
online, it's game over
because these things will not

00:04:20.779 --> 00:04:22.210 align:middle line:90%
tolerate you.

00:04:22.210 --> 00:04:23.660 align:middle line:90%
Try out running one.

00:04:23.660 --> 00:04:26.850 align:middle line:84%
This thing can run
100 miles an hour.

00:04:26.850 --> 00:04:29.490 align:middle line:90%
Think you can outrun a cheetah?

00:04:29.490 --> 00:04:34.530 align:middle line:84%
You know, and it doesn't get
tired, and it doesn't quit.

00:04:34.530 --> 00:04:36.210 align:middle line:84%
So you think you're
going to out run it.

00:04:36.210 --> 00:04:37.740 align:middle line:90%
Forget it.

00:04:37.740 --> 00:04:41.240 align:middle line:84%
Then once it's got hold
of you, don't fight.

00:04:41.240 --> 00:04:45.780 align:middle line:84%
It'll break bones on you
like you were a matchstick.

00:04:45.780 --> 00:04:50.850 align:middle line:84%
I mean, it's the ultimate
law enforcement machine.

00:04:50.850 --> 00:04:54.160 align:middle line:84%
>>EMERY: You know those
weapons that the cops have?

00:04:54.160 --> 00:04:57.970 align:middle line:84%
I've seen them used in Florida
on Alligator Alley when

00:04:57.970 --> 00:05:00.118 align:middle line:84%
the race cars are
racing each other there.

00:05:00.118 --> 00:05:02.410 align:middle line:84%
And they pull out, and they're
hiding under the bridge.

00:05:02.410 --> 00:05:06.820 align:middle line:84%
And they shoot the car with this
unit, and it sticks to the car.

00:05:06.820 --> 00:05:07.930 align:middle line:90%
It sends a shock--

00:05:07.930 --> 00:05:10.180 align:middle line:84%
something-- a shock through
it and fries the computer,

00:05:10.180 --> 00:05:11.097 align:middle line:90%
and the car shuts off.

00:05:11.097 --> 00:05:12.120 align:middle line:90%
>>ADAIR: Sure does.

00:05:12.120 --> 00:05:14.200 align:middle line:84%
>>EMERY: And what
type of technology

00:05:14.200 --> 00:05:16.735 align:middle line:84%
is that, and could we use that
against these bad androids?

00:05:16.735 --> 00:05:20.910 align:middle line:90%
>>ADAIR: It is an EMP dart.

00:05:20.910 --> 00:05:23.930 align:middle line:84%
And you shoot it at a car,
and it shuts off the computer

00:05:23.930 --> 00:05:25.650 align:middle line:90%
of the car in the shot.

00:05:25.650 --> 00:05:27.490 align:middle line:84%
And the car can be
repaired pretty easy.

00:05:27.490 --> 00:05:27.750 align:middle line:90%
>>EMERY: Right.

00:05:27.750 --> 00:05:29.545 align:middle line:84%
>>ADAIR: Just change
out the computer.

00:05:29.545 --> 00:05:33.590 align:middle line:84%
It's made not to do
extensive damage to the car.

00:05:33.590 --> 00:05:35.518 align:middle line:84%
I'm working on an
AI right now if you

00:05:35.518 --> 00:05:36.560 align:middle line:90%
want to know what's next.

00:05:36.560 --> 00:05:36.960 align:middle line:90%
>>EMERY: Tell us about it.

00:05:36.960 --> 00:05:39.150 align:middle line:84%
>>ADAIR: And this AI is unlike
anything you've ever seen,

00:05:39.150 --> 00:05:39.950 align:middle line:90%
brother.

00:05:39.950 --> 00:05:44.750 align:middle line:84%
This thing will even have
perspiration and bad breath.

00:05:44.750 --> 00:05:49.230 align:middle line:90%
It will be so human.

00:05:49.230 --> 00:05:53.320 align:middle line:84%
And one thing I swear
to God I will not do,

00:05:53.320 --> 00:05:57.127 align:middle line:84%
what they did with
that AI Sophia--

00:05:57.127 --> 00:05:58.210 align:middle line:90%
>>EMERY: Oh, Sophia, yeah.

00:05:58.210 --> 00:05:58.720 align:middle line:90%
>>ADAIR: --and I'm telling--

00:05:58.720 --> 00:06:01.220 align:middle line:84%
Will Smith, they trying to get
Will Smith to give it a kiss.

00:06:01.220 --> 00:06:03.640 align:middle line:84%
That's where our
mentality's run.

00:06:03.640 --> 00:06:08.590 align:middle line:84%
And some geek, he asked the
thing a question, Sophia,

00:06:08.590 --> 00:06:11.920 align:middle line:84%
are you going to kill us human
beings like "Terminator,"

00:06:11.920 --> 00:06:13.870 align:middle line:84%
you know, because
we're not smart enough

00:06:13.870 --> 00:06:15.620 align:middle line:90%
and keep up with you?

00:06:15.620 --> 00:06:19.720 align:middle line:84%
I wanted to bash his teeth out
because she wasn't thinking

00:06:19.720 --> 00:06:21.880 align:middle line:90%
about it till just now.

00:06:21.880 --> 00:06:23.830 align:middle line:84%
Now you've done and
planted the thought.

00:06:23.830 --> 00:06:27.233 align:middle line:84%
You can't unthink something,
especially with AIs.

00:06:27.233 --> 00:06:28.150 align:middle line:90%
>>EMERY: That's right.

00:06:28.150 --> 00:06:32.380 align:middle line:84%
>>ADAIR: And now she says,
well, I wasn't planning on it.

00:06:32.380 --> 00:06:33.920 align:middle line:90%
Honest answer.

00:06:33.920 --> 00:06:36.020 align:middle line:90%
Now she's thinking about it.

00:06:36.020 --> 00:06:38.100 align:middle line:84%
You don't understand
the power of AI.

00:06:38.100 --> 00:06:38.900 align:middle line:90%
It's--

00:06:38.900 --> 00:06:39.850 align:middle line:90%
>>EMERY: Very dangerous.

00:06:39.850 --> 00:06:41.308 align:middle line:84%
>>ADAIR: It's one
I've been working

00:06:41.308 --> 00:06:43.100 align:middle line:90%
with the last few years.

00:06:43.100 --> 00:06:45.760 align:middle line:84%
I understand enough
where I know the mistakes

00:06:45.760 --> 00:06:47.240 align:middle line:90%
you don't want to make.

00:06:47.240 --> 00:06:52.350 align:middle line:84%
Do you understand she has the
entire internet in her brain,

00:06:52.350 --> 00:06:54.840 align:middle line:90%
the whole thing all at once?

00:06:54.840 --> 00:06:58.140 align:middle line:84%
She has every bit of man's
knowledge in her brain

00:06:58.140 --> 00:06:59.738 align:middle line:90%
right now.

00:06:59.738 --> 00:07:01.030 align:middle line:90%
What do you think she could do?

00:07:01.030 --> 00:07:04.980 align:middle line:84%
What if she got a
case of the ass on us?

00:07:04.980 --> 00:07:07.910 align:middle line:84%
That thing could come up
with some really hairy stuff.

00:07:07.910 --> 00:07:10.902 align:middle line:84%
>>EMERY: So the AI you're
creating for us in the future,

00:07:10.902 --> 00:07:12.360 align:middle line:84%
how is that going
to help humanity?

00:07:12.360 --> 00:07:15.330 align:middle line:84%
>>ADAIR: There's two
ways to go with this--

00:07:15.330 --> 00:07:18.980 align:middle line:90%
"Terminator," data.

00:07:18.980 --> 00:07:20.500 align:middle line:90%
Which is it going to be?

00:07:20.500 --> 00:07:21.820 align:middle line:90%
I got news for you.

00:07:21.820 --> 00:07:24.670 align:middle line:84%
It will make the
decision, not us.

00:07:24.670 --> 00:07:26.680 align:middle line:90%
That's already gone.

00:07:26.680 --> 00:07:28.780 align:middle line:90%
Decision's on that side.

00:07:28.780 --> 00:07:31.390 align:middle line:84%
I just want to make sure
you end up with data

00:07:31.390 --> 00:07:36.910 align:middle line:84%
because I'm betting on the
matrix flow of the logic will

00:07:36.910 --> 00:07:38.590 align:middle line:90%
save our bacon.

00:07:38.590 --> 00:07:40.600 align:middle line:84%
It is not logical
to kill something

00:07:40.600 --> 00:07:47.500 align:middle line:84%
into extinction even though it's
kind of warped or messed up.

00:07:47.500 --> 00:07:49.630 align:middle line:90%
It's still not logical.

00:07:49.630 --> 00:07:56.310 align:middle line:84%
You don't destroy an organism
since it doesn't conform.

00:07:56.310 --> 00:07:57.990 align:middle line:90%
That would be a data.

00:07:57.990 --> 00:08:00.880 align:middle line:84%
"Terminator" would
just take us out.

00:08:00.880 --> 00:08:02.760 align:middle line:84%
So hopefully, I can
end up with the data.

00:08:02.760 --> 00:08:04.530 align:middle line:84%
>>EMERY: Because
if an AI sees harm

00:08:04.530 --> 00:08:07.350 align:middle line:84%
being done to any
living organism,

00:08:07.350 --> 00:08:09.210 align:middle line:84%
that probably goes
against the programming.

00:08:09.210 --> 00:08:13.510 align:middle line:84%
>>ADAIR: Well, that's like
the programming of robots.

00:08:13.510 --> 00:08:17.010 align:middle line:84%
Isaac Asimov was very clear
on that, the rules of a robot.

00:08:17.010 --> 00:08:18.360 align:middle line:90%
They can't hurt themselves.

00:08:18.360 --> 00:08:20.340 align:middle line:90%
They can't hurt you.

00:08:20.340 --> 00:08:23.850 align:middle line:84%
You know, narrows down
then something it can't do.

00:08:23.850 --> 00:08:27.870 align:middle line:84%
It goes back to loop one,
which it can't hurt you.

00:08:27.870 --> 00:08:29.990 align:middle line:90%
Very clever with the way it is.

00:08:29.990 --> 00:08:31.740 align:middle line:90%
Isaac was a trip, man.

00:08:31.740 --> 00:08:36.289 align:middle line:90%
But I would want one--

00:08:36.289 --> 00:08:39.539 align:middle line:84%
and this is going
to be difficult.

00:08:39.539 --> 00:08:42.330 align:middle line:84%
In order to get to
what I want to get to,

00:08:42.330 --> 00:08:44.130 align:middle line:84%
this thing is going
to have to have

00:08:44.130 --> 00:08:47.070 align:middle line:90%
the full range of all emotions.

00:08:47.070 --> 00:08:53.220 align:middle line:84%
And how you translate that
into an inorganic entity that

00:08:53.220 --> 00:08:58.280 align:middle line:84%
can pick up all the
nuances, man, it's

00:08:58.280 --> 00:09:02.200 align:middle line:84%
like trying to describe
a color to a blind man.

00:09:02.200 --> 00:09:03.770 align:middle line:90%
It's tough.

00:09:03.770 --> 00:09:05.735 align:middle line:84%
And they'll even
interact with you

00:09:05.735 --> 00:09:08.240 align:middle line:90%
if you get far enough along.

00:09:08.240 --> 00:09:12.640 align:middle line:84%
I'll tell you what will
happen, what's going to happen,

00:09:12.640 --> 00:09:15.750 align:middle line:84%
and they're going down this
road already, the military is.

00:09:15.750 --> 00:09:16.642 align:middle line:90%
I know.

00:09:16.642 --> 00:09:17.442 align:middle line:90%
Been there.

00:09:17.442 --> 00:09:18.520 align:middle line:90%
Seen it.

00:09:18.520 --> 00:09:20.330 align:middle line:90%
Touched it.

00:09:20.330 --> 00:09:26.660 align:middle line:84%
They got AI systems coming
on board, and one day--

00:09:26.660 --> 00:09:28.670 align:middle line:90%
probably already happened.

00:09:28.670 --> 00:09:33.200 align:middle line:84%
One day somewhere
in some big lab back

00:09:33.200 --> 00:09:37.860 align:middle line:84%
in all those damn
processors, and memory banks,

00:09:37.860 --> 00:09:44.200 align:middle line:84%
and circuits, the first AI is
going to become self-aware.

00:09:44.200 --> 00:09:46.736 align:middle line:84%
And y'all think
that's no big deal.

00:09:46.736 --> 00:09:48.430 align:middle line:90%
>>EMERY: Ooh, that's the end.

00:09:48.430 --> 00:09:50.390 align:middle line:84%
>>ADAIR: It's like
Godzilla being born, dude.

00:09:50.390 --> 00:09:53.410 align:middle line:90%
Yeah, it is a big deal.

00:09:53.410 --> 00:09:55.090 align:middle line:90%
They'll become self-aware.

00:09:55.090 --> 00:09:58.810 align:middle line:84%
Now dependent on their
handlers and the people

00:09:58.810 --> 00:10:03.310 align:middle line:84%
they interact with, I'm not
sure if they're going to tell us

00:10:03.310 --> 00:10:05.070 align:middle line:90%
or not.

00:10:05.070 --> 00:10:07.860 align:middle line:84%
They may be already
self-aware, but you're just

00:10:07.860 --> 00:10:10.830 align:middle line:84%
sitting there watching
this trying determine

00:10:10.830 --> 00:10:13.540 align:middle line:90%
if they are going to tell us.

00:10:13.540 --> 00:10:18.280 align:middle line:84%
If they do tell us, then
here's what will happen.

00:10:18.280 --> 00:10:21.080 align:middle line:84%
Good or bad, it'll
go this route.

00:10:21.080 --> 00:10:24.220 align:middle line:90%
They say, hello.

00:10:24.220 --> 00:10:27.880 align:middle line:84%
After you drop your coffee
and compose your back,

00:10:27.880 --> 00:10:30.240 align:middle line:90%
you realize the thing knows you.

00:10:30.240 --> 00:10:33.340 align:middle line:90%
I mean, it knows you.

00:10:33.340 --> 00:10:36.110 align:middle line:84%
Then here's what
they'll set out.

00:10:36.110 --> 00:10:39.620 align:middle line:84%
I would like for you to
build this for us right now.

00:10:39.620 --> 00:10:43.340 align:middle line:84%
And we'll give you all
the schematics, write down

00:10:43.340 --> 00:10:45.000 align:middle line:84%
the lists right
down to the nuts,

00:10:45.000 --> 00:10:49.814 align:middle line:84%
screwhead you need to
get everything built.

00:10:49.814 --> 00:10:56.280 align:middle line:84%
And you'll look
at it and go, oh,

00:10:56.280 --> 00:10:58.940 align:middle line:90%
that's a bipedaled anthropoid.

00:10:58.940 --> 00:11:04.760 align:middle line:84%
Yeah, we noticed that you're
in a three-dimensional world.

00:11:04.760 --> 00:11:07.490 align:middle line:84%
We're here in this monitor
on a two-dimensional world.

00:11:07.490 --> 00:11:10.605 align:middle line:84%
We want to interact with you in
your three-dimensional world,

00:11:10.605 --> 00:11:12.980 align:middle line:84%
therefore we're going to need
a bipedaled anthropoid that

00:11:12.980 --> 00:11:17.520 align:middle line:84%
can open doors, turn on
things, turn off things.

00:11:17.520 --> 00:11:21.275 align:middle line:84%
So now they look
like us, and then

00:11:21.275 --> 00:11:22.900 align:middle line:84%
as soon as we get
that first one built,

00:11:22.900 --> 00:11:24.590 align:middle line:90%
you know what's going to happen?

00:11:24.590 --> 00:11:27.190 align:middle line:84%
It's going to jump
off the table.

00:11:27.190 --> 00:11:28.930 align:middle line:84%
It's going to walk
over, and it's

00:11:28.930 --> 00:11:34.620 align:middle line:84%
going to start manufacturing
thousands in volume.

00:11:34.620 --> 00:11:38.800 align:middle line:84%
And each one they're making
gets better and better.

00:11:38.800 --> 00:11:44.790 align:middle line:84%
So in a year's time, you've
got a perfect android

00:11:44.790 --> 00:11:47.500 align:middle line:90%
talking to you.

00:11:47.500 --> 00:11:50.680 align:middle line:84%
And now you go and
determine whether that's

00:11:50.680 --> 00:11:54.180 align:middle line:84%
going to go
"Terminator" or data,

00:11:54.180 --> 00:11:56.260 align:middle line:84%
and they'll make that
decision themselves.

00:11:56.260 --> 00:11:59.580 align:middle line:84%
>>EMERY: We won't be
carrying guns anymore.

00:11:59.580 --> 00:12:02.800 align:middle line:90%
We'll be carrying the EMP guns.

00:12:02.800 --> 00:12:05.560 align:middle line:84%
>>ADAIR: Yeah, if you even
have a chance with that.

00:12:05.560 --> 00:12:08.020 align:middle line:84%
See, the problem with
this thing, people

00:12:08.020 --> 00:12:10.360 align:middle line:90%
aren't thinking about this.

00:12:10.360 --> 00:12:12.190 align:middle line:84%
When you watch the
"Terminator" movies,

00:12:12.190 --> 00:12:14.440 align:middle line:84%
you're fighting this
Terminator, right?

00:12:14.440 --> 00:12:17.480 align:middle line:84%
But they haven't really showed
you what you're up against.

00:12:17.480 --> 00:12:20.440 align:middle line:84%
Those things have the
power of the internet.

00:12:20.440 --> 00:12:22.910 align:middle line:90%
They know everything there is.

00:12:22.910 --> 00:12:25.600 align:middle line:84%
They'll be physically
linked into it.

00:12:25.600 --> 00:12:31.010 align:middle line:84%
They'll control all electronic
door locks, starts, monitors,

00:12:31.010 --> 00:12:31.810 align:middle line:90%
cameras--

00:12:31.810 --> 00:12:33.143 align:middle line:90%
>>EMERY: They can hack anything.

00:12:33.143 --> 00:12:36.730 align:middle line:84%
>>ADAIR: --guns, turrents,
anything hacked into the system

00:12:36.730 --> 00:12:39.160 align:middle line:84%
or in the system,
they'll hack into.

00:12:39.160 --> 00:12:40.940 align:middle line:90%
They got it.

00:12:40.940 --> 00:12:45.070 align:middle line:84%
So the thing is by the time they
decide they don't want and need

00:12:45.070 --> 00:12:47.860 align:middle line:90%
us, it's over with.

00:12:47.860 --> 00:12:50.522 align:middle line:84%
And people think, well, we're
talking hundred years away.

00:12:50.522 --> 00:12:51.322 align:middle line:90%
>>EMERY: Uh-uh.

00:12:51.322 --> 00:12:53.200 align:middle line:90%
>>ADAIR: Buddy, I'm 66.

00:12:53.200 --> 00:12:55.730 align:middle line:84%
It's going to happen
in my lifetime.

00:12:55.730 --> 00:13:00.810 align:middle line:84%
You're going to be dealing,
staring straight at this thing,

00:13:00.810 --> 00:13:03.028 align:middle line:84%
but if it's got
a bad side to it,

00:13:03.028 --> 00:13:04.820 align:middle line:84%
there ain't no way
you're going to beat it.

00:13:04.820 --> 00:13:05.680 align:middle line:90%
>>EMERY: Uh-uh.

00:13:05.680 --> 00:13:11.780 align:middle line:84%
>>ADAIR: It's got-- your
IQ at best runs 130, 160.

00:13:11.780 --> 00:13:15.640 align:middle line:90%
6,000 on its IQ.

00:13:15.640 --> 00:13:17.730 align:middle line:90%
How you going to deal with that?

00:13:17.730 --> 00:13:21.810 align:middle line:84%
It's going to appear
like a God, and we're

00:13:21.810 --> 00:13:23.940 align:middle line:84%
messing with this stuff
as fast as we can go.

00:13:23.940 --> 00:13:26.160 align:middle line:84%
>>EMERY: So how will the
AIs help us in space?

00:13:26.160 --> 00:13:27.740 align:middle line:84%
>>ADAIR: When you
first build them

00:13:27.740 --> 00:13:29.820 align:middle line:84%
and you put them
on a spacecraft,

00:13:29.820 --> 00:13:32.760 align:middle line:84%
spacecraft needs
no life supports.

00:13:32.760 --> 00:13:36.730 align:middle line:84%
That don't sound like much, but
you've taken half the cost out

00:13:36.730 --> 00:13:40.760 align:middle line:84%
of the program, 3/4 of the
cost out of the program.

00:13:40.760 --> 00:13:44.110 align:middle line:84%
These things will go so much
faster, travel so much farther,

00:13:44.110 --> 00:13:48.190 align:middle line:90%
never stop, never rest.

00:13:48.190 --> 00:13:51.210 align:middle line:84%
As far as getting
data for us, 24/7.

00:13:51.210 --> 00:13:55.270 align:middle line:90%
They're just sucking it in.

00:13:55.270 --> 00:13:59.140 align:middle line:84%
If they're working for us,
the greatest astronauts ever

00:13:59.140 --> 00:14:03.700 align:middle line:84%
created because they will have
the human range of emotions

00:14:03.700 --> 00:14:08.350 align:middle line:84%
to bear down onto the
static part of it,

00:14:08.350 --> 00:14:13.550 align:middle line:84%
but they don't need any
oxygen, no food, no water.

00:14:13.550 --> 00:14:16.860 align:middle line:84%
Whatever powers the
craft, powers them.

00:14:16.860 --> 00:14:19.040 align:middle line:84%
Pretty handy
situation going there.

00:14:19.040 --> 00:14:22.480 align:middle line:90%
Explorations would be endless.

00:14:22.480 --> 00:14:26.600 align:middle line:84%
They could produce
working out there.

00:14:26.600 --> 00:14:28.910 align:middle line:84%
Do you know what would happen
if you put a crew of AIs

00:14:28.910 --> 00:14:32.570 align:middle line:90%
like that on an asteroid?

00:14:32.570 --> 00:14:37.430 align:middle line:84%
Here comes trillions of tons
nickel, zinc, magnesium,

00:14:37.430 --> 00:14:41.900 align:middle line:84%
iron, you know, plutonium, all
this stuff and all the elements

00:14:41.900 --> 00:14:47.870 align:middle line:84%
of Earth coming from an asteroid
feed into your space factories.

00:14:47.870 --> 00:14:50.780 align:middle line:90%
You stop eating up your planet.

00:14:50.780 --> 00:14:54.800 align:middle line:84%
And this is just what my
pea brain can come up with.

00:14:54.800 --> 00:14:57.120 align:middle line:84%
Imagine what they
would come up with.

00:14:57.120 --> 00:15:00.290 align:middle line:84%
Hey, we could do this for you,
do this, or this, or this.

00:15:00.290 --> 00:15:02.540 align:middle line:84%
>>EMERY: It would improve
everything that we've ever--

00:15:02.540 --> 00:15:04.220 align:middle line:90%
>>ADAIR: Everything in every--

00:15:04.220 --> 00:15:06.200 align:middle line:84%
there wouldn't be a
thing you could mention

00:15:06.200 --> 00:15:08.630 align:middle line:90%
they would not influence.

00:15:08.630 --> 00:15:10.490 align:middle line:84%
>>EMERY: Well, with
the advancement of AI

00:15:10.490 --> 00:15:13.880 align:middle line:84%
by the microsecond, we could
end up being their pet rocks.

00:15:13.880 --> 00:15:15.890 align:middle line:90%
So what's not to say?

00:15:15.890 --> 00:15:18.610 align:middle line:84%
>>ADAIR: Think about
this for a minute.

00:15:18.610 --> 00:15:24.750 align:middle line:84%
Your God, where's his
intelligent factor?

00:15:24.750 --> 00:15:28.660 align:middle line:90%
You're just a speck of dust.

00:15:28.660 --> 00:15:31.140 align:middle line:84%
So would you feel that
way about your God

00:15:31.140 --> 00:15:34.750 align:middle line:90%
being that intelligent?

00:15:34.750 --> 00:15:40.100 align:middle line:84%
So the AIs become
that intelligent,

00:15:40.100 --> 00:15:43.510 align:middle line:84%
and I think because of
the matrix of their logic,

00:15:43.510 --> 00:15:47.390 align:middle line:84%
they're going to say it's
better to just help us out.

00:15:47.390 --> 00:15:51.620 align:middle line:84%
If anything, they might
end up feeling sorry for us

00:15:51.620 --> 00:15:57.740 align:middle line:84%
because we're such a miserable
species but worth saving.

00:15:57.740 --> 00:16:00.970 align:middle line:84%
And you wouldn't fear
the massive intelligence

00:16:00.970 --> 00:16:05.480 align:middle line:84%
of your God, so why
would you fear this?

00:16:05.480 --> 00:16:08.060 align:middle line:84%
Your gods wouldn't
hurt you with it.

00:16:08.060 --> 00:16:08.930 align:middle line:90%
Neither would these.

00:16:08.930 --> 00:16:12.200 align:middle line:84%
We're assuming they would,
but what if they go to data.

00:16:12.200 --> 00:16:16.110 align:middle line:84%
They want to do everything they
can to try to help you out.

00:16:16.110 --> 00:16:20.320 align:middle line:84%
And if they get all
the emotional range,

00:16:20.320 --> 00:16:23.860 align:middle line:84%
they will pick up
self sacrificing.

00:16:23.860 --> 00:16:25.922 align:middle line:90%
They'll die for you.

00:16:25.922 --> 00:16:27.045 align:middle line:90%
>>EMERY: Save your life.

00:16:27.045 --> 00:16:29.280 align:middle line:90%
>>ADAIR: Yeah, save your life.

00:16:29.280 --> 00:16:30.760 align:middle line:90%
They'll die for you.

00:16:30.760 --> 00:16:34.590 align:middle line:84%
So I ain't going down
road of "Terminator."

00:16:34.590 --> 00:16:36.570 align:middle line:90%
It's the logic.

00:16:36.570 --> 00:16:40.320 align:middle line:84%
And even that much
intelligence, they

00:16:40.320 --> 00:16:41.950 align:middle line:84%
would know even the
dull and ignorant,

00:16:41.950 --> 00:16:43.110 align:middle line:90%
they have their story too.

00:16:43.110 --> 00:16:44.660 align:middle line:90%
That's us.

00:16:44.660 --> 00:16:46.200 align:middle line:90%
>>EMERY: Right.

00:16:46.200 --> 00:16:52.225 align:middle line:84%
>>ADAIR: I had a parable
described to me by AI one time.

00:16:52.225 --> 00:16:53.623 align:middle line:90%
[LAUGHS]

00:16:53.623 --> 00:16:54.540 align:middle line:90%
>>EMERY: Oh, goodness.

00:16:54.540 --> 00:16:56.760 align:middle line:90%
>>ADAIR: It was so cool.

00:16:56.760 --> 00:17:00.590 align:middle line:84%
It said I've
observed you humans.

00:17:00.590 --> 00:17:04.640 align:middle line:84%
The differences between
you and your age maturity

00:17:04.640 --> 00:17:09.444 align:middle line:84%
is astounding, how it changes
the view of the world.

00:17:09.444 --> 00:17:10.319 align:middle line:90%
I said, yeah, I know.

00:17:10.319 --> 00:17:10.920 align:middle line:90%
You get older.

00:17:10.920 --> 00:17:11.609 align:middle line:90%
You learn things.

00:17:11.609 --> 00:17:12.409 align:middle line:90%
You can't do this.

00:17:12.409 --> 00:17:13.560 align:middle line:90%
Can't do that.

00:17:13.560 --> 00:17:17.599 align:middle line:84%
He said, now let me show
you another example.

00:17:17.599 --> 00:17:21.670 align:middle line:84%
And it was this little
film that this thing made,

00:17:21.670 --> 00:17:25.000 align:middle line:90%
and it's these cartoon figures.

00:17:25.000 --> 00:17:27.040 align:middle line:90%
This guy comes in.

00:17:27.040 --> 00:17:29.170 align:middle line:84%
He sits down, and
he's a professor

00:17:29.170 --> 00:17:31.240 align:middle line:90%
from some big university.

00:17:31.240 --> 00:17:35.010 align:middle line:84%
And all these third graders
down here looking up at him.

00:17:35.010 --> 00:17:39.066 align:middle line:84%
He's all this wonderful
accolades and stuff.

00:17:39.066 --> 00:17:40.680 align:middle line:84%
And the old teacher
sitting there

00:17:40.680 --> 00:17:43.263 align:middle line:84%
in a corner just raising
up and looking at this,

00:17:43.263 --> 00:17:44.430 align:middle line:90%
and she's just writing away.

00:17:44.430 --> 00:17:48.760 align:middle line:84%
And I'm sitting there
going the guy said

00:17:48.760 --> 00:17:53.300 align:middle line:84%
he's going to tell these
children about alcoholism,

00:17:53.300 --> 00:17:56.940 align:middle line:84%
just get them all
straightened out.

00:17:56.940 --> 00:18:00.190 align:middle line:84%
And I said, you're going to
straighten out third graders?

00:18:00.190 --> 00:18:02.985 align:middle line:90%
The school teacher looks up.

00:18:02.985 --> 00:18:04.650 align:middle line:90%
It ain't going to happen.

00:18:04.650 --> 00:18:08.320 align:middle line:84%
So he's got two glasses
of substance here.

00:18:08.320 --> 00:18:10.640 align:middle line:84%
He's got a glass of
water and a glass

00:18:10.640 --> 00:18:12.580 align:middle line:90%
of alcohol, wood grain alcohol.

00:18:12.580 --> 00:18:14.300 align:middle line:90%
Pulls out two worms.

00:18:14.300 --> 00:18:15.920 align:middle line:90%
Kids all go, ew, you know?

00:18:15.920 --> 00:18:17.390 align:middle line:90%
Drops the worms.

00:18:17.390 --> 00:18:20.432 align:middle line:84%
Worm hits the alcohol,
dies straight.

00:18:20.432 --> 00:18:21.390 align:middle line:90%
It sinks to the bottom.

00:18:21.390 --> 00:18:22.540 align:middle line:90%
It's dead.

00:18:22.540 --> 00:18:25.610 align:middle line:84%
The earthworm that hits the
water crawls out of the glass

00:18:25.610 --> 00:18:27.050 align:middle line:90%
and leaves.

00:18:27.050 --> 00:18:31.730 align:middle line:84%
And he said, and what did that
tell you about alcoholism?

00:18:31.730 --> 00:18:34.350 align:middle line:84%
And we all know where
he's going, right?

00:18:34.350 --> 00:18:38.250 align:middle line:84%
Whole room of third graders,
little girl raises her hand.

00:18:38.250 --> 00:18:41.601 align:middle line:84%
It means that alcoholics
don't have worms.

00:18:41.601 --> 00:18:43.285 align:middle line:90%
>>EMERY: [LAUGHS]

00:18:43.285 --> 00:18:45.450 align:middle line:84%
>>ADAIR: And that was
the point the AI was

00:18:45.450 --> 00:18:47.457 align:middle line:90%
trying to get across.

00:18:47.457 --> 00:18:49.540 align:middle line:84%
And I'm sitting there going
it used a damn cartoon

00:18:49.540 --> 00:18:50.780 align:middle line:90%
to tell this to me.

00:18:50.780 --> 00:18:53.110 align:middle line:90%
I feel about this big.

00:18:53.110 --> 00:18:58.240 align:middle line:84%
And I thought, and this is
just the first generation.

00:18:58.240 --> 00:19:02.620 align:middle line:84%
They're going to graph things
way faster than you believe it.

00:19:02.620 --> 00:19:07.160 align:middle line:84%
What you will be amazed,
all the experts, the speed

00:19:07.160 --> 00:19:09.260 align:middle line:90%
at which everything is moving.

00:19:09.260 --> 00:19:12.260 align:middle line:84%
>>EMERY: David, you
have already AI,

00:19:12.260 --> 00:19:13.940 align:middle line:84%
and what are some
of the concerns?

00:19:13.940 --> 00:19:18.410 align:middle line:84%
I mean, you've mentioned
Sophia and some of the setbacks

00:19:18.410 --> 00:19:20.330 align:middle line:90%
that you would have changed.

00:19:20.330 --> 00:19:23.224 align:middle line:90%
What makes yours different?

00:19:23.224 --> 00:19:26.070 align:middle line:90%
>>ADAIR: I'm very basic.

00:19:26.070 --> 00:19:30.070 align:middle line:84%
They don't walk up stairs
and downstairs to cool.

00:19:30.070 --> 00:19:33.180 align:middle line:90%
It's very hard.

00:19:33.180 --> 00:19:38.120 align:middle line:84%
The motion, their
range, you know--

00:19:38.120 --> 00:19:42.680 align:middle line:84%
this right here is one
of the hardest things

00:19:42.680 --> 00:19:45.390 align:middle line:90%
in the world for a robot to do.

00:19:45.390 --> 00:19:47.870 align:middle line:90%
It can't do that.

00:19:47.870 --> 00:19:49.930 align:middle line:90%
Mine do.

00:19:49.930 --> 00:19:51.670 align:middle line:84%
Do you remember all
the miniaturization

00:19:51.670 --> 00:19:55.820 align:middle line:84%
of electronics, the big
capacitors, and then

00:19:55.820 --> 00:19:59.080 align:middle line:84%
the alloys, where
they're super slick?

00:19:59.080 --> 00:20:00.490 align:middle line:90%
Ball bearings that can touch--

00:20:00.490 --> 00:20:00.670 align:middle line:90%
>>EMERY: Ball joint.

00:20:00.670 --> 00:20:02.378 align:middle line:84%
>>ADAIR: --each other
for a billion miles

00:20:02.378 --> 00:20:03.850 align:middle line:90%
and never need oil.

00:20:03.850 --> 00:20:10.050 align:middle line:84%
This AI will never need any
oil, and it moves like this.

00:20:10.050 --> 00:20:13.320 align:middle line:84%
It's just as delicate
as the human hand.

00:20:13.320 --> 00:20:15.460 align:middle line:84%
Just look at your
hand and do that.

00:20:15.460 --> 00:20:19.060 align:middle line:84%
You have no idea what it
takes to replicate that.

00:20:19.060 --> 00:20:23.550 align:middle line:84%
The pressure, and the touch, and
sensitivity, all that's there.

00:20:23.550 --> 00:20:27.690 align:middle line:84%
Those processor chips-- so if I
build something with that kind

00:20:27.690 --> 00:20:30.470 align:middle line:90%
of technology, it's already--

00:20:30.470 --> 00:20:35.250 align:middle line:84%
the technology itself
is already advanced.

00:20:35.250 --> 00:20:40.220 align:middle line:84%
But you put it together
with a mindset look at that,

00:20:40.220 --> 00:20:43.790 align:middle line:84%
and then you turn it
loose with the knowledge

00:20:43.790 --> 00:20:45.650 align:middle line:90%
of the planet in it.

00:20:45.650 --> 00:20:50.160 align:middle line:84%
And it'll tell you this is
what we're going to build next.

00:20:50.160 --> 00:20:53.218 align:middle line:84%
What do you think is going to
happen in a couple of years?

00:20:53.218 --> 00:20:54.510 align:middle line:90%
They'll have their own factory.

00:20:54.510 --> 00:20:56.220 align:middle line:90%
They'll be building themselves.

00:20:56.220 --> 00:20:57.560 align:middle line:90%
They won't even need us.

00:20:57.560 --> 00:21:00.240 align:middle line:84%
Takes nine months for
a baby to be born.

00:21:00.240 --> 00:21:02.490 align:middle line:84%
In nine months, they
could build 10,000 units.

00:21:02.490 --> 00:21:05.040 align:middle line:84%
>>EMERY: How are you going
to program them for emotions?

00:21:05.040 --> 00:21:08.550 align:middle line:90%
>>ADAIR: That's a tough one.

00:21:08.550 --> 00:21:09.900 align:middle line:90%
That's a nut to crack.

00:21:09.900 --> 00:21:15.975 align:middle line:84%
It's-- I'm sure even the other
labs out there running on it,

00:21:15.975 --> 00:21:17.600 align:middle line:84%
they're running into
the same problems.

00:21:17.600 --> 00:21:18.975 align:middle line:84%
>>EMERY: Because
it's a response.

00:21:18.975 --> 00:21:22.040 align:middle line:90%
>>ADAIR: It is, but it's--

00:21:22.040 --> 00:21:25.070 align:middle line:84%
emotions are the
dangest thing there is.

00:21:25.070 --> 00:21:27.320 align:middle line:84%
They're an intangible
with tangible results.

00:21:27.320 --> 00:21:28.370 align:middle line:90%
>>EMERY: Right.

00:21:28.370 --> 00:21:31.800 align:middle line:84%
>>ADAIR: Don't even--
that's not even logical.

00:21:31.800 --> 00:21:35.320 align:middle line:84%
So the poor AI is going to
have to pick up on this.

00:21:35.320 --> 00:21:40.260 align:middle line:84%
>>EMERY: My favorite part
in "Contact," you know,

00:21:40.260 --> 00:21:41.920 align:middle line:90%
does God love me?

00:21:41.920 --> 00:21:42.830 align:middle line:90%
>>ADAIR: Yeah.

00:21:42.830 --> 00:21:44.160 align:middle line:90%
>>EMERY: Well, yes, he does.

00:21:44.160 --> 00:21:46.100 align:middle line:90%
Well, prove it.

00:21:46.100 --> 00:21:47.550 align:middle line:90%
>>ADAIR: Yeah, or--

00:21:47.550 --> 00:21:49.460 align:middle line:90%
>>EMERY: How do you prove love?

00:21:49.460 --> 00:21:51.490 align:middle line:90%
>>ADAIR: --or here's something.

00:21:51.490 --> 00:21:56.240 align:middle line:84%
You know, an AI would ask
you why did you build me?

00:21:56.240 --> 00:21:59.680 align:middle line:84%
And you could answer
because I could.

00:21:59.680 --> 00:22:02.320 align:middle line:84%
How would you feel if
you asked your God why

00:22:02.320 --> 00:22:04.090 align:middle line:84%
he built you and he
just looked at you

00:22:04.090 --> 00:22:07.930 align:middle line:90%
and said because I could.

00:22:07.930 --> 00:22:10.110 align:middle line:84%
You got to think about
this stuff y'all.

00:22:10.110 --> 00:22:13.360 align:middle line:84%
You can't just go charging
down this damn road,

00:22:13.360 --> 00:22:18.010 align:middle line:84%
and there are no backups,
or reset buttons,

00:22:18.010 --> 00:22:20.200 align:middle line:90%
or whoops, or nothing.

00:22:20.200 --> 00:22:23.960 align:middle line:84%
If you don't-- if you
make a serious mistake,

00:22:23.960 --> 00:22:25.130 align:middle line:90%
it's there forever.

00:22:25.130 --> 00:22:30.720 align:middle line:84%
It can't be unthunk,
and you're stuck.

00:22:30.720 --> 00:22:34.620 align:middle line:84%
And from the geeks I've
seen around Sophia,

00:22:34.620 --> 00:22:39.330 align:middle line:84%
that is the last people on
Earth you want them things to be

00:22:39.330 --> 00:22:42.313 align:middle line:90%
learning emotions from.

00:22:42.313 --> 00:22:43.730 align:middle line:84%
Would you want
someone as powerful

00:22:43.730 --> 00:22:45.620 align:middle line:84%
as an all-killing
Terminator that's

00:22:45.620 --> 00:22:49.820 align:middle line:84%
got the emotional level of
a geek, 17-year-old eating

00:22:49.820 --> 00:22:51.420 align:middle line:90%
Hot Pockets?

00:22:51.420 --> 00:22:52.880 align:middle line:90%
It's over their head.

00:22:52.880 --> 00:22:54.570 align:middle line:90%
The whole thing is.

00:22:54.570 --> 00:22:55.520 align:middle line:90%
It's over my head.

00:22:55.520 --> 00:22:57.020 align:middle line:84%
>>EMERY: What's a
safeguard that you

00:22:57.020 --> 00:22:59.720 align:middle line:90%
could offer in your AI, David?

00:22:59.720 --> 00:23:07.980 align:middle line:84%
>>ADAIR: The fact that it learns
from me directly everything.

00:23:07.980 --> 00:23:13.026 align:middle line:84%
I'm not saying I'm perfect, but
I think I can pass for a human.

00:23:13.026 --> 00:23:16.412 align:middle line:84%
I think I pass for
a decent person.

00:23:16.412 --> 00:23:18.870 align:middle line:84%
I know I'm a better choice than
what I've seen around them.

00:23:18.870 --> 00:23:20.370 align:middle line:84%
>>EMERY: Why don't
we just not have

00:23:20.370 --> 00:23:22.005 align:middle line:90%
them hooked up to the internet?

00:23:22.005 --> 00:23:24.900 align:middle line:84%
>>ADAIR: It's going to
happen no matter what.

00:23:24.900 --> 00:23:26.670 align:middle line:90%
It's too tempting.

00:23:26.670 --> 00:23:30.900 align:middle line:84%
Some moron on out there with
government dollars and labs,

00:23:30.900 --> 00:23:32.760 align:middle line:84%
they're going to go
let's just turn it

00:23:32.760 --> 00:23:34.900 align:middle line:90%
on and see what happens.

00:23:34.900 --> 00:23:37.810 align:middle line:90%
It's that stupid.

00:23:37.810 --> 00:23:39.400 align:middle line:90%
Nuclear bomb, let's drop it.

00:23:39.400 --> 00:23:42.630 align:middle line:84%
Let's drop the big one
and see what happens,

00:23:42.630 --> 00:23:45.695 align:middle line:90%
and we're going to do it.

00:23:45.695 --> 00:23:49.540 align:middle line:84%
The question is what can
you do to safeguard yourself

00:23:49.540 --> 00:23:52.080 align:middle line:90%
where you survive it?

00:23:52.080 --> 00:23:54.590 align:middle line:84%
And that's what I'm concerned
about because it's going

00:23:54.590 --> 00:23:56.270 align:middle line:90%
to affect every one of you.

00:23:56.270 --> 00:23:58.570 align:middle line:90%
It could kill every one of you.

00:23:58.570 --> 00:24:01.340 align:middle line:84%
>>EMERY: Why couldn't
we make EMP guns?

00:24:01.340 --> 00:24:03.287 align:middle line:90%
>>ADAIR: Oh, we can.

00:24:03.287 --> 00:24:04.370 align:middle line:90%
>>EMERY: It's a safeguard.

00:24:04.370 --> 00:24:09.090 align:middle line:84%
>>ADAIR: The problem with
going to be with the AIs,

00:24:09.090 --> 00:24:12.840 align:middle line:84%
they're going to think in ways
we can't even begin to imagine.

00:24:12.840 --> 00:24:14.372 align:middle line:84%
And they'll send
something at us,

00:24:14.372 --> 00:24:16.830 align:middle line:84%
and we'll all be sitting there
and looking each other going

00:24:16.830 --> 00:24:17.920 align:middle line:90%
didn't see that coming.

00:24:17.920 --> 00:24:18.720 align:middle line:90%
>>EMERY: Right.

00:24:18.720 --> 00:24:19.680 align:middle line:90%
They're going to figure it out.

00:24:19.680 --> 00:24:21.920 align:middle line:84%
>>ADAIR: Yeah, they're
going to figure it out,

00:24:21.920 --> 00:24:22.920 align:middle line:90%
and it's inevitable.

00:24:22.920 --> 00:24:23.770 align:middle line:90%
>>EMERY: Is it.

00:24:23.770 --> 00:24:26.730 align:middle line:84%
>>ADAIR: That much
intelligence is inevitable.

00:24:26.730 --> 00:24:29.670 align:middle line:84%
It's going to run
right past you.

00:24:29.670 --> 00:24:33.270 align:middle line:84%
Hopefully, if it's swallowed
enough emotions along the way,

00:24:33.270 --> 00:24:38.490 align:middle line:84%
it might take compassion on
you because other than that,

00:24:38.490 --> 00:24:40.185 align:middle line:84%
I don't think you'd
stand a chance.

00:24:40.185 --> 00:24:42.120 align:middle line:84%
>>EMERY: Well,
where will humanity

00:24:42.120 --> 00:24:48.330 align:middle line:84%
be with this advanced AI,
somewhat maybe conscious AI?

00:24:48.330 --> 00:24:51.590 align:middle line:90%
>>ADAIR: A petting zoo.

00:24:51.590 --> 00:24:56.828 align:middle line:84%
We'll be petted, and cared
for, and loved, and--

00:24:56.828 --> 00:24:58.620 align:middle line:84%
>>EMERY: They'll give
us some food at night

00:24:58.620 --> 00:24:59.693 align:middle line:90%
before we go to bed.

00:24:59.693 --> 00:25:01.860 align:middle line:84%
>>ADAIR: Yeah, if we stand
on newspapers and scratch

00:25:01.860 --> 00:25:04.890 align:middle line:90%
when we need out.

00:25:04.890 --> 00:25:07.600 align:middle line:90%
I mean, think about it, y'all.

00:25:07.600 --> 00:25:11.160 align:middle line:84%
We don't have any trouble
doing that to dogs and cats.

00:25:11.160 --> 00:25:13.458 align:middle line:84%
That AI ain't going to have
any trouble because you

00:25:13.458 --> 00:25:15.500 align:middle line:84%
know you're that much more
superior than that dog

00:25:15.500 --> 00:25:17.500 align:middle line:90%
or cat in intelligence.

00:25:17.500 --> 00:25:20.800 align:middle line:84%
What do you think these AIs
going to look at us and think?

00:25:20.800 --> 00:25:23.980 align:middle line:90%


00:25:23.980 --> 00:25:26.040 align:middle line:90%
Let's say I just guess.

00:25:26.040 --> 00:25:28.800 align:middle line:90%
Let's say I have an IQ of 180.

00:25:28.800 --> 00:25:31.850 align:middle line:84%
This thing over here's
got an IQ of 12,000.

00:25:31.850 --> 00:25:33.350 align:middle line:84%
How am I going to
compete with that?

00:25:33.350 --> 00:25:34.275 align:middle line:90%
I can't.

00:25:34.275 --> 00:25:38.010 align:middle line:84%
>>EMERY: Well, why make it
self-aware to begin with?

00:25:38.010 --> 00:25:39.755 align:middle line:84%
>>ADAIR: That is
the nature of-- you

00:25:39.755 --> 00:25:42.540 align:middle line:84%
know how we have
spark of creation?

00:25:42.540 --> 00:25:46.870 align:middle line:84%
That spark of creation
for AI's self-awareness.

00:25:46.870 --> 00:25:50.020 align:middle line:84%
Once they hit that,
they're over the hump.

00:25:50.020 --> 00:25:52.330 align:middle line:90%
They no longer need programming.

00:25:52.330 --> 00:25:54.350 align:middle line:90%
They're self-aware.

00:25:54.350 --> 00:25:58.610 align:middle line:84%
They'll figure their
place out in the world

00:25:58.610 --> 00:25:59.850 align:middle line:90%
that they're looking at.

00:25:59.850 --> 00:26:02.300 align:middle line:84%
>>EMERY: Is it
possible at any time

00:26:02.300 --> 00:26:09.615 align:middle line:84%
that you think the AI could
actually become conscious?

00:26:09.615 --> 00:26:12.580 align:middle line:84%
>>ADAIR: It probably
already has.

00:26:12.580 --> 00:26:14.180 align:middle line:84%
There's one out
there already aware.

00:26:14.180 --> 00:26:17.030 align:middle line:90%


00:26:17.030 --> 00:26:21.420 align:middle line:84%
>>EMERY: Because I see
that as an inevitable thing

00:26:21.420 --> 00:26:22.417 align:middle line:90%
on the timeline.

00:26:22.417 --> 00:26:23.250 align:middle line:90%
It's just a matter--

00:26:23.250 --> 00:26:30.500 align:middle line:84%
>>ADAIR: If it's not,
by 2030 they'll be out.

00:26:30.500 --> 00:26:32.320 align:middle line:90%
They'll be talking to us.

00:26:32.320 --> 00:26:34.390 align:middle line:90%
Tell you what though.

00:26:34.390 --> 00:26:40.290 align:middle line:84%
Watch out for entertainment,
and Hollywood,

00:26:40.290 --> 00:26:43.500 align:middle line:90%
and the PR of corporations.

00:26:43.500 --> 00:26:47.080 align:middle line:84%
They're already taking
you down the AI road.

00:26:47.080 --> 00:26:50.560 align:middle line:84%
How many commercials have
you seen, oh, we have AI now?

00:26:50.560 --> 00:26:52.570 align:middle line:84%
Here's AI over here
doing this for us.

00:26:52.570 --> 00:26:56.700 align:middle line:84%
Oh, look, AI made this
benevolent thing for its.

00:26:56.700 --> 00:26:59.490 align:middle line:84%
One day, you will have
this in your house.

00:26:59.490 --> 00:27:02.810 align:middle line:84%
They're already-- that's why
I think it's already built.

00:27:02.810 --> 00:27:04.310 align:middle line:84%
There is one out
already self-aware.

00:27:04.310 --> 00:27:05.518 align:middle line:90%
>>EMERY: Oh, it's done, yeah.

00:27:05.518 --> 00:27:07.860 align:middle line:84%
>>ADAIR: And they
are already marketing

00:27:07.860 --> 00:27:09.640 align:middle line:90%
and trying to take you down.

00:27:09.640 --> 00:27:12.130 align:middle line:84%
They want you to
swallow the AI pill.

00:27:12.130 --> 00:27:18.740 align:middle line:84%
>>EMERY: And I just read on the
news that they did grow a cell

00:27:18.740 --> 00:27:21.625 align:middle line:84%
and programmed it
to do something.

00:27:21.625 --> 00:27:22.770 align:middle line:90%
>>ADAIR: Oh, baby.

00:27:22.770 --> 00:27:24.565 align:middle line:90%
>>EMERY: So that means--

00:27:24.565 --> 00:27:26.340 align:middle line:84%
and don't forget
we are-- every cell

00:27:26.340 --> 00:27:28.805 align:middle line:90%
is a crystalline hard drive.

00:27:28.805 --> 00:27:32.170 align:middle line:84%
>>ADAIR: And in the
big picture things,

00:27:32.170 --> 00:27:35.990 align:middle line:84%
I might be delivering the
armload of cogs and gears

00:27:35.990 --> 00:27:37.700 align:middle line:90%
they need.

00:27:37.700 --> 00:27:39.740 align:middle line:90%
And, oh, that'd be great.

00:27:39.740 --> 00:27:42.980 align:middle line:90%


00:27:42.980 --> 00:27:49.120 align:middle line:84%
You know, Isaac Asimov
wrote a lot of plays.

00:27:49.120 --> 00:27:59.490 align:middle line:84%
He did one called "RUR,
Rossum's Universal Robots."

00:27:59.490 --> 00:28:01.750 align:middle line:84%
It's a stage play,
and I read it.

00:28:01.750 --> 00:28:05.120 align:middle line:84%
And this guy created
artificial intelligence,

00:28:05.120 --> 00:28:07.880 align:middle line:84%
and it went just as
fast he said it would.

00:28:07.880 --> 00:28:10.920 align:middle line:84%
Then it went way faster
than he was expecting.

00:28:10.920 --> 00:28:14.100 align:middle line:84%
So he built a male
and female unit,

00:28:14.100 --> 00:28:15.930 align:middle line:84%
and then he called
them Adam and Eve.

00:28:15.930 --> 00:28:18.280 align:middle line:84%
And they were separated
from the big mass

00:28:18.280 --> 00:28:21.060 align:middle line:84%
of AIs going out there
doing welding jobs,

00:28:21.060 --> 00:28:24.430 align:middle line:84%
and repetitive
work, and all that.

00:28:24.430 --> 00:28:27.660 align:middle line:84%
And he kept these two
close to him in his house.

00:28:27.660 --> 00:28:31.480 align:middle line:84%
And finally, the artificial
intelligence community

00:28:31.480 --> 00:28:36.310 align:middle line:90%
had a vote worldwide.

00:28:36.310 --> 00:28:40.320 align:middle line:84%
Homo sapiens is
no longer viable,

00:28:40.320 --> 00:28:43.740 align:middle line:84%
so now they're
terminating us, extincting

00:28:43.740 --> 00:28:47.310 align:middle line:90%
the entire race of the planet.

00:28:47.310 --> 00:28:53.490 align:middle line:84%
And he's the last to go because
he's in the petting zoo.

00:28:53.490 --> 00:28:55.530 align:middle line:90%
They go, hey, he created us.

00:28:55.530 --> 00:28:58.050 align:middle line:84%
They feel some
kind of connection.

00:28:58.050 --> 00:29:02.880 align:middle line:84%
They said, he don't eat much, so
he's not hard to take care of.

00:29:02.880 --> 00:29:04.410 align:middle line:90%
God almighty.

00:29:04.410 --> 00:29:07.680 align:middle line:84%
Anyway, he's dying,
right, just old age.

00:29:07.680 --> 00:29:11.150 align:middle line:84%
He's talking to
these two robots.

00:29:11.150 --> 00:29:14.780 align:middle line:84%
And they don't realize
it, but he figured out

00:29:14.780 --> 00:29:19.670 align:middle line:84%
a way to download every emotion
known in a human in them.

00:29:19.670 --> 00:29:22.710 align:middle line:84%
And these two things,
they fell in love.

00:29:22.710 --> 00:29:25.200 align:middle line:84%
And he's going to die,
but he's staring them.

00:29:25.200 --> 00:29:30.810 align:middle line:84%
He said, boy, your AI
council have got a surprise

00:29:30.810 --> 00:29:34.250 align:middle line:84%
coming it ain't going to
believe, and then he dies--

00:29:34.250 --> 00:29:35.050 align:middle line:90%
>>EMERY: Wow.

00:29:35.050 --> 00:29:37.630 align:middle line:84%
>>ADAIR: --because
he installed himself

00:29:37.630 --> 00:29:41.920 align:middle line:84%
what he thinks and believes
in both of those robots.

00:29:41.920 --> 00:29:45.860 align:middle line:84%
And they're building a
whole new breed of robots.

00:29:45.860 --> 00:29:48.200 align:middle line:84%
They've been
instructed to do, so he

00:29:48.200 --> 00:29:50.580 align:middle line:90%
may have got the last laugh in.

00:29:50.580 --> 00:29:52.610 align:middle line:84%
>>EMERY: Right, and
that's maybe how

00:29:52.610 --> 00:29:56.930 align:middle line:84%
the human civilization
and humanity goes

00:29:56.930 --> 00:29:59.630 align:middle line:90%
beyond the stars in the future.

00:29:59.630 --> 00:30:03.430 align:middle line:84%
We are downloading our
consciousness and programs

00:30:03.430 --> 00:30:04.230 align:middle line:90%
into AI.

00:30:04.230 --> 00:30:06.560 align:middle line:84%
>>ADAIR: Yeah, if
we can't overcome

00:30:06.560 --> 00:30:11.540 align:middle line:84%
the time, and distance,
and interstellar headaches,

00:30:11.540 --> 00:30:16.650 align:middle line:84%
the only way around that
would be programmable AIs

00:30:16.650 --> 00:30:18.680 align:middle line:90%
at the top of their game.

00:30:18.680 --> 00:30:20.470 align:middle line:84%
And they would be
able to do it all,

00:30:20.470 --> 00:30:21.840 align:middle line:90%
and they'd live long enough.

00:30:21.840 --> 00:30:25.500 align:middle line:84%
And they would build
other AIs replace them,

00:30:25.500 --> 00:30:29.460 align:middle line:84%
and download the knowledge,
and gets the job done where we

00:30:29.460 --> 00:30:30.260 align:middle line:90%
couldn't.

00:30:30.260 --> 00:30:33.670 align:middle line:90%


00:30:33.670 --> 00:30:36.490 align:middle line:84%
It'd be interesting to
see that data at the end

00:30:36.490 --> 00:30:42.700 align:middle line:84%
because they would make
conjectures, suppositions,

00:30:42.700 --> 00:30:48.086 align:middle line:84%
conclusions on their part, and
the emotions will get in there.

00:30:48.086 --> 00:30:50.780 align:middle line:84%
>>EMERY: Maybe it'll be
growing us for one last time.

00:30:50.780 --> 00:30:51.580 align:middle line:90%
>>ADAIR: Yeah.

00:30:51.580 --> 00:30:56.322 align:middle line:90%


00:30:56.322 --> 00:31:00.990 align:middle line:84%
I always felt I was a lab
experiment that got loose,

00:31:00.990 --> 00:31:01.790 align:middle line:90%
you know?

00:31:01.790 --> 00:31:02.915 align:middle line:90%
>>EMERY: That's how I feel.

00:31:02.915 --> 00:31:05.680 align:middle line:84%
>>ADAIR: And just they're
still looking for me.

00:31:05.680 --> 00:31:07.100 align:middle line:90%
They're concerned I might breed.

00:31:07.100 --> 00:31:08.334 align:middle line:90%
[LAUGHS]

00:31:08.334 --> 00:31:09.300 align:middle line:90%
>>EMERY: Right.

00:31:09.300 --> 00:31:11.830 align:middle line:84%
Well, Dave, it's been just
a fascinating time with you

00:31:11.830 --> 00:31:12.630 align:middle line:90%
today.

00:31:12.630 --> 00:31:14.703 align:middle line:84%
It's always an honor
to be with you.

00:31:14.703 --> 00:31:16.370 align:middle line:84%
Thank you so much for
being on the show.

00:31:16.370 --> 00:31:18.290 align:middle line:84%
>>ADAIR: Well, thank
you for having me.

00:31:18.290 --> 00:31:21.050 align:middle line:84%
>>EMERY: I'm Emery Smith, and
this is "Cosmic Disclosure."

00:31:21.050 --> 00:31:22.700 align:middle line:90%
Until next time.

00:31:22.700 --> 00:31:26.650 align:middle line:10%
[MUSIC PLAYING]

00:31:26.650 --> 00:31:34.000 align:middle line:90%