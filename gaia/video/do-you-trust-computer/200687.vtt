WEBVTT

00:00:00.000 --> 00:00:16.830 align:middle line:90%


00:00:16.830 --> 00:00:20.295 align:middle line:90%
[SPACE AND ELECTRONIC SOUNDS]

00:00:20.295 --> 00:00:35.145 align:middle line:90%


00:00:35.145 --> 00:00:39.800 align:middle line:84%
>> What we're on the brink
of is a world of increasingly

00:00:39.800 --> 00:00:45.410 align:middle line:84%
intense, sophisticated
artificial intelligence.

00:00:45.410 --> 00:00:48.500 align:middle line:84%
>> Technology is evolving so
much faster than our society

00:00:48.500 --> 00:00:51.660 align:middle line:84%
has the ability to
protect us as citizens.

00:00:51.660 --> 00:00:54.320 align:middle line:10%
>> The robots are coming,
and they will destroy

00:00:54.320 --> 00:00:56.236 align:middle line:10%
our livelihoods.

00:00:56.236 --> 00:00:59.074 align:middle line:90%
[MUSIC PLAYING]

00:00:59.074 --> 00:01:01.920 align:middle line:90%


00:01:01.920 --> 00:01:04.530 align:middle line:84%
>> We have a networked
intelligence that watches us,

00:01:04.530 --> 00:01:08.580 align:middle line:84%
knows everything about us, and
begins to try to change us.

00:01:08.580 --> 00:01:12.660 align:middle line:84%
>> Twitter has become the
world's number one news site.

00:01:12.660 --> 00:01:15.280 align:middle line:84%
>> Technology is
never good or bad.

00:01:15.280 --> 00:01:19.410 align:middle line:84%
It's what we do
with the technology.

00:01:19.410 --> 00:01:21.840 align:middle line:84%
>> Eventually, millions of
people are going to be thrown

00:01:21.840 --> 00:01:25.304 align:middle line:84%
out of jobs because their skills
are going to be obsoleted.

00:01:25.304 --> 00:01:29.190 align:middle line:10%
>> Mass unemployment,
greater inequalities,

00:01:29.190 --> 00:01:32.540 align:middle line:10%
even social unrest.

00:01:32.540 --> 00:01:35.510 align:middle line:84%
>> Regardless of whether
to be afraid or not afraid,

00:01:35.510 --> 00:01:37.950 align:middle line:84%
the change is coming,
and nobody can stop it.

00:01:37.950 --> 00:01:44.600 align:middle line:90%


00:01:44.600 --> 00:01:46.310 align:middle line:84%
>> We've invested
huge amounts of money,

00:01:46.310 --> 00:01:49.220 align:middle line:84%
and so it stands to
reason that the military,

00:01:49.220 --> 00:01:52.070 align:middle line:84%
with their own desires, are
going to start to use these

00:01:52.070 --> 00:01:53.420 align:middle line:90%
technologies.

00:01:53.420 --> 00:01:57.020 align:middle line:84%
>> Autonomous weapons systems
could lead to a global arms

00:01:57.020 --> 00:01:59.630 align:middle line:90%
race to rival the nuclear era.

00:01:59.630 --> 00:02:02.270 align:middle line:90%


00:02:02.270 --> 00:02:03.960 align:middle line:90%
>> We know what the answer is.

00:02:03.960 --> 00:02:05.979 align:middle line:84%
They'll eventually
be killing us.

00:02:05.979 --> 00:02:07.815 align:middle line:90%
[TANK FIRES]

00:02:07.815 --> 00:02:11.030 align:middle line:90%


00:02:11.030 --> 00:02:13.940 align:middle line:84%
>> These technology leaps
are going to yield incredible

00:02:13.940 --> 00:02:17.145 align:middle line:90%
miracles and incredible horrors.

00:02:17.145 --> 00:02:24.440 align:middle line:90%


00:02:24.440 --> 00:02:29.480 align:middle line:84%
>> We created it, so I think
as we move forward this

00:02:29.480 --> 00:02:34.230 align:middle line:84%
intelligence will
contain parts of us.

00:02:34.230 --> 00:02:36.130 align:middle line:84%
And I think the
question is will it

00:02:36.130 --> 00:02:40.430 align:middle line:84%
contain the good parts
or the bad parts?

00:02:40.430 --> 00:03:03.695 align:middle line:90%


00:03:03.695 --> 00:03:04.820 align:middle line:90%
[ELECTRONIC WEAPONS FIRING]

00:03:04.820 --> 00:03:08.790 align:middle line:10%
>> The survivors called
the war Judgment Day.

00:03:08.790 --> 00:03:12.430 align:middle line:84%
They lived only to
face a new nightmare--

00:03:12.430 --> 00:03:14.610 align:middle line:90%
the war against the machines.

00:03:14.610 --> 00:03:15.410 align:middle line:90%
[SCREAMS]

00:03:15.410 --> 00:03:18.290 align:middle line:84%
>> I think we've
completely fucked this up.

00:03:18.290 --> 00:03:21.530 align:middle line:10%
I think Hollywood has managed
to inoculate the general public

00:03:21.530 --> 00:03:25.250 align:middle line:10%
against this question,
the idea of machines

00:03:25.250 --> 00:03:28.670 align:middle line:10%
that will take over the world.

00:03:28.670 --> 00:03:32.030 align:middle line:10%
>> Open the pod bay doors, Hal.

00:03:32.030 --> 00:03:33.740 align:middle line:10%
>> I'm sorry, Dave.

00:03:33.740 --> 00:03:35.270 align:middle line:90%
I'm afraid I can't do that.

00:03:35.270 --> 00:03:38.070 align:middle line:90%


00:03:38.070 --> 00:03:38.930 align:middle line:90%
>> Hal?

00:03:38.930 --> 00:03:40.440 align:middle line:84%
>> We've cried
wolf enough times.

00:03:40.440 --> 00:03:40.760 align:middle line:90%
>> Hal?

00:03:40.760 --> 00:03:43.430 align:middle line:84%
>> The public has stopped paying
attention because it feels like

00:03:43.430 --> 00:03:43.910 align:middle line:90%
science fiction.

00:03:43.910 --> 00:03:45.785 align:middle line:84%
Even sitting here talking
about it right now,

00:03:45.785 --> 00:03:48.440 align:middle line:84%
it feels a little bit silly,
a little bit like, oh, this

00:03:48.440 --> 00:03:51.680 align:middle line:84%
is an artifact of
some cheeseball movie.

00:03:51.680 --> 00:03:56.750 align:middle line:10%
>> The WOPR spends all its time
thinking about World War 3.

00:03:56.750 --> 00:03:59.030 align:middle line:90%
>> But it's not.

00:03:59.030 --> 00:04:01.580 align:middle line:84%
The general public is about
to get blindsided by this.

00:04:01.580 --> 00:04:11.500 align:middle line:90%


00:04:11.500 --> 00:04:13.540 align:middle line:10%
>> As a society,
and as individuals,

00:04:13.540 --> 00:04:19.079 align:middle line:10%
we're increasingly surrounded
by machine intelligence.

00:04:19.079 --> 00:04:22.630 align:middle line:84%
We carry this pocket device
in the palm of our hand

00:04:22.630 --> 00:04:25.960 align:middle line:84%
that we use to make a striking
array of life decisions right

00:04:25.960 --> 00:04:28.815 align:middle line:84%
now, aided by a set
of distant algorithms

00:04:28.815 --> 00:04:30.190 align:middle line:84%
that we have no
understanding of.

00:04:30.190 --> 00:04:34.174 align:middle line:90%


00:04:34.174 --> 00:04:37.170 align:middle line:10%
>> We're already pretty jaded
about the idea that we can talk

00:04:37.170 --> 00:04:40.040 align:middle line:10%
through our phone and that
it mostly understands us.

00:04:40.040 --> 00:04:42.230 align:middle line:84%
>> I found quite a
number of action films.

00:04:42.230 --> 00:04:44.940 align:middle line:90%
>> Five years ago, no way.

00:04:44.940 --> 00:04:48.810 align:middle line:84%
>> Robotics, machines that
see, and speak, and listen.

00:04:48.810 --> 00:04:50.250 align:middle line:90%
All that's real now.

00:04:50.250 --> 00:04:53.280 align:middle line:84%
And these technologies are
going to fundamentally change

00:04:53.280 --> 00:04:56.140 align:middle line:90%
our society.

00:04:56.140 --> 00:05:00.160 align:middle line:84%
>> Now we have this great
movement of self-driving cars.

00:05:00.160 --> 00:05:03.310 align:middle line:10%
Driving a car autonomously
can move people's lives

00:05:03.310 --> 00:05:06.070 align:middle line:10%
into a better place.

00:05:06.070 --> 00:05:09.460 align:middle line:10%
>> I've lost a number of family
members, including my mother,

00:05:09.460 --> 00:05:12.640 align:middle line:10%
my brother and sister-in-law,
and their kids to automobile

00:05:12.640 --> 00:05:13.990 align:middle line:10%
accidents.

00:05:13.990 --> 00:05:17.170 align:middle line:84%
It's pretty clear we
can almost eliminate

00:05:17.170 --> 00:05:20.040 align:middle line:90%
car accidents with automation.

00:05:20.040 --> 00:05:22.300 align:middle line:84%
30,000 lives in the US
alone, about a million

00:05:22.300 --> 00:05:25.560 align:middle line:90%
around the world per year.

00:05:25.560 --> 00:05:28.200 align:middle line:10%
>> In health care, early
indicators are the name

00:05:28.200 --> 00:05:30.050 align:middle line:10%
of the game in that space.

00:05:30.050 --> 00:05:33.750 align:middle line:84%
So that's another place where
it can save somebody's life.

00:05:33.750 --> 00:05:35.760 align:middle line:10%
>> Here in the
breast cancer center,

00:05:35.760 --> 00:05:40.200 align:middle line:10%
all the things that the
radiologist's brain does in two

00:05:40.200 --> 00:05:43.680 align:middle line:10%
minutes, a computer
does instantaneously.

00:05:43.680 --> 00:05:47.250 align:middle line:84%
The computer has looked
at one billion mammograms.

00:05:47.250 --> 00:05:49.170 align:middle line:84%
And it takes that
data and applies it

00:05:49.170 --> 00:05:51.450 align:middle line:90%
to this image instantaneously.

00:05:51.450 --> 00:05:53.595 align:middle line:84%
So the medical
application is profound.

00:05:53.595 --> 00:05:56.328 align:middle line:90%


00:05:56.328 --> 00:05:58.620 align:middle line:10%
>> Another really exciting
area that we're seeing a lot

00:05:58.620 --> 00:06:03.360 align:middle line:10%
of development in is actually
understanding our genetic code,

00:06:03.360 --> 00:06:06.930 align:middle line:10%
and using that to both diagnose
disease and create personalized

00:06:06.930 --> 00:06:07.908 align:middle line:10%
treatments.

00:06:07.908 --> 00:06:11.900 align:middle line:90%


00:06:11.900 --> 00:06:14.390 align:middle line:10%
>> The primary application
of all these machines will be

00:06:14.390 --> 00:06:17.110 align:middle line:10%
to extend our own intelligence.

00:06:17.110 --> 00:06:19.400 align:middle line:84%
We'll be able to make
ourselves smarter,

00:06:19.400 --> 00:06:22.670 align:middle line:84%
and we'll be better
at solving problems.

00:06:22.670 --> 00:06:23.580 align:middle line:90%
We don't have to age.

00:06:23.580 --> 00:06:27.110 align:middle line:84%
We'll actually understand aging,
and we'll be able to stop it.

00:06:27.110 --> 00:06:29.600 align:middle line:84%
>> There's really no limit to
what intelligent machines can

00:06:29.600 --> 00:06:30.517 align:middle line:90%
do for the human race.

00:06:30.517 --> 00:06:36.220 align:middle line:90%


00:06:36.220 --> 00:06:38.770 align:middle line:84%
>> How could a smarter machine
not be a better machine?

00:06:38.770 --> 00:06:41.990 align:middle line:90%


00:06:41.990 --> 00:06:44.690 align:middle line:84%
It's hard to say exactly
when I began to think

00:06:44.690 --> 00:06:47.629 align:middle line:90%
that that was a bit naive.

00:06:47.629 --> 00:06:51.080 align:middle line:10%
[MUSIC PLAYING]

00:06:51.080 --> 00:06:56.503 align:middle line:90%


00:06:56.503 --> 00:06:59.775 align:middle line:10%
>> Stuart Russell-- he's
basically a god in the field

00:06:59.775 --> 00:07:00.900 align:middle line:10%
of artificial intelligence.

00:07:00.900 --> 00:07:04.350 align:middle line:10%
He wrote the book that
almost every university uses.

00:07:04.350 --> 00:07:06.930 align:middle line:10%
>> I used to say it's the
best-selling AI textbook.

00:07:06.930 --> 00:07:09.720 align:middle line:10%
Now I just say it's the PDF
that's stolen most often.

00:07:09.720 --> 00:07:13.650 align:middle line:90%


00:07:13.650 --> 00:07:17.250 align:middle line:84%
Artificial intelligence is
about making computers smart.

00:07:17.250 --> 00:07:19.950 align:middle line:10%
And from the point of
view of the public, what

00:07:19.950 --> 00:07:22.680 align:middle line:10%
counts as AI is just
something that's surprisingly

00:07:22.680 --> 00:07:24.960 align:middle line:10%
intelligent compared
to what we thought

00:07:24.960 --> 00:07:28.380 align:middle line:10%
computers would
typically be able to do.

00:07:28.380 --> 00:07:33.930 align:middle line:10%
>> AI is a field of research to
try to basically simulate all

00:07:33.930 --> 00:07:36.760 align:middle line:10%
kinds of human capabilities.

00:07:36.760 --> 00:07:38.740 align:middle line:90%
We're in an AI era.

00:07:38.740 --> 00:07:40.450 align:middle line:84%
Silicon Valley has
the ability to focus

00:07:40.450 --> 00:07:42.400 align:middle line:90%
on one bright, shiny thing.

00:07:42.400 --> 00:07:44.448 align:middle line:84%
It was social networking
and social media

00:07:44.448 --> 00:07:46.240 align:middle line:84%
over the last decade,
and it's pretty clear

00:07:46.240 --> 00:07:48.010 align:middle line:90%
that the bit has flipped.

00:07:48.010 --> 00:07:50.800 align:middle line:10%
>> And it starts with
machine learning.

00:07:50.800 --> 00:07:54.310 align:middle line:84%
>> When we look back at this
moment, what was the first AI?

00:07:54.310 --> 00:07:57.190 align:middle line:84%
It's not sexy, and it isn't the
thing we consume in the movies.

00:07:57.190 --> 00:08:01.210 align:middle line:10%
But you'd make a great case
that Google created not a search

00:08:01.210 --> 00:08:03.340 align:middle line:10%
engine, but a godhead--

00:08:03.340 --> 00:08:06.460 align:middle line:10%
a way for people to ask
any question they wanted

00:08:06.460 --> 00:08:08.260 align:middle line:10%
and get the answer they needed.

00:08:08.260 --> 00:08:11.350 align:middle line:84%
>> Most people are not aware
that what Google is doing is

00:08:11.350 --> 00:08:13.720 align:middle line:84%
actually a form of
artificial intelligence.

00:08:13.720 --> 00:08:16.210 align:middle line:84%
They just go there,
they type in a thing,

00:08:16.210 --> 00:08:18.250 align:middle line:90%
Google gives them the answer.

00:08:18.250 --> 00:08:21.433 align:middle line:84%
>> With each search, we
train it to be better.

00:08:21.433 --> 00:08:23.850 align:middle line:10%
Sometimes we type in the search
and it tells us the answer

00:08:23.850 --> 00:08:27.820 align:middle line:10%
before you've finished
asking the question.

00:08:27.820 --> 00:08:30.050 align:middle line:84%
Who is the president
of Kazakhstan?

00:08:30.050 --> 00:08:31.288 align:middle line:90%
And it'll just tell you.

00:08:31.288 --> 00:08:33.580 align:middle line:84%
You don't have to go to the
Kazakhstan national website

00:08:33.580 --> 00:08:34.900 align:middle line:90%
to find out.

00:08:34.900 --> 00:08:37.000 align:middle line:84%
Didn't used to be
able to do that.

00:08:37.000 --> 00:08:39.270 align:middle line:84%
>> That is artificial
intelligence.

00:08:39.270 --> 00:08:41.919 align:middle line:84%
Year from now, when
we try to understand,

00:08:41.919 --> 00:08:44.590 align:middle line:90%
we will say, how did we miss it?

00:08:44.590 --> 00:08:47.560 align:middle line:84%
>> It's one of these
striking contradictions that

00:08:47.560 --> 00:08:48.400 align:middle line:90%
we're facing.

00:08:48.400 --> 00:08:49.990 align:middle line:90%
Google and Facebook et al.

00:08:49.990 --> 00:08:54.100 align:middle line:84%
Have built businesses on giving
us, as a society, free stuff.

00:08:54.100 --> 00:08:56.000 align:middle line:90%
But it's a Faustian bargain.

00:08:56.000 --> 00:09:00.290 align:middle line:84%
They're extracting something
from us in exchange.

00:09:00.290 --> 00:09:02.920 align:middle line:84%
But we don't know what code
is running on the other side

00:09:02.920 --> 00:09:03.720 align:middle line:90%
and why.

00:09:03.720 --> 00:09:06.570 align:middle line:90%
We have no idea.

00:09:06.570 --> 00:09:08.913 align:middle line:84%
It does strike right at
the issue of how much we

00:09:08.913 --> 00:09:10.080 align:middle line:90%
should trust these machines.

00:09:10.080 --> 00:09:14.110 align:middle line:90%


00:09:14.110 --> 00:09:17.080 align:middle line:84%
>> I use computers
literally for everything.

00:09:17.080 --> 00:09:18.200 align:middle line:90%
[LAUGHS]

00:09:18.200 --> 00:09:21.430 align:middle line:84%
>> There's so many
computer advancements now,

00:09:21.430 --> 00:09:23.830 align:middle line:84%
and it's become such a
big part of our lives.

00:09:23.830 --> 00:09:26.120 align:middle line:84%
>> It's incredible
what a computer can do.

00:09:26.120 --> 00:09:29.080 align:middle line:84%
You can actually carry your
computer in your purse!

00:09:29.080 --> 00:09:30.700 align:middle line:90%
I mean, how awesome is that?

00:09:30.700 --> 00:09:31.750 align:middle line:90%
[LAUGHS]

00:09:31.750 --> 00:09:35.020 align:middle line:84%
>> I think most technology is
meant to make things easier

00:09:35.020 --> 00:09:37.240 align:middle line:90%
and simpler for all of us.

00:09:37.240 --> 00:09:40.740 align:middle line:84%
So hopefully that just
remains the focus.

00:09:40.740 --> 00:09:43.030 align:middle line:84%
>> I think everybody
loves their computers.

00:09:43.030 --> 00:09:51.640 align:middle line:90%
[LAUGHS]

00:09:51.640 --> 00:09:55.180 align:middle line:10%
>> People don't realize they
are constantly being negotiated

00:09:55.180 --> 00:09:56.740 align:middle line:10%
with by machines.

00:09:56.740 --> 00:09:59.450 align:middle line:90%


00:09:59.450 --> 00:10:03.010 align:middle line:10%
Whether it's the price of
products in your Amazon cart,

00:10:03.010 --> 00:10:05.560 align:middle line:10%
whether you can get on
a particular flight,

00:10:05.560 --> 00:10:09.070 align:middle line:10%
whether you can reserve a room
at a particular hotel, what

00:10:09.070 --> 00:10:11.950 align:middle line:10%
you're experiencing are
machine learning algorithms

00:10:11.950 --> 00:10:13.600 align:middle line:10%
that have determined
that a person like

00:10:13.600 --> 00:10:17.050 align:middle line:10%
you is willing to pay $0.02
more and is changing the price.

00:10:17.050 --> 00:10:21.730 align:middle line:90%


00:10:21.730 --> 00:10:24.070 align:middle line:84%
>> Now, computer looks
at millions of people

00:10:24.070 --> 00:10:28.070 align:middle line:84%
simultaneously for
very subtle patterns.

00:10:28.070 --> 00:10:31.270 align:middle line:10%
It can take seemingly
innocent digital footprints,

00:10:31.270 --> 00:10:34.720 align:middle line:10%
such as someone's
playlist on Spotify,

00:10:34.720 --> 00:10:37.150 align:middle line:10%
or stuff that they
bought on Amazon,

00:10:37.150 --> 00:10:40.270 align:middle line:10%
and then use algorithms
to translate this

00:10:40.270 --> 00:10:43.585 align:middle line:10%
into a very detailed and very
accurate intimate profile.

00:10:43.585 --> 00:10:47.620 align:middle line:90%


00:10:47.620 --> 00:10:51.100 align:middle line:84%
>> There is a dossier on each
of us that is so extensive it

00:10:51.100 --> 00:10:53.710 align:middle line:84%
would be possibly accurate to
say that they know more about

00:10:53.710 --> 00:10:55.360 align:middle line:90%
you than your mother does.

00:10:55.360 --> 00:11:04.610 align:middle line:90%


00:11:04.610 --> 00:11:07.430 align:middle line:10%
>> Major cause of the recent AI
breakthrough it isn't just that

00:11:07.430 --> 00:11:11.480 align:middle line:10%
some dude had a brilliant
insight or something,

00:11:11.480 --> 00:11:14.840 align:middle line:10%
but simply that we have much
bigger data to train them

00:11:14.840 --> 00:11:18.390 align:middle line:10%
on and vastly better computers.

00:11:18.390 --> 00:11:20.070 align:middle line:90%
>> Magic is in the data.

00:11:20.070 --> 00:11:21.740 align:middle line:90%
It's a ton of data.

00:11:21.740 --> 00:11:23.780 align:middle line:10%
I mean, it's data that's
never existed before.

00:11:23.780 --> 00:11:26.630 align:middle line:10%
We've never had
this data before.

00:11:26.630 --> 00:11:31.040 align:middle line:10%
>> We've created technologies
that allow us to capture vast

00:11:31.040 --> 00:11:33.050 align:middle line:10%
amounts of information.

00:11:33.050 --> 00:11:34.940 align:middle line:84%
If you think of a
billion cell phones

00:11:34.940 --> 00:11:39.080 align:middle line:84%
on the planet with gyroscopes,
and accelerometers, fingerprint

00:11:39.080 --> 00:11:42.170 align:middle line:84%
readers, couple that with a
GPS and the photos they take,

00:11:42.170 --> 00:11:43.900 align:middle line:84%
and the tweets that
you send, we're

00:11:43.900 --> 00:11:47.630 align:middle line:84%
all giving off huge amounts
of data individually.

00:11:47.630 --> 00:11:49.700 align:middle line:84%
Cars that drive as
the cameras on them

00:11:49.700 --> 00:11:52.050 align:middle line:84%
suck up information about
the world around them.

00:11:52.050 --> 00:11:54.800 align:middle line:84%
The satellites that are now in
orbit the size of a toaster.

00:11:54.800 --> 00:11:57.530 align:middle line:84%
The infrared about the
vegetation on the planet.

00:11:57.530 --> 00:11:59.060 align:middle line:10%
The buoys that are
out in the oceans

00:11:59.060 --> 00:12:00.530 align:middle line:10%
to feed into the climate models.

00:12:00.530 --> 00:12:05.030 align:middle line:90%


00:12:05.030 --> 00:12:08.270 align:middle line:84%
And the NSA, the
CIA, as they collect

00:12:08.270 --> 00:12:12.550 align:middle line:84%
information about the
geopolitical situations.

00:12:12.550 --> 00:12:16.480 align:middle line:84%
The world today is literally
swimming in this data.

00:12:16.480 --> 00:12:18.830 align:middle line:90%
[DRAMATIC MUSIC PLAYING]

00:12:18.830 --> 00:12:20.720 align:middle line:90%


00:12:20.720 --> 00:12:25.940 align:middle line:84%
>> Back in 2012, IBM estimated
that an average human being

00:12:25.940 --> 00:12:31.070 align:middle line:84%
leaves 500 megabytes of
digital footprints every day.

00:12:31.070 --> 00:12:34.760 align:middle line:10%
If you wanted to back up
only one day worth of data

00:12:34.760 --> 00:12:38.600 align:middle line:10%
that humanity produces and you
print it out on a letter size

00:12:38.600 --> 00:12:43.780 align:middle line:10%
paper, double-sided, font
size 12, and you stack it up,

00:12:43.780 --> 00:12:47.750 align:middle line:10%
it would reach from the
surface of the Earth to the sun

00:12:47.750 --> 00:12:49.370 align:middle line:10%
four times over.

00:12:49.370 --> 00:12:51.320 align:middle line:90%
This every day.

00:12:51.320 --> 00:12:53.810 align:middle line:84%
>> The data itself
is not good or evil.

00:12:53.810 --> 00:12:55.550 align:middle line:90%
It's how it's used.

00:12:55.550 --> 00:12:58.310 align:middle line:84%
We're relying really on the
goodwill of these people

00:12:58.310 --> 00:13:01.190 align:middle line:84%
and on the policies
of these companies.

00:13:01.190 --> 00:13:04.160 align:middle line:84%
There is no legal requirement
for how they can and should

00:13:04.160 --> 00:13:06.260 align:middle line:90%
use that kind of data.

00:13:06.260 --> 00:13:08.670 align:middle line:84%
>> That, to me, is at the
heart of the trust issue.

00:13:08.670 --> 00:13:11.230 align:middle line:90%


00:13:11.230 --> 00:13:13.870 align:middle line:84%
>> Right now there's a giant
race for creating machines that

00:13:13.870 --> 00:13:15.670 align:middle line:90%
are as smart as humans.

00:13:15.670 --> 00:13:17.350 align:middle line:10%
Google-- they're
working on what's

00:13:17.350 --> 00:13:18.850 align:middle line:10%
really the kind of
Manhattan Project

00:13:18.850 --> 00:13:19.840 align:middle line:10%
of artificial intelligence.

00:13:19.840 --> 00:13:21.070 align:middle line:10%
They've got the most money.

00:13:21.070 --> 00:13:22.630 align:middle line:10%
They've got the most talent.

00:13:22.630 --> 00:13:27.030 align:middle line:10%
They're buying up AI companies
and robotics companies.

00:13:27.030 --> 00:13:29.250 align:middle line:84%
>> People still think of
Google as a search engine,

00:13:29.250 --> 00:13:32.190 align:middle line:84%
and their email provider, and a
lot of other things that we use

00:13:32.190 --> 00:13:33.840 align:middle line:90%
on a daily basis.

00:13:33.840 --> 00:13:39.660 align:middle line:10%
But behind that search box
are 10 million servers.

00:13:39.660 --> 00:13:41.610 align:middle line:84%
That makes Google the
most powerful computing

00:13:41.610 --> 00:13:43.860 align:middle line:90%
platform in the world.

00:13:43.860 --> 00:13:47.310 align:middle line:84%
Google is now working on an
AI computing platform that

00:13:47.310 --> 00:13:49.180 align:middle line:90%
will have 100 million servers.

00:13:49.180 --> 00:13:52.150 align:middle line:90%


00:13:52.150 --> 00:13:53.910 align:middle line:84%
So when you're
interacting with Google,

00:13:53.910 --> 00:13:56.190 align:middle line:84%
we're just seeing the
toenail of something that

00:13:56.190 --> 00:13:58.890 align:middle line:90%
is a giant beast in the making.

00:13:58.890 --> 00:14:01.080 align:middle line:84%
And the truth is I'm not
even sure that Google

00:14:01.080 --> 00:14:02.150 align:middle line:90%
knows what it's becoming.

00:14:02.150 --> 00:14:11.490 align:middle line:90%


00:14:11.490 --> 00:14:14.080 align:middle line:10%
>> If you look inside of what
algorithms are being used

00:14:14.080 --> 00:14:20.070 align:middle line:10%
at Google, it's technology
largely from the '80s.

00:14:20.070 --> 00:14:22.920 align:middle line:84%
So these are models that
you train by showing them

00:14:22.920 --> 00:14:24.720 align:middle line:90%
a 1, a 2, and a 3.

00:14:24.720 --> 00:14:27.360 align:middle line:10%
And it learns not what
a 1 is or what a 2 is.

00:14:27.360 --> 00:14:30.420 align:middle line:10%
It learns what the difference
between a 1 and a 2 is.

00:14:30.420 --> 00:14:32.430 align:middle line:10%
It's just a computation.

00:14:32.430 --> 00:14:35.400 align:middle line:10%
>> In the last half-decade where
we've made this rapid progress,

00:14:35.400 --> 00:14:38.280 align:middle line:10%
it has all been in
pattern recognition.

00:14:38.280 --> 00:14:42.390 align:middle line:84%
>> Most of the good,
old-fashioned AI was when we

00:14:42.390 --> 00:14:46.680 align:middle line:84%
would tell our computers how
to play a game like chess,

00:14:46.680 --> 00:14:49.960 align:middle line:84%
from the old paradigm where you
just tell the computer exactly

00:14:49.960 --> 00:14:50.760 align:middle line:90%
what to do.

00:14:50.760 --> 00:14:54.162 align:middle line:90%


00:14:54.162 --> 00:14:57.564 align:middle line:10%
>> This is "Jeopardy."

00:14:57.564 --> 00:14:59.510 align:middle line:10%
[THEME MUSIC]

00:14:59.510 --> 00:15:01.592 align:middle line:90%
The IBM Challenge.

00:15:01.592 --> 00:15:02.920 align:middle line:90%
[APPLAUSE]

00:15:02.920 --> 00:15:05.790 align:middle line:10%
>> No one at the time had
thought that a machine could

00:15:05.790 --> 00:15:08.300 align:middle line:10%
have the precision,
and the confidence,

00:15:08.300 --> 00:15:10.970 align:middle line:10%
and the speed to play Jeopardy
well enough against the best

00:15:10.970 --> 00:15:11.930 align:middle line:10%
humans.

00:15:11.930 --> 00:15:15.320 align:middle line:10%
>> Let's play Jeopardy.

00:15:15.320 --> 00:15:18.670 align:middle line:84%
Four-letter word for the iron
fitting on the hoof of a horse.

00:15:18.670 --> 00:15:19.470 align:middle line:10%
Watson?

00:15:19.470 --> 00:15:20.360 align:middle line:10%
>> What is shoe?

00:15:20.360 --> 00:15:20.930 align:middle line:10%
>> You are right.

00:15:20.930 --> 00:15:21.830 align:middle line:10%
You get to pick.

00:15:21.830 --> 00:15:24.965 align:middle line:10%
>> Literary character
APB for $800.

00:15:24.965 --> 00:15:27.106 align:middle line:10%
>> Answer-- the Daily Double.

00:15:27.106 --> 00:15:28.100 align:middle line:10%
[APPLAUSE]

00:15:28.100 --> 00:15:31.670 align:middle line:10%
>> Watson actually got its
knowledge by reading Wikipedia

00:15:31.670 --> 00:15:34.640 align:middle line:10%
and 200 million pages of
natural language documents.

00:15:34.640 --> 00:15:38.170 align:middle line:84%
>> You can't program every
line of how the world works.

00:15:38.170 --> 00:15:40.640 align:middle line:10%
The machine has to
learn by reading.

00:15:40.640 --> 00:15:41.940 align:middle line:10%
>> Now we come to Watson.

00:15:41.940 --> 00:15:43.590 align:middle line:10%
Who is Bram Stoker?

00:15:43.590 --> 00:15:44.890 align:middle line:10%
And the wager--

00:15:44.890 --> 00:15:45.990 align:middle line:10%
[CHEERING]

00:15:45.990 --> 00:15:46.790 align:middle line:10%
Hello!

00:15:46.790 --> 00:15:49.010 align:middle line:10%
$17,973.

00:15:49.010 --> 00:15:52.870 align:middle line:84%
$41,413, and a
two-day total of--

00:15:52.870 --> 00:15:56.650 align:middle line:84%
>> Watson is trained on
huge amounts of text.

00:15:56.650 --> 00:15:59.780 align:middle line:84%
But it's not like it
understands what it's saying.

00:15:59.780 --> 00:16:02.427 align:middle line:10%
It doesn't know that water makes
things wet by touching water

00:16:02.427 --> 00:16:04.760 align:middle line:10%
and by seeing the way things
behave in the world the way

00:16:04.760 --> 00:16:06.170 align:middle line:10%
you and I do.

00:16:06.170 --> 00:16:10.100 align:middle line:84%
>> A lot of language AI today
is not building logical models

00:16:10.100 --> 00:16:11.570 align:middle line:90%
of how the world works.

00:16:11.570 --> 00:16:15.320 align:middle line:84%
Rather, it's looking
at how the words appear

00:16:15.320 --> 00:16:18.230 align:middle line:90%
in the context of other words.

00:16:18.230 --> 00:16:20.210 align:middle line:84%
>> David Ferrucci
developed IBM's Watson.

00:16:20.210 --> 00:16:23.540 align:middle line:10%
And somebody asked
him, does Watson think?

00:16:23.540 --> 00:16:26.752 align:middle line:10%
And he said, does
a submarine swim?

00:16:26.752 --> 00:16:28.460 align:middle line:84%
And what he meant was
when they developed

00:16:28.460 --> 00:16:31.370 align:middle line:84%
submarines they borrowed
basic principles of swimming

00:16:31.370 --> 00:16:32.970 align:middle line:90%
from fish.

00:16:32.970 --> 00:16:35.110 align:middle line:84%
But a submarine swims
farther and faster than fish

00:16:35.110 --> 00:16:36.110 align:middle line:90%
and can carry a huge payload.

00:16:36.110 --> 00:16:36.990 align:middle line:90%
It out-swims fish.

00:16:36.990 --> 00:16:39.820 align:middle line:90%


00:16:39.820 --> 00:16:42.640 align:middle line:10%
>> Watson winning the game
of Jeopardy will go down

00:16:42.640 --> 00:16:46.570 align:middle line:10%
in the history of AI as
a significant milestone.

00:16:46.570 --> 00:16:49.270 align:middle line:10%
We tend to be amazed when
the machine does so well.

00:16:49.270 --> 00:16:52.300 align:middle line:10%
I'm even more amazed when
the computer beats humans

00:16:52.300 --> 00:16:55.090 align:middle line:10%
at things that humans
are naturally good that.

00:16:55.090 --> 00:16:58.060 align:middle line:90%
This is how we make progress.

00:16:58.060 --> 00:17:00.640 align:middle line:84%
In the early days of the
Google Brain project,

00:17:00.640 --> 00:17:02.740 align:middle line:84%
I gave the team a very
simple instruction,

00:17:02.740 --> 00:17:05.790 align:middle line:84%
which was build the biggest
neural network possible,

00:17:05.790 --> 00:17:07.826 align:middle line:90%
like a thousand computers.

00:17:07.826 --> 00:17:10.359 align:middle line:84%
>> A neural net is something
very close to a simulation

00:17:10.359 --> 00:17:12.230 align:middle line:90%
of how the brain works.

00:17:12.230 --> 00:17:16.593 align:middle line:84%
It's very probabilistic, but
with contextual relevance.

00:17:16.593 --> 00:17:18.760 align:middle line:84%
>> In your brain, you have
long neurons that connect

00:17:18.760 --> 00:17:20.230 align:middle line:90%
to thousands of other neurons.

00:17:20.230 --> 00:17:22.810 align:middle line:84%
And you have these pathways
that are formed and forged based

00:17:22.810 --> 00:17:24.700 align:middle line:90%
on what the brain needs to do.

00:17:24.700 --> 00:17:26.950 align:middle line:84%
When a baby tries
something and it succeeds,

00:17:26.950 --> 00:17:30.410 align:middle line:84%
there's a reward, and
that pathway that created

00:17:30.410 --> 00:17:32.300 align:middle line:90%
the success is strengthened.

00:17:32.300 --> 00:17:34.640 align:middle line:84%
If it fails at something,
the pathway is weakened.

00:17:34.640 --> 00:17:36.170 align:middle line:84%
And so, over time,
the brain becomes

00:17:36.170 --> 00:17:40.053 align:middle line:84%
honed to be good at the
environment around it.

00:17:40.053 --> 00:17:41.970 align:middle line:84%
>> Really, it's just
getting machines to learn

00:17:41.970 --> 00:17:43.040 align:middle line:90%
by themselves.

00:17:43.040 --> 00:17:44.418 align:middle line:90%
It's called deep learning.

00:17:44.418 --> 00:17:45.960 align:middle line:84%
And deep learning
and neural networks

00:17:45.960 --> 00:17:48.630 align:middle line:90%
mean roughly the same thing.

00:17:48.630 --> 00:17:52.620 align:middle line:84%
>> Deep learning is a totally
different approach where

00:17:52.620 --> 00:17:55.230 align:middle line:84%
the computer learns
more like a toddler,

00:17:55.230 --> 00:17:58.920 align:middle line:84%
by just getting a lot of data
and eventually figuring stuff

00:17:58.920 --> 00:18:00.300 align:middle line:90%
out.

00:18:00.300 --> 00:18:03.090 align:middle line:10%
The computer just gets
smarter and smarter

00:18:03.090 --> 00:18:05.950 align:middle line:10%
as it has more experiences.

00:18:05.950 --> 00:18:08.090 align:middle line:84%
>> So imagine, if you
will, a neural network,

00:18:08.090 --> 00:18:09.660 align:middle line:90%
like a thousand computers.

00:18:09.660 --> 00:18:11.460 align:middle line:84%
And it wakes up not
knowing anything,

00:18:11.460 --> 00:18:16.035 align:middle line:84%
and we made it watch
YouTube for a week.

00:18:16.035 --> 00:18:20.070 align:middle line:90%
[MUSIC - PSY, "GANGNAM STYLE"]

00:18:20.070 --> 00:18:22.830 align:middle line:90%
[SCREAMS]

00:18:22.830 --> 00:18:24.790 align:middle line:90%
[GIGGLING]

00:18:24.790 --> 00:18:26.260 align:middle line:90%
>> You're annoying.

00:18:26.260 --> 00:18:27.730 align:middle line:90%
That really hurt!

00:18:27.730 --> 00:18:36.985 align:middle line:90%


00:18:36.985 --> 00:18:38.860 align:middle line:84%
>> And so, after watching
YouTube for a week,

00:18:38.860 --> 00:18:39.850 align:middle line:90%
what would they learn?

00:18:39.850 --> 00:18:42.130 align:middle line:84%
We had a hypothesis they'll
learn to detect commonly

00:18:42.130 --> 00:18:44.410 align:middle line:90%
occurring objects in videos.

00:18:44.410 --> 00:18:47.707 align:middle line:84%
And so we know that human
faces appear a lot in videos.

00:18:47.707 --> 00:18:49.290 align:middle line:84%
So we looked, and
lo and behold, there

00:18:49.290 --> 00:18:52.106 align:middle line:84%
was a neuron that had learned
to detect human faces.

00:18:52.106 --> 00:18:55.840 align:middle line:84%
>> (ANGRILY) Leave
Britney alone!

00:18:55.840 --> 00:19:00.040 align:middle line:84%
>> And there were, well, what
else appears in videos a lot?

00:19:00.040 --> 00:19:00.840 align:middle line:90%
So we looked.

00:19:00.840 --> 00:19:02.465 align:middle line:84%
And to our surprise,
there was actually

00:19:02.465 --> 00:19:05.110 align:middle line:84%
a neuron that had
learned to detect cats.

00:19:05.110 --> 00:19:08.540 align:middle line:90%
[SILLY MUSIC PLAYING]

00:19:08.540 --> 00:19:14.930 align:middle line:90%


00:19:14.930 --> 00:19:17.070 align:middle line:84%
I still remember
scenes of recognition.

00:19:17.070 --> 00:19:18.000 align:middle line:90%
Wow, there's a cat!

00:19:18.000 --> 00:19:18.560 align:middle line:90%
OK, cool.

00:19:18.560 --> 00:19:19.360 align:middle line:90%
Great.

00:19:19.360 --> 00:19:22.928 align:middle line:90%


00:19:22.928 --> 00:19:25.220 align:middle line:84%
>> It's all pretty innocuous
when you're thinking about

00:19:25.220 --> 00:19:26.340 align:middle line:90%
the future.

00:19:26.340 --> 00:19:29.660 align:middle line:84%
It all seems kind of
harmless and benign.

00:19:29.660 --> 00:19:31.610 align:middle line:84%
But we're making
cognitive architectures

00:19:31.610 --> 00:19:33.500 align:middle line:84%
that will fly farther
and faster than us

00:19:33.500 --> 00:19:35.030 align:middle line:90%
and carry a bigger payload.

00:19:35.030 --> 00:19:37.490 align:middle line:84%
And they won't be
warm and fuzzy.

00:19:37.490 --> 00:19:39.720 align:middle line:84%
>> I think that, in
three to five years,

00:19:39.720 --> 00:19:42.590 align:middle line:84%
you will see a computer
system that will be able

00:19:42.590 --> 00:19:46.460 align:middle line:84%
to autonomously learn
how to understand,

00:19:46.460 --> 00:19:49.920 align:middle line:84%
how to build understanding, not
unlike the way the human mind

00:19:49.920 --> 00:19:50.720 align:middle line:90%
works.

00:19:50.720 --> 00:19:54.180 align:middle line:90%


00:19:54.180 --> 00:19:57.500 align:middle line:10%
>> Whatever that lunch was,
it was certainly delicious.

00:19:57.500 --> 00:19:59.990 align:middle line:10%
>> Simply some of
Robby's synthetics.

00:19:59.990 --> 00:20:01.520 align:middle line:10%
>> He's your cook, too?

00:20:01.520 --> 00:20:04.540 align:middle line:84%
>> He even manufactures
the raw materials.

00:20:04.540 --> 00:20:11.020 align:middle line:84%
Come round here, Robby, I'll
show you how this works.

00:20:11.020 --> 00:20:15.370 align:middle line:10%
One introduces a sample of human
food through this aperture.

00:20:15.370 --> 00:20:18.040 align:middle line:84%
Down here there's a small,
built-in chemical laboratory

00:20:18.040 --> 00:20:19.030 align:middle line:90%
where he'll analyze it.

00:20:19.030 --> 00:20:21.260 align:middle line:10%
Later he can reproduce
identical molecules

00:20:21.260 --> 00:20:22.960 align:middle line:10%
in any shape or quantity.

00:20:22.960 --> 00:20:25.840 align:middle line:10%
>> That's a housewife's dream.

00:20:25.840 --> 00:20:29.500 align:middle line:84%
>> Meet Baxter, revolutionary
new category of robots with

00:20:29.500 --> 00:20:31.630 align:middle line:90%
common sense.

00:20:31.630 --> 00:20:33.730 align:middle line:84%
>> Baxter is a really
good example of the kind

00:20:33.730 --> 00:20:37.480 align:middle line:84%
of competition we
face from machines.

00:20:37.480 --> 00:20:42.640 align:middle line:84%
Baxter can do almost anything
we can do with our hands.

00:20:42.640 --> 00:20:45.400 align:middle line:84%
Baxter costs about
what a minimum wage

00:20:45.400 --> 00:20:47.327 align:middle line:90%
worker makes in a year.

00:20:47.327 --> 00:20:49.660 align:middle line:84%
But Baxter won't be taking
the place of one minimum wage

00:20:49.660 --> 00:20:49.990 align:middle line:90%
worker.

00:20:49.990 --> 00:20:51.100 align:middle line:84%
He'll be taking
the place of three

00:20:51.100 --> 00:20:52.600 align:middle line:90%
because they never get tired.

00:20:52.600 --> 00:20:55.480 align:middle line:90%
They never take breaks.

00:20:55.480 --> 00:20:57.820 align:middle line:84%
>> That's probably the first
thing we're going to see--

00:20:57.820 --> 00:20:59.382 align:middle line:90%
displacement of jobs.

00:20:59.382 --> 00:21:01.090 align:middle line:10%
They're going to be
done quicker, faster,

00:21:01.090 --> 00:21:04.670 align:middle line:10%
cheaper by machines.

00:21:04.670 --> 00:21:07.610 align:middle line:10%
>> Our ability to even stay
current is so insanely limited

00:21:07.610 --> 00:21:10.940 align:middle line:10%
compared to the machines
we built. For example,

00:21:10.940 --> 00:21:14.037 align:middle line:10%
now we have this great movement
of Uber and Lyft kind of making

00:21:14.037 --> 00:21:16.370 align:middle line:10%
transportation cheaper and
democratizing transportation,

00:21:16.370 --> 00:21:17.405 align:middle line:10%
which is great.

00:21:17.405 --> 00:21:19.280 align:middle line:84%
The next step is going
to be that they're all

00:21:19.280 --> 00:21:21.080 align:middle line:84%
going to be replaced
by driverless cars.

00:21:21.080 --> 00:21:22.150 align:middle line:84%
And then all the
Uber and Lyft drivers

00:21:22.150 --> 00:21:23.525 align:middle line:84%
have to find
something new to do.

00:21:23.525 --> 00:21:25.960 align:middle line:90%


00:21:25.960 --> 00:21:28.080 align:middle line:84%
>> There are four million
professional drivers

00:21:28.080 --> 00:21:29.690 align:middle line:90%
in the United States.

00:21:29.690 --> 00:21:31.340 align:middle line:90%
They're unemployed soon.

00:21:31.340 --> 00:21:34.010 align:middle line:84%
Seven million people
that do data entry.

00:21:34.010 --> 00:21:37.360 align:middle line:10%
Those people are
going to be jobless.

00:21:37.360 --> 00:21:40.450 align:middle line:84%
>> A job isn't just
about money, right?

00:21:40.450 --> 00:21:43.360 align:middle line:84%
On a biological level,
it serves a purpose,

00:21:43.360 --> 00:21:45.370 align:middle line:90%
becomes a defining thing.

00:21:45.370 --> 00:21:48.380 align:middle line:84%
When the jobs went away
in any given civilization,

00:21:48.380 --> 00:21:50.709 align:middle line:84%
it doesn't take long until
that turns into violence.

00:21:50.709 --> 00:21:53.104 align:middle line:90%
[SHOUTING]

00:21:53.104 --> 00:21:55.499 align:middle line:90%


00:21:55.499 --> 00:21:58.373 align:middle line:90%
[EXPLOSIONS]

00:21:58.373 --> 00:22:00.032 align:middle line:90%


00:22:00.032 --> 00:22:01.990 align:middle line:84%
>> We face a giant divide
between rich and poor

00:22:01.990 --> 00:22:04.950 align:middle line:84%
because that's what automation
and AI will provoke--

00:22:04.950 --> 00:22:08.590 align:middle line:84%
a greater divide between
the haves and the have nots.

00:22:08.590 --> 00:22:10.780 align:middle line:84%
Right now, it's working
into the middle class,

00:22:10.780 --> 00:22:12.910 align:middle line:90%
into white collar jobs.

00:22:12.910 --> 00:22:15.280 align:middle line:10%
IBM's Watson does
business analytics

00:22:15.280 --> 00:22:19.950 align:middle line:10%
that we used to pay a business
analyst $300 an hour to do.

00:22:19.950 --> 00:22:23.020 align:middle line:84%
>> Today, you go into
college to be a doctor,

00:22:23.020 --> 00:22:25.090 align:middle line:84%
to be an accountant,
to be a journalist.

00:22:25.090 --> 00:22:29.420 align:middle line:10%
It's unclear that there's
going to be jobs there for you.

00:22:29.420 --> 00:22:32.560 align:middle line:84%
>> If someone is planning for
a 40-year career in radiology

00:22:32.560 --> 00:22:35.680 align:middle line:84%
just reading images, I think
that could be a challenge

00:22:35.680 --> 00:22:36.960 align:middle line:90%
to the new graduates of today.

00:22:36.960 --> 00:22:39.918 align:middle line:90%


00:22:39.918 --> 00:22:42.876 align:middle line:90%
[BEEPING]

00:22:42.876 --> 00:22:56.340 align:middle line:90%


00:22:56.340 --> 00:22:59.580 align:middle line:10%
>> Today we're going
to do a robotic case.

00:22:59.580 --> 00:23:03.930 align:middle line:84%
>> The Da Vinci robot is
currently utilized by a variety

00:23:03.930 --> 00:23:08.550 align:middle line:84%
of surgeons for its accuracy
and its ability to avoid

00:23:08.550 --> 00:23:12.090 align:middle line:84%
the inevitable fluctuations
of the human hand.

00:23:12.090 --> 00:23:23.320 align:middle line:90%


00:23:23.320 --> 00:23:26.270 align:middle line:84%
Anybody who watches this
feels the amazingness of it.

00:23:26.270 --> 00:23:30.900 align:middle line:90%


00:23:30.900 --> 00:23:34.650 align:middle line:84%
You look through the scope,
and you see the claw hand

00:23:34.650 --> 00:23:36.900 align:middle line:90%
holding that woman's ovary.

00:23:36.900 --> 00:23:38.850 align:middle line:84%
Humanity was
resting right there,

00:23:38.850 --> 00:23:42.600 align:middle line:90%
in the hands of this robot.

00:23:42.600 --> 00:23:44.830 align:middle line:90%
People say it's the future.

00:23:44.830 --> 00:23:46.334 align:middle line:90%
But it's not the future.

00:23:46.334 --> 00:23:47.726 align:middle line:90%
It's the present.

00:23:47.726 --> 00:23:50.510 align:middle line:90%


00:23:50.510 --> 00:23:52.377 align:middle line:84%
>> If you think about
a surgical robot,

00:23:52.377 --> 00:23:54.710 align:middle line:84%
there's often not a lot of
intelligence in these things.

00:23:54.710 --> 00:23:57.260 align:middle line:10%
But over time, as we put
more and more intelligence

00:23:57.260 --> 00:23:59.630 align:middle line:10%
into these systems,
the surgical robots

00:23:59.630 --> 00:24:02.238 align:middle line:10%
can actually learn from
each robot surgery.

00:24:02.238 --> 00:24:03.530 align:middle line:90%
They're tracking the movements.

00:24:03.530 --> 00:24:05.960 align:middle line:84%
They're understanding what
worked and what didn't work.

00:24:05.960 --> 00:24:08.720 align:middle line:84%
And eventually the robot
for routine surgeries

00:24:08.720 --> 00:24:11.060 align:middle line:84%
is going to be able to
perform that entirely

00:24:11.060 --> 00:24:14.570 align:middle line:84%
by itself, or with
human supervision.

00:24:14.570 --> 00:24:17.620 align:middle line:10%
>> Normally, I do about 150
cases of hysterectomies,

00:24:17.620 --> 00:24:18.800 align:middle line:10%
let's say.

00:24:18.800 --> 00:24:23.320 align:middle line:10%
And now most of them
are done robotically.

00:24:23.320 --> 00:24:26.330 align:middle line:10%
I do maybe one open case a year.

00:24:26.330 --> 00:24:28.190 align:middle line:10%
So do I feel uncomfortable?

00:24:28.190 --> 00:24:29.780 align:middle line:10%
Of course I do
feel uncomfortable,

00:24:29.780 --> 00:24:32.458 align:middle line:10%
because I don't remember how
to open patients anymore.

00:24:32.458 --> 00:24:35.067 align:middle line:90%


00:24:35.067 --> 00:24:37.150 align:middle line:84%
>> It seems that we're
feeding it and creating it,

00:24:37.150 --> 00:24:43.970 align:middle line:84%
but, in a way, we are a slave
to the technology because we

00:24:43.970 --> 00:24:44.770 align:middle line:90%
can't go back.

00:24:44.770 --> 00:24:50.340 align:middle line:90%


00:24:50.340 --> 00:24:53.070 align:middle line:84%
>> The machines are taking
bigger and bigger bites out

00:24:53.070 --> 00:24:57.200 align:middle line:84%
of our skill set at an
ever-increasing speed.

00:24:57.200 --> 00:24:59.160 align:middle line:84%
And so we've got to
run faster and faster

00:24:59.160 --> 00:25:02.540 align:middle line:90%
to keep ahead of the machines.

00:25:02.540 --> 00:25:04.640 align:middle line:90%
>> How do I look?

00:25:04.640 --> 00:25:05.818 align:middle line:10%
>> Good.

00:25:05.818 --> 00:25:09.950 align:middle line:90%


00:25:09.950 --> 00:25:11.280 align:middle line:90%
>> Are you attracted to me?

00:25:11.280 --> 00:25:12.080 align:middle line:90%
>> What?

00:25:12.080 --> 00:25:14.210 align:middle line:90%
>> Are you attracted to me?

00:25:14.210 --> 00:25:18.020 align:middle line:84%
You give me indications
that you are.

00:25:18.020 --> 00:25:18.820 align:middle line:90%
>> I do?

00:25:18.820 --> 00:25:20.590 align:middle line:90%
>> Yes.

00:25:20.590 --> 00:25:22.720 align:middle line:84%
>> This is the future
we're headed into.

00:25:22.720 --> 00:25:26.450 align:middle line:84%
We want to design
our companions.

00:25:26.450 --> 00:25:29.200 align:middle line:84%
And we're going to like to
see a human face on the AI.

00:25:29.200 --> 00:25:34.210 align:middle line:84%
Therefore, gaming our emotions
will be depressingly easy.

00:25:34.210 --> 00:25:35.560 align:middle line:90%
We're not that complicated.

00:25:35.560 --> 00:25:38.140 align:middle line:84%
We're simple--
stimulus-response.

00:25:38.140 --> 00:25:42.725 align:middle line:84%
I can make you like me basically
by smiling at you a lot.

00:25:42.725 --> 00:25:45.380 align:middle line:84%
AI is going to be fantastic
at manipulating us.

00:25:45.380 --> 00:25:54.272 align:middle line:90%


00:25:54.272 --> 00:25:58.470 align:middle line:84%
>> So you've developed a
technology that can sense what

00:25:58.470 --> 00:25:59.500 align:middle line:90%
people are feeling.

00:25:59.500 --> 00:26:00.300 align:middle line:90%
>> Right.

00:26:00.300 --> 00:26:01.830 align:middle line:84%
We've developed
technology that can

00:26:01.830 --> 00:26:03.930 align:middle line:84%
read your facial
expressions and map that

00:26:03.930 --> 00:26:06.450 align:middle line:90%
to a number of emotional states.

00:26:06.450 --> 00:26:08.670 align:middle line:84%
15 years ago, I
had just finished

00:26:08.670 --> 00:26:11.460 align:middle line:84%
my undergraduate studies
in computer science.

00:26:11.460 --> 00:26:13.590 align:middle line:84%
And it struck me
that I was spending

00:26:13.590 --> 00:26:17.730 align:middle line:84%
a lot of time interacting with
my laptops and my devices.

00:26:17.730 --> 00:26:23.550 align:middle line:84%
Yet these devices had absolutely
no clue how I was feeling.

00:26:23.550 --> 00:26:25.950 align:middle line:10%
I started thinking, what
if this device could

00:26:25.950 --> 00:26:29.250 align:middle line:10%
sense that I was stressed
or I was having a bad day?

00:26:29.250 --> 00:26:32.282 align:middle line:90%
What would that open up?

00:26:32.282 --> 00:26:34.310 align:middle line:90%
Hi, first graders?

00:26:34.310 --> 00:26:35.840 align:middle line:90%
How are you?

00:26:35.840 --> 00:26:37.920 align:middle line:90%
Can I get a hug?

00:26:37.920 --> 00:26:40.760 align:middle line:84%
We have kids interact
with the technology.

00:26:40.760 --> 00:26:42.650 align:middle line:84%
A lot of it is still
in development.

00:26:42.650 --> 00:26:44.420 align:middle line:90%
But it was just amazing.

00:26:44.420 --> 00:26:45.590 align:middle line:90%
Who likes robots?

00:26:45.590 --> 00:26:46.790 align:middle line:90%
[GASPING]

00:26:46.790 --> 00:26:48.530 align:middle line:84%
Who wants to have a
robot in their house?

00:26:48.530 --> 00:26:49.363 align:middle line:90%
[GASPING]

00:26:49.363 --> 00:26:51.280 align:middle line:84%
What would you use a
robot for, Jack, at home?

00:26:51.280 --> 00:26:55.880 align:middle line:84%
>> I would use it to ask my
mom very hard math questions.

00:26:55.880 --> 00:26:56.680 align:middle line:90%
>> OK.

00:26:56.680 --> 00:26:58.090 align:middle line:90%
[LAUGHS] What about you, Theo?

00:26:58.090 --> 00:27:01.800 align:middle line:84%
>> I would use it
for scaring people.

00:27:01.800 --> 00:27:02.600 align:middle line:90%
>> All right.

00:27:02.600 --> 00:27:05.300 align:middle line:90%
So start by smiling.

00:27:05.300 --> 00:27:06.560 align:middle line:90%
Nice.

00:27:06.560 --> 00:27:08.850 align:middle line:90%
Brow furrow.

00:27:08.850 --> 00:27:09.650 align:middle line:90%
Nice one.

00:27:09.650 --> 00:27:11.060 align:middle line:90%
Eyebrow raised.

00:27:11.060 --> 00:27:12.740 align:middle line:84%
This generation,
technology is just

00:27:12.740 --> 00:27:15.352 align:middle line:90%
surrounding them all the time.

00:27:15.352 --> 00:27:17.810 align:middle line:84%
It's almost like they expect
to have robots in their homes,

00:27:17.810 --> 00:27:22.250 align:middle line:84%
and they expect these robots
to be socially intelligent.

00:27:22.250 --> 00:27:25.460 align:middle line:90%
What makes robots smart?

00:27:25.460 --> 00:27:29.690 align:middle line:84%
>> Put them in, like, a
math or biology class.

00:27:29.690 --> 00:27:31.980 align:middle line:84%
>> I think you would
have to train it.

00:27:31.980 --> 00:27:32.780 align:middle line:90%
>> All right.

00:27:32.780 --> 00:27:35.310 align:middle line:90%
Let's walk over here.

00:27:35.310 --> 00:27:37.370 align:middle line:84%
So if you smile and you
raise your eyebrows,

00:27:37.370 --> 00:27:38.730 align:middle line:90%
it's going to run over to you.

00:27:38.730 --> 00:27:39.530 align:middle line:90%
It's coming over!

00:27:39.530 --> 00:27:40.210 align:middle line:90%
It's coming over!

00:27:40.210 --> 00:27:41.120 align:middle line:90%
What?

00:27:41.120 --> 00:27:43.040 align:middle line:90%
[LAUGHTER]

00:27:43.040 --> 00:27:45.050 align:middle line:84%
But if you look angry,
it's going to run away.

00:27:45.050 --> 00:27:45.994 align:middle line:90%
>> Run away?

00:27:45.994 --> 00:27:46.940 align:middle line:90%
[ROARING]

00:27:46.940 --> 00:27:49.190 align:middle line:90%
>> Oh, that's good.

00:27:49.190 --> 00:27:52.670 align:middle line:84%
We're training computers to
read and recognize emotions.

00:27:52.670 --> 00:27:54.970 align:middle line:90%
Ready, set, go!

00:27:54.970 --> 00:27:57.320 align:middle line:84%
And the response so far
has been really amazing.

00:27:57.320 --> 00:28:00.560 align:middle line:84%
People are integrating this into
health apps, meditation apps,

00:28:00.560 --> 00:28:04.560 align:middle line:90%
robots, cars.

00:28:04.560 --> 00:28:06.484 align:middle line:84%
We're going to see
how this unfolds.

00:28:06.484 --> 00:28:09.390 align:middle line:90%
[CHILDREN SHOUTING EXCITEDLY]

00:28:09.390 --> 00:28:11.460 align:middle line:90%
Robots can contain AI.

00:28:11.460 --> 00:28:14.310 align:middle line:84%
But the robot is just a
physical instantiation,

00:28:14.310 --> 00:28:16.750 align:middle line:84%
and the artificial
intelligence is the brain.

00:28:16.750 --> 00:28:19.950 align:middle line:84%
And so brains can exist purely
in software-based systems.

00:28:19.950 --> 00:28:22.440 align:middle line:84%
They don't need to
have a physical form.

00:28:22.440 --> 00:28:25.210 align:middle line:84%
Robots can exist without
any artificial intelligence.

00:28:25.210 --> 00:28:28.040 align:middle line:84%
We have a lot of dumb
robots out there.

00:28:28.040 --> 00:28:31.670 align:middle line:84%
But a dumb robot can be
a smart robot overnight,

00:28:31.670 --> 00:28:35.330 align:middle line:84%
given the right software,
given the right sensors.

00:28:35.330 --> 00:28:38.600 align:middle line:84%
>> We can't help but impute
motive into inanimate objects.

00:28:38.600 --> 00:28:40.128 align:middle line:90%
We do it with machines.

00:28:40.128 --> 00:28:41.420 align:middle line:90%
We'll treat them like children.

00:28:41.420 --> 00:28:43.760 align:middle line:84%
We'll treat them
like surrogates.

00:28:43.760 --> 00:28:45.500 align:middle line:90%
>> Bye!

00:28:45.500 --> 00:28:47.090 align:middle line:90%
>> And we'll pay the price.

00:28:47.090 --> 00:29:08.510 align:middle line:90%


00:29:08.510 --> 00:29:09.560 align:middle line:90%
>> OK, welcome to ATR.

00:29:09.560 --> 00:29:18.920 align:middle line:90%


00:29:18.920 --> 00:29:21.980 align:middle line:10%
My purpose is to have a
more human-like robot which

00:29:21.980 --> 00:29:24.450 align:middle line:10%
has the human-like
intention and desire.

00:29:24.450 --> 00:29:36.040 align:middle line:90%


00:29:36.040 --> 00:29:39.470 align:middle line:10%
The name of the robot is Erica.

00:29:39.470 --> 00:29:43.540 align:middle line:10%
Erica is the most advanced
human-like robot in the world,

00:29:43.540 --> 00:29:45.000 align:middle line:10%
I think.

00:29:45.000 --> 00:29:47.150 align:middle line:10%
Erica can gaze at your face.

00:29:47.150 --> 00:29:52.140 align:middle line:90%


00:29:52.140 --> 00:29:53.132 align:middle line:90%
[SPEAKING JAPANESE]

00:29:53.132 --> 00:29:54.460 align:middle line:10%
>> [SPEAKING JAPANESE]

00:29:54.460 --> 00:29:57.900 align:middle line:10%
>> Robot can be pretty
good conversation partners,

00:29:57.900 --> 00:30:01.200 align:middle line:10%
especially for the elderly,
and young children,

00:30:01.200 --> 00:30:02.220 align:middle line:10%
handicapped people.

00:30:02.220 --> 00:30:03.420 align:middle line:10%
[SPEAKING JAPANESE]

00:30:03.420 --> 00:30:06.750 align:middle line:10%
When we talk to the robot, we
don't feel the social barriers,

00:30:06.750 --> 00:30:09.750 align:middle line:10%
social pressures.

00:30:09.750 --> 00:30:13.560 align:middle line:10%
Finally, everybody
accepts the android

00:30:13.560 --> 00:30:17.130 align:middle line:10%
as just our friend,
our partners.

00:30:17.130 --> 00:30:19.560 align:middle line:10%
We have implemented
a simple desire.

00:30:19.560 --> 00:30:23.130 align:middle line:10%
She wants to be well-recognized,
and she wants to take risks.

00:30:23.130 --> 00:30:29.370 align:middle line:90%


00:30:29.370 --> 00:30:31.770 align:middle line:10%
If a robot could have
intentional desires,

00:30:31.770 --> 00:30:36.300 align:middle line:10%
the robot can understand other
people's intentional desires.

00:30:36.300 --> 00:30:38.106 align:middle line:10%
[SPEAKING JAPANESE]

00:30:38.106 --> 00:30:44.160 align:middle line:10%
>> [SPEAKING JAPANESE]

00:30:44.160 --> 00:30:46.740 align:middle line:84%
That means tighter
relationships with people,

00:30:46.740 --> 00:30:49.660 align:middle line:84%
and that means they
may like each other.

00:30:49.660 --> 00:30:52.260 align:middle line:10%
That means-- well, I'm not sure.

00:30:52.260 --> 00:30:53.220 align:middle line:10%
To love each other?

00:30:53.220 --> 00:30:57.090 align:middle line:90%


00:30:57.090 --> 00:30:58.670 align:middle line:84%
>> We build artificial
intelligence,

00:30:58.670 --> 00:31:02.970 align:middle line:84%
and the very first thing we
want to do is replicate us.

00:31:02.970 --> 00:31:07.360 align:middle line:84%
I think the key point will come
when all the major senses are

00:31:07.360 --> 00:31:09.303 align:middle line:90%
replicated.

00:31:09.303 --> 00:31:14.600 align:middle line:90%
Sight, touch, smell.

00:31:14.600 --> 00:31:17.600 align:middle line:84%
When we replicate our senses
is that when it becomes alive.

00:31:17.600 --> 00:31:27.610 align:middle line:90%


00:31:27.610 --> 00:31:31.250 align:middle line:84%
>> So many of our machines are
being built to understand us.

00:31:31.250 --> 00:31:32.740 align:middle line:90%
>> [NON-ENGLISH SPEECH]

00:31:32.740 --> 00:31:35.440 align:middle line:10%
>> But what happens when an
anthropomorphic discovers that

00:31:35.440 --> 00:31:37.470 align:middle line:10%
they can adjust their loyalty?

00:31:37.470 --> 00:31:39.040 align:middle line:10%
Adjust their courage?

00:31:39.040 --> 00:31:40.000 align:middle line:90%
Adjust their avarice?

00:31:40.000 --> 00:31:41.410 align:middle line:90%
Adjust their cunning?

00:31:41.410 --> 00:31:45.080 align:middle line:90%


00:31:45.080 --> 00:31:47.560 align:middle line:10%
>> The average person, they
don't see killer robots going

00:31:47.560 --> 00:31:48.360 align:middle line:10%
down the streets.

00:31:48.360 --> 00:31:50.840 align:middle line:10%
They're like, what
are you talking about?

00:31:50.840 --> 00:31:53.600 align:middle line:10%
Man, we want to make sure
that we don't have killer

00:31:53.600 --> 00:31:56.815 align:middle line:10%
robots going down the street.

00:31:56.815 --> 00:31:58.940 align:middle line:84%
Once they're going down
the street, it is too late.

00:31:58.940 --> 00:32:05.180 align:middle line:90%


00:32:05.180 --> 00:32:08.540 align:middle line:84%
>> The thing that worries me
right now, that keeps me awake,

00:32:08.540 --> 00:32:11.995 align:middle line:84%
is the development of
autonomous weapons.

00:32:11.995 --> 00:32:14.420 align:middle line:90%
[HEAVY RUMBLING]

00:32:14.420 --> 00:32:20.725 align:middle line:90%


00:32:20.725 --> 00:32:23.635 align:middle line:90%
[JET ENGINE TAKING OFF]

00:32:23.635 --> 00:32:28.020 align:middle line:90%


00:32:28.020 --> 00:32:30.540 align:middle line:84%
Up to now, people
have expressed unease

00:32:30.540 --> 00:32:34.957 align:middle line:84%
about drones, which are
remotely-piloted aircraft.

00:32:34.957 --> 00:32:37.392 align:middle line:90%
[OMINOUS MUSIC PLAYING]

00:32:37.392 --> 00:32:39.830 align:middle line:90%


00:32:39.830 --> 00:32:43.340 align:middle line:84%
If you take a drone's camera,
feed it into the AI system,

00:32:43.340 --> 00:32:47.420 align:middle line:84%
it's a very easy step from here
to fully autonomous weapons

00:32:47.420 --> 00:32:50.060 align:middle line:84%
that choose their own targets
and release their own missiles.

00:32:50.060 --> 00:33:12.600 align:middle line:90%


00:33:12.600 --> 00:33:15.053 align:middle line:84%
The expected lifespan
of a human being

00:33:15.053 --> 00:33:16.470 align:middle line:84%
in that kind of
battle environment

00:33:16.470 --> 00:33:17.637 align:middle line:90%
will be measured in seconds.

00:33:17.637 --> 00:33:20.570 align:middle line:90%


00:33:20.570 --> 00:33:23.820 align:middle line:84%
>> At one point, drones
were science fiction.

00:33:23.820 --> 00:33:28.910 align:middle line:10%
And now they've become
the normal thing in war.

00:33:28.910 --> 00:33:33.410 align:middle line:84%
There's over 10,000 in the
US military inventory alone.

00:33:33.410 --> 00:33:35.280 align:middle line:84%
But they're not just
a US phenomenon.

00:33:35.280 --> 00:33:39.020 align:middle line:84%
There's more than 80
countries that operate them.

00:33:39.020 --> 00:33:42.200 align:middle line:84%
>> It stands to reason that
people making some of the most

00:33:42.200 --> 00:33:44.660 align:middle line:84%
important and difficult
decisions in the world are

00:33:44.660 --> 00:33:46.730 align:middle line:84%
going to start to use
and implement artificial

00:33:46.730 --> 00:33:47.858 align:middle line:90%
intelligence.

00:33:47.858 --> 00:33:50.730 align:middle line:90%


00:33:50.730 --> 00:33:52.860 align:middle line:84%
The Air Force just
designed a $400 billion

00:33:52.860 --> 00:33:56.300 align:middle line:84%
jet program to put
pilots in the sky.

00:33:56.300 --> 00:34:01.230 align:middle line:84%
And a $500 AI, designed by a
couple of graduate students,

00:34:01.230 --> 00:34:04.710 align:middle line:84%
is beating the best human
pilots with a relatively simple

00:34:04.710 --> 00:34:05.714 align:middle line:90%
algorithm.

00:34:05.714 --> 00:34:09.429 align:middle line:90%


00:34:09.429 --> 00:34:13.449 align:middle line:84%
AI will have as big an
impact on the military

00:34:13.449 --> 00:34:17.620 align:middle line:84%
as the combustion engine had
at the turn of the century.

00:34:17.620 --> 00:34:19.449 align:middle line:84%
It will literally
touch everything

00:34:19.449 --> 00:34:22.989 align:middle line:84%
that the military does,
from driverless convoys

00:34:22.989 --> 00:34:27.550 align:middle line:84%
delivering logistical supplies,
to unmanned drones delivering

00:34:27.550 --> 00:34:30.600 align:middle line:84%
medical aid, to
computational propaganda,

00:34:30.600 --> 00:34:34.310 align:middle line:84%
trying to win the hearts
and minds of a population.

00:34:34.310 --> 00:34:37.300 align:middle line:84%
And so it stands to
reason that whoever

00:34:37.300 --> 00:34:39.500 align:middle line:84%
has the best AI will
probably achieve dominance

00:34:39.500 --> 00:34:40.300 align:middle line:90%
on this planet.

00:34:40.300 --> 00:34:44.116 align:middle line:90%


00:34:44.116 --> 00:34:45.550 align:middle line:90%
[THUNDER CRACKS]

00:34:45.550 --> 00:34:47.679 align:middle line:10%
>> At some point in
the early 21st century,

00:34:47.679 --> 00:34:51.159 align:middle line:10%
all of mankind was
united in celebration.

00:34:51.159 --> 00:34:53.800 align:middle line:10%
We marveled at our
own magnificence

00:34:53.800 --> 00:34:56.476 align:middle line:10%
as we gave birth to AI.

00:34:56.476 --> 00:34:58.300 align:middle line:90%
>> AI.

00:34:58.300 --> 00:35:00.460 align:middle line:84%
You mean artificial
intelligence.

00:35:00.460 --> 00:35:03.790 align:middle line:84%
>> A singular consciousness
that spawned an entire race

00:35:03.790 --> 00:35:05.780 align:middle line:90%
of machines.

00:35:05.780 --> 00:35:09.650 align:middle line:84%
We don't know who struck
first, us or them,

00:35:09.650 --> 00:35:12.550 align:middle line:84%
but we know that it was
us that scorched the sky.

00:35:12.550 --> 00:35:14.002 align:middle line:90%
[THUNDER CRACKS]

00:35:14.002 --> 00:35:14.802 align:middle line:90%


00:35:14.802 --> 00:35:17.010 align:middle line:10%
>> There's a long history
of science fiction not just

00:35:17.010 --> 00:35:20.238 align:middle line:10%
predicting the future,
but shaping the future.

00:35:20.238 --> 00:35:23.046 align:middle line:90%
[DRAMATIC MUSIC PLAYING]

00:35:23.046 --> 00:35:26.790 align:middle line:90%


00:35:26.790 --> 00:35:30.390 align:middle line:84%
Arthur Conan Doyle,
writing before World War I

00:35:30.390 --> 00:35:33.750 align:middle line:84%
on the danger of
how submarines might

00:35:33.750 --> 00:35:37.980 align:middle line:84%
be used to carry out
civilian blockades.

00:35:37.980 --> 00:35:40.380 align:middle line:84%
At the time he's
writing this fiction

00:35:40.380 --> 00:35:43.110 align:middle line:84%
the Royal Navy made
fun of Arthur Conan

00:35:43.110 --> 00:35:46.200 align:middle line:84%
Doyle for this absurd
idea that submarines

00:35:46.200 --> 00:35:47.580 align:middle line:90%
could be useful in war.

00:35:47.580 --> 00:35:50.947 align:middle line:90%


00:35:50.947 --> 00:35:52.390 align:middle line:90%
[EXPLOSION]

00:35:52.390 --> 00:35:53.715 align:middle line:90%


00:35:53.715 --> 00:35:55.340 align:middle line:84%
One of the things
we've seen in history

00:35:55.340 --> 00:35:58.220 align:middle line:84%
is that our attitude
towards technology,

00:35:58.220 --> 00:36:01.880 align:middle line:84%
but also ethics, are
very context-dependent.

00:36:01.880 --> 00:36:03.710 align:middle line:90%
For example, the submarine--

00:36:03.710 --> 00:36:06.500 align:middle line:84%
nations like Great Britain
and even the United States

00:36:06.500 --> 00:36:09.830 align:middle line:84%
found it horrifying
to use the submarine.

00:36:09.830 --> 00:36:13.130 align:middle line:84%
In fact, the German use of the
submarine to carry out attacks

00:36:13.130 --> 00:36:18.670 align:middle line:84%
was the reason why the United
States joined World War I.

00:36:18.670 --> 00:36:20.880 align:middle line:90%
But move the timeline forward.

00:36:20.880 --> 00:36:24.590 align:middle line:84%
>> The United States of America
was suddenly and deliberately

00:36:24.590 --> 00:36:28.370 align:middle line:90%
attacked by the Empire of Japan.

00:36:28.370 --> 00:36:30.980 align:middle line:84%
>> Five hours
after Pearl Harbor,

00:36:30.980 --> 00:36:35.150 align:middle line:84%
the order goes out to commit
unrestricted submarine warfare

00:36:35.150 --> 00:36:36.882 align:middle line:90%
against Japan.

00:36:36.882 --> 00:36:38.866 align:middle line:90%
[EXPLOSION]

00:36:38.866 --> 00:36:39.860 align:middle line:90%


00:36:39.860 --> 00:36:44.270 align:middle line:84%
So Arthur Conan Doyle
turned out to be right.

00:36:44.270 --> 00:36:46.820 align:middle line:84%
>> That's the great old
line about science fiction.

00:36:46.820 --> 00:36:49.010 align:middle line:90%
It's a lie that tells the truth.

00:36:49.010 --> 00:36:52.070 align:middle line:10%
>> Fellow executives, it gives
me great pleasure to introduce

00:36:52.070 --> 00:37:03.430 align:middle line:10%
you to the future of
law enforcement, ED-209.

00:37:03.430 --> 00:37:06.010 align:middle line:84%
>> This isn't just a
question of science fiction.

00:37:06.010 --> 00:37:09.190 align:middle line:84%
This is about what's next, about
what's happening right now.

00:37:09.190 --> 00:37:13.880 align:middle line:90%


00:37:13.880 --> 00:37:17.450 align:middle line:10%
>> The role of intelligent
systems is growing very rapidly

00:37:17.450 --> 00:37:19.310 align:middle line:10%
in warfare.

00:37:19.310 --> 00:37:21.710 align:middle line:84%
Everyone is pushing
in the unmanned realm.

00:37:21.710 --> 00:37:26.510 align:middle line:90%


00:37:26.510 --> 00:37:29.100 align:middle line:10%
>> Today, the Secretary of
Defense is very, very clear.

00:37:29.100 --> 00:37:32.720 align:middle line:84%
We will not create fully
autonomous attack vehicles.

00:37:32.720 --> 00:37:34.650 align:middle line:84%
Not everyone is going
to hold themselves

00:37:34.650 --> 00:37:36.500 align:middle line:90%
to that same set of values.

00:37:36.500 --> 00:37:40.700 align:middle line:84%
And when China and Russia start
deploying autonomous vehicles

00:37:40.700 --> 00:37:43.898 align:middle line:84%
that can attack and
kill, what's the move

00:37:43.898 --> 00:37:44.940 align:middle line:90%
that we're going to make?

00:37:44.940 --> 00:37:50.048 align:middle line:90%


00:37:50.048 --> 00:37:52.590 align:middle line:84%
>> You can't say, well, we're
going to use autonomous weapons

00:37:52.590 --> 00:37:55.940 align:middle line:84%
for our military dominance but
no one else is going to use

00:37:55.940 --> 00:37:56.810 align:middle line:90%
them.

00:37:56.810 --> 00:37:58.510 align:middle line:84%
If you make these
weapons, they're

00:37:58.510 --> 00:38:04.235 align:middle line:84%
going to be used to attack human
populations in large numbers.

00:38:04.235 --> 00:38:06.175 align:middle line:90%
[GUNSHOTS]

00:38:06.175 --> 00:38:12.480 align:middle line:90%


00:38:12.480 --> 00:38:14.827 align:middle line:84%
Autonomous weapons
are, by their nature,

00:38:14.827 --> 00:38:16.410 align:middle line:84%
weapons of mass
destruction because it

00:38:16.410 --> 00:38:19.410 align:middle line:84%
doesn't need a human being
to guide it or carry it.

00:38:19.410 --> 00:38:25.770 align:middle line:84%
You only need one person
to write a little program.

00:38:25.770 --> 00:38:30.150 align:middle line:10%
>> It just captures the
complexity of this field.

00:38:30.150 --> 00:38:31.300 align:middle line:10%
It is cool.

00:38:31.300 --> 00:38:32.490 align:middle line:90%
It is important.

00:38:32.490 --> 00:38:34.500 align:middle line:90%
It is amazing.

00:38:34.500 --> 00:38:37.060 align:middle line:90%
It is also frightening.

00:38:37.060 --> 00:38:38.260 align:middle line:90%
And it's all about trust.

00:38:38.260 --> 00:38:42.150 align:middle line:90%


00:38:42.150 --> 00:38:44.510 align:middle line:84%
>> It's an open letter about
artificial intelligence,

00:38:44.510 --> 00:38:47.210 align:middle line:84%
signed by some of the
biggest names in science.

00:38:47.210 --> 00:38:48.280 align:middle line:90%
What do they want?

00:38:48.280 --> 00:38:50.720 align:middle line:84%
Ban the use of
autonomous weapons.

00:38:50.720 --> 00:38:53.780 align:middle line:84%
>> The author stated, quote,
"Autonomous weapons have been

00:38:53.780 --> 00:38:56.562 align:middle line:84%
described as the third
revolution in warfare."

00:38:56.562 --> 00:38:58.520 align:middle line:10%
>> Thousand artificial
intelligence specialists

00:38:58.520 --> 00:39:02.330 align:middle line:10%
calling for a global
ban on killer robots.

00:39:02.330 --> 00:39:05.300 align:middle line:10%
>> This open letter basically
says that we should redefine

00:39:05.300 --> 00:39:08.240 align:middle line:10%
the goal of the field of
artificial intelligence away

00:39:08.240 --> 00:39:11.870 align:middle line:10%
from just creating pure,
undirected intelligence towards

00:39:11.870 --> 00:39:13.700 align:middle line:10%
spreading beneficial
intelligence.

00:39:13.700 --> 00:39:16.040 align:middle line:10%
>> The development of
AI is not going to stop.

00:39:16.040 --> 00:39:18.080 align:middle line:10%
It is going to continue
and get better.

00:39:18.080 --> 00:39:21.620 align:middle line:10%
If the international community
isn't putting certain controls

00:39:21.620 --> 00:39:24.650 align:middle line:10%
on this, people will develop
things that can do anything.

00:39:24.650 --> 00:39:27.620 align:middle line:10%
>> The letter says that
we are years, not decades,

00:39:27.620 --> 00:39:29.510 align:middle line:10%
away from these
weapons being deployed.

00:39:29.510 --> 00:39:30.310 align:middle line:90%
So first of all--

00:39:30.310 --> 00:39:32.420 align:middle line:84%
>> We had 6,000
signatories of that letter,

00:39:32.420 --> 00:39:34.510 align:middle line:84%
including many of the
major figures in the field.

00:39:34.510 --> 00:39:37.020 align:middle line:90%


00:39:37.020 --> 00:39:40.230 align:middle line:84%
I'm getting a lot of visits
from high-ranking officials who

00:39:40.230 --> 00:39:43.290 align:middle line:84%
wish to emphasize that American
military dominance is very

00:39:43.290 --> 00:39:47.670 align:middle line:84%
important and autonomous weapons
may be part of the defense

00:39:47.670 --> 00:39:50.010 align:middle line:90%
department's plan.

00:39:50.010 --> 00:39:52.440 align:middle line:84%
That's very, very scary
because a value system

00:39:52.440 --> 00:39:54.215 align:middle line:84%
of military developments
of technology

00:39:54.215 --> 00:39:56.690 align:middle line:84%
is not the same as the value
system of the human race.

00:39:56.690 --> 00:40:00.780 align:middle line:90%


00:40:00.780 --> 00:40:03.390 align:middle line:10%
>> Out of the concerns about
the possibility that this

00:40:03.390 --> 00:40:06.660 align:middle line:10%
technology might be a
threat to human existence,

00:40:06.660 --> 00:40:09.660 align:middle line:10%
a number of the technologists
have funded the Future of Life

00:40:09.660 --> 00:40:13.180 align:middle line:10%
Institute to try to grapple
with these problems.

00:40:13.180 --> 00:40:16.020 align:middle line:84%
All of these guys are secretive,
and so it's interesting to me

00:40:16.020 --> 00:40:18.325 align:middle line:90%
to see them all altogether.

00:40:18.325 --> 00:40:21.000 align:middle line:90%


00:40:21.000 --> 00:40:24.000 align:middle line:84%
>> Everything we have is a
result of our intelligence.

00:40:24.000 --> 00:40:26.610 align:middle line:84%
It's not the result of
our big, scary teeth,

00:40:26.610 --> 00:40:29.490 align:middle line:84%
or our large claws, or
our enormous muscles.

00:40:29.490 --> 00:40:32.460 align:middle line:84%
It's because we're actually
relatively intelligent.

00:40:32.460 --> 00:40:34.980 align:middle line:84%
And among my
generation, we're all

00:40:34.980 --> 00:40:39.030 align:middle line:84%
having what we call holy cow,
or holy something else moments,

00:40:39.030 --> 00:40:42.240 align:middle line:84%
because we see that the
technology is accelerating

00:40:42.240 --> 00:40:44.270 align:middle line:90%
faster than we expected.

00:40:44.270 --> 00:40:47.310 align:middle line:10%
>> I remember sitting around
the table there with some

00:40:47.310 --> 00:40:50.070 align:middle line:10%
of the best and the
smartest minds in the world.

00:40:50.070 --> 00:40:52.470 align:middle line:84%
And what really
struck me was maybe

00:40:52.470 --> 00:40:56.100 align:middle line:84%
the human brain is not
able to fully grasp

00:40:56.100 --> 00:40:59.760 align:middle line:84%
the complexity of the world
that we're confronted with.

00:40:59.760 --> 00:41:01.560 align:middle line:84%
>> As it's currently
constructed,

00:41:01.560 --> 00:41:04.800 align:middle line:84%
the road that AI is
following heads off a cliff.

00:41:04.800 --> 00:41:06.870 align:middle line:84%
And we need to
change the direction

00:41:06.870 --> 00:41:08.460 align:middle line:84%
that we're going so
that we don't take

00:41:08.460 --> 00:41:10.741 align:middle line:90%
the human race off the cliff.

00:41:10.741 --> 00:41:12.665 align:middle line:10%
[APPLAUSE]

00:41:12.665 --> 00:41:13.630 align:middle line:90%


00:41:13.630 --> 00:41:17.780 align:middle line:84%
>> Google acquired
DeepMind several years ago.

00:41:17.780 --> 00:41:20.170 align:middle line:10%
DeepMind operates as a
semi-independent subsidiary

00:41:20.170 --> 00:41:22.670 align:middle line:10%
of Google.

00:41:22.670 --> 00:41:24.910 align:middle line:84%
The thing that makes
DeepMind unique

00:41:24.910 --> 00:41:28.330 align:middle line:84%
is that DeepMind is
absolutely focused on creating

00:41:28.330 --> 00:41:31.450 align:middle line:84%
digital super intelligence,
an AI that is vastly

00:41:31.450 --> 00:41:34.690 align:middle line:84%
smarter than any human
on Earth and ultimately

00:41:34.690 --> 00:41:37.180 align:middle line:84%
smarter than all humans
on Earth combined.

00:41:37.180 --> 00:41:40.690 align:middle line:84%
>> This is from the DeepMind
reinforcement learning system.

00:41:40.690 --> 00:41:43.480 align:middle line:84%
Basically, wakes up
like a newborn baby

00:41:43.480 --> 00:41:46.970 align:middle line:84%
and is shown the screen
of an Atari video game.

00:41:46.970 --> 00:41:50.270 align:middle line:84%
And then has to learn
to play the video game.

00:41:50.270 --> 00:41:54.740 align:middle line:84%
It knows nothing about objects,
about motion, about time.

00:41:54.740 --> 00:41:57.568 align:middle line:90%


00:41:57.568 --> 00:41:59.610 align:middle line:84%
It only knows that there's
an image on the screen

00:41:59.610 --> 00:42:02.630 align:middle line:90%
and there's a score.

00:42:02.630 --> 00:42:06.370 align:middle line:84%
So if your baby woke
up the day it was born,

00:42:06.370 --> 00:42:10.360 align:middle line:84%
and by late afternoon was
playing 40 different Atari

00:42:10.360 --> 00:42:15.280 align:middle line:84%
video games at a superhuman
level, you would be terrified.

00:42:15.280 --> 00:42:17.170 align:middle line:84%
You would say my
baby is possessed.

00:42:17.170 --> 00:42:19.160 align:middle line:90%
Send it back.

00:42:19.160 --> 00:42:23.600 align:middle line:10%
>> The DeepMind system
can win at any game.

00:42:23.600 --> 00:42:27.710 align:middle line:10%
It can already beat all
the original Atari games.

00:42:27.710 --> 00:42:28.870 align:middle line:10%
It is superhuman.

00:42:28.870 --> 00:42:30.350 align:middle line:10%
It plays the games
at super-speed,

00:42:30.350 --> 00:42:31.400 align:middle line:10%
in less than a minute.

00:42:31.400 --> 00:42:36.940 align:middle line:90%


00:42:36.940 --> 00:42:38.650 align:middle line:84%
>> DeepMind turned
to another challenge,

00:42:38.650 --> 00:42:40.630 align:middle line:84%
and the challenge
was the game of Go,

00:42:40.630 --> 00:42:43.840 align:middle line:84%
which people have generally
argued has been beyond

00:42:43.840 --> 00:42:48.220 align:middle line:84%
the power of computers to play
with the best human Go players.

00:42:48.220 --> 00:42:50.405 align:middle line:84%
First they challenged
a European Go champion.

00:42:50.405 --> 00:42:53.090 align:middle line:90%


00:42:53.090 --> 00:42:55.860 align:middle line:10%
Then they challenged
a Korean Go champion.

00:42:55.860 --> 00:42:57.800 align:middle line:10%
>> Please start again.

00:42:57.800 --> 00:43:00.860 align:middle line:84%
>> And they were able to win
both times in kind of striking

00:43:00.860 --> 00:43:02.465 align:middle line:90%
fashion.

00:43:02.465 --> 00:43:05.090 align:middle line:10%
>> You were reading articles in
the "New York Times" years ago,

00:43:05.090 --> 00:43:08.720 align:middle line:10%
talking about how Go would
take a hundred years for us

00:43:08.720 --> 00:43:10.100 align:middle line:10%
to solve.

00:43:10.100 --> 00:43:13.460 align:middle line:84%
>> People said, well, you know,
but that's still just a board.

00:43:13.460 --> 00:43:14.930 align:middle line:90%
Poker is an art.

00:43:14.930 --> 00:43:16.340 align:middle line:90%
Poker involves reading people.

00:43:16.340 --> 00:43:18.170 align:middle line:84%
Poker involves
lying and bluffing.

00:43:18.170 --> 00:43:19.460 align:middle line:90%
It's not an exact thing.

00:43:19.460 --> 00:43:21.350 align:middle line:90%
That will never be a computer.

00:43:21.350 --> 00:43:22.670 align:middle line:90%
You can't do that.

00:43:22.670 --> 00:43:24.710 align:middle line:10%
They took the best poker
players in the world,

00:43:24.710 --> 00:43:27.380 align:middle line:10%
and it took seven
days for the computer

00:43:27.380 --> 00:43:30.460 align:middle line:10%
to start demolishing the humans.

00:43:30.460 --> 00:43:32.440 align:middle line:10%
So it's the best poker
player in the world.

00:43:32.440 --> 00:43:33.982 align:middle line:10%
It's the best Go
player in the world.

00:43:33.982 --> 00:43:37.580 align:middle line:10%
And the pattern here is that
AI might take a little while

00:43:37.580 --> 00:43:40.380 align:middle line:10%
to wrap its tentacles
around a new skill,

00:43:40.380 --> 00:43:44.978 align:middle line:10%
but when it does, when it
gets it, it is unstoppable.

00:43:44.978 --> 00:43:51.950 align:middle line:90%


00:43:51.950 --> 00:43:55.140 align:middle line:84%
>> DeepMind's AI has
administrator-level access

00:43:55.140 --> 00:43:59.280 align:middle line:84%
to Google's servers to optimize
energy usage at the data

00:43:59.280 --> 00:44:00.820 align:middle line:90%
centers.

00:44:00.820 --> 00:44:04.790 align:middle line:84%
However, this could be an
unintentional Trojan horse.

00:44:04.790 --> 00:44:07.230 align:middle line:84%
DeepMind has to have complete
control of the data centers.

00:44:07.230 --> 00:44:08.910 align:middle line:84%
So with a little
software update,

00:44:08.910 --> 00:44:10.620 align:middle line:84%
that AI could take
complete control

00:44:10.620 --> 00:44:12.330 align:middle line:84%
of the whole Google
system, which

00:44:12.330 --> 00:44:13.762 align:middle line:90%
means they can do anything.

00:44:13.762 --> 00:44:15.720 align:middle line:84%
They can look at all your
data and do anything.

00:44:15.720 --> 00:44:20.660 align:middle line:90%


00:44:20.660 --> 00:44:23.210 align:middle line:84%
>> Were rapidly headed towards
digital superintelligence that

00:44:23.210 --> 00:44:24.330 align:middle line:90%
far exceeds any human.

00:44:24.330 --> 00:44:26.300 align:middle line:90%
I think it's very obvious.

00:44:26.300 --> 00:44:28.700 align:middle line:84%
>> The problem is we're
not going to suddenly hit

00:44:28.700 --> 00:44:32.760 align:middle line:84%
human-level intelligence and
say, OK, let's stop research.

00:44:32.760 --> 00:44:34.760 align:middle line:84%
It's going to go beyond
human-level intelligence

00:44:34.760 --> 00:44:36.302 align:middle line:84%
into what's called
superintelligence,

00:44:36.302 --> 00:44:39.440 align:middle line:84%
and that's anything
smarter than us.

00:44:39.440 --> 00:44:42.440 align:middle line:84%
>> AI, at the superhuman
level, if we succeed with that,

00:44:42.440 --> 00:44:46.520 align:middle line:84%
will be by far the most powerful
invention we've ever made,

00:44:46.520 --> 00:44:50.360 align:middle line:84%
and the last invention
we ever have to make.

00:44:50.360 --> 00:44:53.110 align:middle line:84%
And if we create AI
that's smarter than us,

00:44:53.110 --> 00:44:54.730 align:middle line:84%
we have to be open
to the possibility

00:44:54.730 --> 00:44:57.190 align:middle line:84%
that we might actually
lose control to them.

00:44:57.190 --> 00:45:01.040 align:middle line:90%


00:45:01.040 --> 00:45:03.540 align:middle line:84%
>> Let's say you give it some
objective, like curing cancer,

00:45:03.540 --> 00:45:07.230 align:middle line:84%
and then you discover that the
way it chooses to go about that

00:45:07.230 --> 00:45:09.750 align:middle line:84%
is actually in conflict with
a lot of other things you care

00:45:09.750 --> 00:45:12.330 align:middle line:90%
about.

00:45:12.330 --> 00:45:16.620 align:middle line:84%
>> AI doesn't have to be
evil to destroy humanity.

00:45:16.620 --> 00:45:20.640 align:middle line:84%
If AI has a goal and humanity
just happens to be in the way,

00:45:20.640 --> 00:45:22.890 align:middle line:84%
it will destroy humanity
as a matter of course

00:45:22.890 --> 00:45:24.182 align:middle line:90%
without even thinking about it.

00:45:24.182 --> 00:45:25.080 align:middle line:90%
No hard feelings.

00:45:25.080 --> 00:45:27.120 align:middle line:10%
It's just like if
we're building a road

00:45:27.120 --> 00:45:31.410 align:middle line:10%
and an anthill happens to be
in the way, we don't hate ants.

00:45:31.410 --> 00:45:33.050 align:middle line:90%
We're just building a road.

00:45:33.050 --> 00:45:34.050 align:middle line:90%
And so, goodbye anthill.

00:45:34.050 --> 00:45:37.930 align:middle line:90%


00:45:37.930 --> 00:45:40.210 align:middle line:84%
>> It's tempting to
dismiss these concerns

00:45:40.210 --> 00:45:43.870 align:middle line:84%
because it's like something that
might happen in a few decades

00:45:43.870 --> 00:45:47.560 align:middle line:84%
or a hundred years,
so why worry?

00:45:47.560 --> 00:45:50.590 align:middle line:84%
>> But if you go back
to September 11, 1933,

00:45:50.590 --> 00:45:53.260 align:middle line:84%
Ernest Rutherford, who was
the most well-known nuclear

00:45:53.260 --> 00:45:57.010 align:middle line:84%
physicist of his time, said
that the possibility of ever

00:45:57.010 --> 00:45:59.710 align:middle line:84%
extracting useful amounts of
energy from the transmutation

00:45:59.710 --> 00:46:03.100 align:middle line:84%
of atoms, as he called
it, was moonshine.

00:46:03.100 --> 00:46:05.590 align:middle line:10%
The next morning, Leo Szilard,
who was a much younger

00:46:05.590 --> 00:46:08.530 align:middle line:10%
physicist, read this
and got really annoyed,

00:46:08.530 --> 00:46:12.100 align:middle line:10%
and figured out how to make
a nuclear chain reaction just

00:46:12.100 --> 00:46:12.940 align:middle line:10%
a few months later.

00:46:12.940 --> 00:46:16.237 align:middle line:90%


00:46:16.237 --> 00:46:18.592 align:middle line:90%
[EXPLOSION]

00:46:18.592 --> 00:46:20.490 align:middle line:90%


00:46:20.490 --> 00:46:24.450 align:middle line:10%
>> We have spent more than
$2 billion on the greatest

00:46:24.450 --> 00:46:28.030 align:middle line:10%
scientific gamble in history.

00:46:28.030 --> 00:46:30.240 align:middle line:10%
>> So when people say that,
oh, this is so far off

00:46:30.240 --> 00:46:32.680 align:middle line:10%
in the future we don't
have to worry about it,

00:46:32.680 --> 00:46:36.250 align:middle line:10%
there might only be three, four
breakthroughs of that magnitude

00:46:36.250 --> 00:46:40.310 align:middle line:10%
that will get us from here
to superintelligent machines.

00:46:40.310 --> 00:46:43.860 align:middle line:84%
>> If it's going to take 20
years to figure out how to keep

00:46:43.860 --> 00:46:47.610 align:middle line:84%
AI beneficial, then
we should start today,

00:46:47.610 --> 00:46:51.487 align:middle line:84%
not at the last second when some
dudes drinking Red Bull decide

00:46:51.487 --> 00:46:53.070 align:middle line:84%
to flip the switch
and test the thing.

00:46:53.070 --> 00:46:56.720 align:middle line:90%


00:46:56.720 --> 00:46:59.030 align:middle line:10%
>> We have five years.

00:46:59.030 --> 00:47:00.590 align:middle line:84%
I think digital
superintelligence

00:47:00.590 --> 00:47:03.850 align:middle line:90%
will happen in my lifetime.

00:47:03.850 --> 00:47:06.360 align:middle line:10%
100%.

00:47:06.360 --> 00:47:09.600 align:middle line:10%
>> When this happens, it will be
surrounded by a bunch of people

00:47:09.600 --> 00:47:13.110 align:middle line:10%
who are really just excited
about the technology.

00:47:13.110 --> 00:47:15.570 align:middle line:84%
They want to see it succeed,
but they're not anticipating

00:47:15.570 --> 00:47:16.862 align:middle line:90%
that it can get out of control.

00:47:16.862 --> 00:47:25.320 align:middle line:90%


00:47:25.320 --> 00:47:26.120 align:middle line:90%
>> Oh, my god.

00:47:26.120 --> 00:47:28.620 align:middle line:90%
I trust my computer so much.

00:47:28.620 --> 00:47:30.252 align:middle line:90%
That's an amazing question.

00:47:30.252 --> 00:47:31.460 align:middle line:90%
>> I don't trust my computer.

00:47:31.460 --> 00:47:33.650 align:middle line:90%
If it's on, I take it off.

00:47:33.650 --> 00:47:35.420 align:middle line:84%
Even when it's off, I
still think it's on.

00:47:35.420 --> 00:47:35.660 align:middle line:90%
You know?

00:47:35.660 --> 00:47:36.730 align:middle line:90%
Like, you really cannot--

00:47:36.730 --> 00:47:39.500 align:middle line:84%
like the webcams, you don't know
if someone might turn it on.

00:47:39.500 --> 00:47:41.210 align:middle line:90%
You don't know.

00:47:41.210 --> 00:47:43.210 align:middle line:90%
>> I don't trust my computer.

00:47:43.210 --> 00:47:48.590 align:middle line:84%
In my phone, every time they
ask we send your information

00:47:48.590 --> 00:47:50.920 align:middle line:90%
to Apple, every time I--

00:47:50.920 --> 00:47:53.190 align:middle line:90%
so I don't trust my phone.

00:47:53.190 --> 00:47:53.990 align:middle line:90%
>> OK.

00:47:53.990 --> 00:47:56.630 align:middle line:84%
So part of it is,
yes, I do trust it

00:47:56.630 --> 00:47:59.610 align:middle line:84%
because it would be
really hard to get

00:47:59.610 --> 00:48:01.940 align:middle line:84%
through the day in the
way our world is set up

00:48:01.940 --> 00:48:03.879 align:middle line:90%
without computers.

00:48:03.879 --> 00:48:10.870 align:middle line:90%


00:48:10.870 --> 00:48:13.390 align:middle line:84%
>> Trust is such a
human experience.

00:48:13.390 --> 00:48:21.390 align:middle line:90%


00:48:21.390 --> 00:48:24.740 align:middle line:84%
I have a patient coming in
with intracranial aneurysm.

00:48:24.740 --> 00:48:30.270 align:middle line:90%


00:48:30.270 --> 00:48:32.910 align:middle line:84%
They want to look in my eyes
and know that they can trust

00:48:32.910 --> 00:48:34.980 align:middle line:90%
this person with their life.

00:48:34.980 --> 00:48:39.460 align:middle line:84%
>> I'm not horribly
concerned about anything.

00:48:39.460 --> 00:48:40.260 align:middle line:90%
>> Good.

00:48:40.260 --> 00:48:42.540 align:middle line:84%
>> Part of that is because
I have confidence in you.

00:48:42.540 --> 00:48:50.630 align:middle line:90%


00:48:50.630 --> 00:48:52.220 align:middle line:84%
>> This procedure
we're doing today,

00:48:52.220 --> 00:48:57.140 align:middle line:84%
20 years ago was
essentially impossible.

00:48:57.140 --> 00:48:59.791 align:middle line:84%
We just didn't have the
materials and the technologies.

00:48:59.791 --> 00:49:06.174 align:middle line:90%


00:49:06.174 --> 00:49:07.495 align:middle line:10%
Get on that corner!

00:49:07.495 --> 00:49:14.578 align:middle line:90%


00:49:14.578 --> 00:49:15.870 align:middle line:10%
Could it be any more difficult?

00:49:15.870 --> 00:49:16.670 align:middle line:10%
My God!

00:49:16.670 --> 00:49:22.332 align:middle line:90%


00:49:22.332 --> 00:49:26.460 align:middle line:84%
So the coil is barely
in there right now.

00:49:26.460 --> 00:49:29.890 align:middle line:84%
It's just a feather
holding it in.

00:49:29.890 --> 00:49:30.780 align:middle line:90%
It's a nervous time.

00:49:30.780 --> 00:49:36.310 align:middle line:90%


00:49:36.310 --> 00:49:39.270 align:middle line:84%
We're just in purgatory,
intellectual, humanistic

00:49:39.270 --> 00:49:40.890 align:middle line:90%
purgatory.

00:49:40.890 --> 00:49:42.990 align:middle line:84%
An AI might know
exactly what to do here.

00:49:42.990 --> 00:49:50.550 align:middle line:90%


00:49:50.550 --> 00:49:52.470 align:middle line:84%
We got the coil
into the aneurysm,

00:49:52.470 --> 00:49:54.570 align:middle line:84%
but it wasn't in
tremendously well

00:49:54.570 --> 00:49:56.430 align:middle line:90%
that I knew that it would stay.

00:49:56.430 --> 00:50:01.040 align:middle line:84%
So with a maybe 20% risk
of a very bad situation,

00:50:01.040 --> 00:50:04.380 align:middle line:84%
I elected to just
bring her back.

00:50:04.380 --> 00:50:05.970 align:middle line:84%
Because of my
relationship with her

00:50:05.970 --> 00:50:08.700 align:middle line:84%
and knowing the difficulties
of coming in and having

00:50:08.700 --> 00:50:11.230 align:middle line:84%
the procedure, I
consider things.

00:50:11.230 --> 00:50:14.190 align:middle line:84%
I should only consider
the safest possible route

00:50:14.190 --> 00:50:16.350 align:middle line:90%
to achieve success.

00:50:16.350 --> 00:50:19.680 align:middle line:84%
But I had to stand there for
10 minutes agonizing about it.

00:50:19.680 --> 00:50:21.720 align:middle line:90%
The computer feels nothing.

00:50:21.720 --> 00:50:23.910 align:middle line:84%
The computer just does
what it's supposed

00:50:23.910 --> 00:50:25.500 align:middle line:90%
to do, better and better.

00:50:25.500 --> 00:50:30.290 align:middle line:90%


00:50:30.290 --> 00:50:31.910 align:middle line:90%
I want to be AI in this case.

00:50:31.910 --> 00:50:35.870 align:middle line:90%


00:50:35.870 --> 00:50:39.000 align:middle line:90%
But can AI be compassionate?

00:50:39.000 --> 00:50:43.000 align:middle line:90%


00:50:43.000 --> 00:50:45.140 align:middle line:84%
I mean, it's everybody's
question about AI.

00:50:45.140 --> 00:50:47.800 align:middle line:90%


00:50:47.800 --> 00:50:51.846 align:middle line:84%
We are the sole
embodiment of humanity.

00:50:51.846 --> 00:50:54.340 align:middle line:84%
And it's a stretch
for us to accept

00:50:54.340 --> 00:50:57.580 align:middle line:84%
that a machine can be
compassionate and loving

00:50:57.580 --> 00:50:58.470 align:middle line:90%
in that way.

00:50:58.470 --> 00:51:05.080 align:middle line:90%


00:51:05.080 --> 00:51:07.210 align:middle line:84%
Part of me doesn't
believe in magic.

00:51:07.210 --> 00:51:09.250 align:middle line:84%
But part of me has
faith that there

00:51:09.250 --> 00:51:11.310 align:middle line:84%
is something beyond
the sum of the parts,

00:51:11.310 --> 00:51:15.670 align:middle line:84%
that there is at least a
oneness in our shared ancestry,

00:51:15.670 --> 00:51:21.590 align:middle line:84%
our shared biology, our shared
history, some connection there

00:51:21.590 --> 00:51:22.580 align:middle line:90%
beyond machine.

00:51:22.580 --> 00:51:30.280 align:middle line:90%


00:51:30.280 --> 00:51:32.437 align:middle line:84%
So then you have the
other side of that is,

00:51:32.437 --> 00:51:34.020 align:middle line:84%
does the computer
know it's conscious,

00:51:34.020 --> 00:51:37.050 align:middle line:84%
or can it be conscious,
or does it care?

00:51:37.050 --> 00:51:39.940 align:middle line:90%
Does it need to be conscious?

00:51:39.940 --> 00:51:41.080 align:middle line:90%
Does it need to be aware?

00:51:41.080 --> 00:51:53.160 align:middle line:90%


00:51:53.160 --> 00:51:56.440 align:middle line:84%
>> I do not think that a
robot could ever be conscious.

00:51:56.440 --> 00:51:58.700 align:middle line:84%
>> Unless they
programmed it that way.

00:51:58.700 --> 00:51:59.860 align:middle line:90%
>> Conscious?

00:51:59.860 --> 00:52:00.660 align:middle line:90%
No.

00:52:00.660 --> 00:52:01.460 align:middle line:90%
>> No.

00:52:01.460 --> 00:52:03.320 align:middle line:90%
>> No.

00:52:03.320 --> 00:52:05.990 align:middle line:84%
>> I mean, I think a robot could
be programmed to be conscious.

00:52:05.990 --> 00:52:09.690 align:middle line:84%
How do they program it
to do everything else?

00:52:09.690 --> 00:52:12.290 align:middle line:84%
>> That's another big part
of artificial intelligence,

00:52:12.290 --> 00:52:14.930 align:middle line:84%
is to make them conscious
and make them feel.

00:52:14.930 --> 00:52:22.390 align:middle line:90%


00:52:22.390 --> 00:52:26.320 align:middle line:10%
>> Back in 2005, we started
trying to build machines with

00:52:26.320 --> 00:52:27.644 align:middle line:10%
self-awareness.

00:52:27.644 --> 00:52:32.970 align:middle line:90%


00:52:32.970 --> 00:52:36.810 align:middle line:84%
This robot, to begin with,
didn't know what it was.

00:52:36.810 --> 00:52:39.570 align:middle line:84%
All it knew is that it needed
to do something like walk.

00:52:39.570 --> 00:52:44.380 align:middle line:90%


00:52:44.380 --> 00:52:46.060 align:middle line:10%
Through trial and
error it figured out

00:52:46.060 --> 00:52:49.710 align:middle line:10%
how to walk using
its imagination.

00:52:49.710 --> 00:52:50.710 align:middle line:10%
And then it walked away.

00:52:50.710 --> 00:52:53.980 align:middle line:90%


00:52:53.980 --> 00:52:56.320 align:middle line:84%
And then we did
something very cruel.

00:52:56.320 --> 00:52:58.480 align:middle line:84%
We chopped off a leg and
watched what happened.

00:52:58.480 --> 00:53:02.920 align:middle line:90%


00:53:02.920 --> 00:53:07.670 align:middle line:84%
At the beginning, it didn't
quite know what had happened.

00:53:07.670 --> 00:53:13.220 align:middle line:84%
But over by the period of a
day, it then began to limp.

00:53:13.220 --> 00:53:16.820 align:middle line:84%
And then, a year ago, we
were training an AI system

00:53:16.820 --> 00:53:20.220 align:middle line:90%
for a live demonstration.

00:53:20.220 --> 00:53:23.400 align:middle line:84%
We wanted to show how we wave
all these objects in front

00:53:23.400 --> 00:53:27.320 align:middle line:84%
of the camera and the AI
can recognize the objects.

00:53:27.320 --> 00:53:29.010 align:middle line:84%
And so we're
preparing this demo,

00:53:29.010 --> 00:53:31.170 align:middle line:84%
and we had on the side
screen this ability

00:53:31.170 --> 00:53:36.780 align:middle line:84%
to watch what certain
neurons were responding to.

00:53:36.780 --> 00:53:38.940 align:middle line:84%
And suddenly we noticed
that one of the neurons

00:53:38.940 --> 00:53:41.090 align:middle line:90%
was tracking faces.

00:53:41.090 --> 00:53:45.370 align:middle line:84%
It was tracking our faces
as we were moving around.

00:53:45.370 --> 00:53:47.550 align:middle line:84%
Now, the spooky
thing about this is

00:53:47.550 --> 00:53:52.500 align:middle line:84%
that we never trained the
system to recognize human faces.

00:53:52.500 --> 00:53:55.220 align:middle line:84%
And yet somehow it
learned to do that.

00:53:55.220 --> 00:53:57.870 align:middle line:90%


00:53:57.870 --> 00:53:59.710 align:middle line:84%
Even though these
robots are very simple,

00:53:59.710 --> 00:54:02.370 align:middle line:84%
we can see there's something
else going on there.

00:54:02.370 --> 00:54:05.820 align:middle line:90%
It's not just programmed.

00:54:05.820 --> 00:54:07.371 align:middle line:90%
So this is just the beginning.

00:54:07.371 --> 00:54:10.460 align:middle line:90%


00:54:10.460 --> 00:54:14.230 align:middle line:84%
>> I often think about
that beach in Kitty Hawk,

00:54:14.230 --> 00:54:17.740 align:middle line:84%
the 1903 flight by
Orville and Wilbur Wright.

00:54:17.740 --> 00:54:21.300 align:middle line:90%


00:54:21.300 --> 00:54:24.140 align:middle line:84%
It was a kind of a canvas
plane, and it's wood and iron.

00:54:24.140 --> 00:54:25.640 align:middle line:84%
And it gets off the
ground for what?

00:54:25.640 --> 00:54:29.510 align:middle line:84%
A minute and 20 seconds on
this windy day before touching

00:54:29.510 --> 00:54:30.380 align:middle line:90%
back down again.

00:54:30.380 --> 00:54:33.200 align:middle line:90%


00:54:33.200 --> 00:54:39.300 align:middle line:84%
And it was just around 65
summers or so after that moment

00:54:39.300 --> 00:54:42.335 align:middle line:84%
that you have a 747
taking off from JFK.

00:54:42.335 --> 00:54:50.190 align:middle line:90%


00:54:50.190 --> 00:54:51.940 align:middle line:84%
A major concern of
someone on the airplane

00:54:51.940 --> 00:54:55.650 align:middle line:84%
might be whether or not their
salt-free diet meal is going

00:54:55.650 --> 00:54:57.290 align:middle line:90%
to be coming to them or not.

00:54:57.290 --> 00:55:00.578 align:middle line:84%
We have a whole infrastructure
with travel agents and tower

00:55:00.578 --> 00:55:03.120 align:middle line:84%
control, and it's all casual,
and it's all part of the world.

00:55:03.120 --> 00:55:06.970 align:middle line:90%


00:55:06.970 --> 00:55:09.520 align:middle line:10%
Right now, as far as
we've come with machines

00:55:09.520 --> 00:55:12.370 align:middle line:10%
that think and solve problems,
we're at Kitty Hawk now.

00:55:12.370 --> 00:55:13.660 align:middle line:10%
We're in the wind.

00:55:13.660 --> 00:55:16.660 align:middle line:10%
We have our tattered canvas
planes up in the air.

00:55:16.660 --> 00:55:20.850 align:middle line:90%


00:55:20.850 --> 00:55:23.850 align:middle line:84%
But what happens in
65 summers or so?

00:55:23.850 --> 00:55:27.790 align:middle line:84%
We will have machines that
are beyond human control.

00:55:27.790 --> 00:55:29.050 align:middle line:90%
Should we worry about that?

00:55:29.050 --> 00:55:32.620 align:middle line:90%


00:55:32.620 --> 00:55:34.240 align:middle line:90%
I'm not sure it's going to help.

00:55:34.240 --> 00:55:40.180 align:middle line:90%


00:55:40.180 --> 00:55:44.200 align:middle line:84%
>> Nobody has any idea today
what it means for a robot to be

00:55:44.200 --> 00:55:46.360 align:middle line:90%
conscious.

00:55:46.360 --> 00:55:48.617 align:middle line:90%
There is no such thing.

00:55:48.617 --> 00:55:50.950 align:middle line:84%
There are a lot of smart
people, and I have a great deal

00:55:50.950 --> 00:55:52.990 align:middle line:90%
of respect for them.

00:55:52.990 --> 00:55:57.753 align:middle line:84%
But the truth is machines
are natural psychopaths.

00:55:57.753 --> 00:55:59.170 align:middle line:84%
>> Fear came back
into the market.

00:55:59.170 --> 00:56:01.405 align:middle line:84%
>> Went down 800, nearly
1,000 in a heartbeat.

00:56:01.405 --> 00:56:03.400 align:middle line:84%
>> I mean, it is
classic capitulation.

00:56:03.400 --> 00:56:05.900 align:middle line:84%
>> There are some people who
are proposing it with some kind

00:56:05.900 --> 00:56:07.060 align:middle line:90%
of fat finger error.

00:56:07.060 --> 00:56:09.580 align:middle line:90%
>> Take the flash crash of 2010.

00:56:09.580 --> 00:56:13.420 align:middle line:10%
In a matter of minutes, a
trillion dollars in value

00:56:13.420 --> 00:56:15.370 align:middle line:10%
was lost in the stock market.

00:56:15.370 --> 00:56:18.940 align:middle line:10%
>> The Dow dropped nearly
1,000 points in a half hour.

00:56:18.940 --> 00:56:22.490 align:middle line:90%
>> So what went wrong?

00:56:22.490 --> 00:56:25.610 align:middle line:84%
By that point in
time, more than 60%

00:56:25.610 --> 00:56:29.060 align:middle line:84%
of all the trades that took
place on the stock exchange

00:56:29.060 --> 00:56:32.725 align:middle line:84%
were actually being
initiated by computers.

00:56:32.725 --> 00:56:34.100 align:middle line:84%
>> Panic selling
on the way down,

00:56:34.100 --> 00:56:35.808 align:middle line:84%
and all of a sudden
it stopped on a dime.

00:56:35.808 --> 00:56:37.610 align:middle line:84%
It's all happening
in real time, folks.

00:56:37.610 --> 00:56:39.985 align:middle line:10%
>> The short story of what
happened in the flash crash is

00:56:39.985 --> 00:56:42.380 align:middle line:10%
that algorithms
responded to algorithms.

00:56:42.380 --> 00:56:45.230 align:middle line:10%
And it compounded upon itself
over and over and over again

00:56:45.230 --> 00:56:46.760 align:middle line:10%
in a matter of minutes.

00:56:46.760 --> 00:56:50.960 align:middle line:84%
>> At one point, the market
fell as if down a well.

00:56:50.960 --> 00:56:54.290 align:middle line:84%
>> There is no regulatory body
that can adapt quickly enough

00:56:54.290 --> 00:56:58.790 align:middle line:84%
to prevent potentially
disastrous consequences of AI

00:56:58.790 --> 00:57:00.980 align:middle line:84%
operating in our
financial systems.

00:57:00.980 --> 00:57:03.898 align:middle line:84%
They are so prime
for manipulation.

00:57:03.898 --> 00:57:06.440 align:middle line:10%
>> Let's talk about the speed
with which we are watching this

00:57:06.440 --> 00:57:07.900 align:middle line:10%
market deteriorate.

00:57:07.900 --> 00:57:11.570 align:middle line:10%
>> That's the type of AI
run amok that scares people.

00:57:11.570 --> 00:57:15.140 align:middle line:84%
>> When you give them a goal,
they will relentlessly pursue

00:57:15.140 --> 00:57:17.860 align:middle line:90%
that goal.

00:57:17.860 --> 00:57:20.260 align:middle line:84%
How many computer programs
are there like this?

00:57:20.260 --> 00:57:23.490 align:middle line:90%
Nobody knows.

00:57:23.490 --> 00:57:27.840 align:middle line:84%
>> One of the fascinating
aspects about AI in general is

00:57:27.840 --> 00:57:32.140 align:middle line:84%
that no one really
understands how it works.

00:57:32.140 --> 00:57:37.000 align:middle line:84%
Even the people who create AI
don't really fully understand.

00:57:37.000 --> 00:57:39.740 align:middle line:10%
Because it has
millions of elements,

00:57:39.740 --> 00:57:42.990 align:middle line:10%
it becomes completely
impossible for a human being

00:57:42.990 --> 00:57:45.030 align:middle line:10%
to understand what's going on.

00:57:45.030 --> 00:57:52.460 align:middle line:90%


00:57:52.460 --> 00:57:56.270 align:middle line:84%
>> Microsoft had set up this
artificial intelligence called

00:57:56.270 --> 00:57:58.250 align:middle line:84%
Tay on Twitter
which was a chatbot.

00:57:58.250 --> 00:58:01.320 align:middle line:90%


00:58:01.320 --> 00:58:04.800 align:middle line:84%
They started out in the morning,
and Tay was starting to tweet

00:58:04.800 --> 00:58:08.010 align:middle line:84%
and learning from stuff
that was being sent to him

00:58:08.010 --> 00:58:10.860 align:middle line:90%
from other Twitter people.

00:58:10.860 --> 00:58:14.880 align:middle line:10%
Because some people like troll
attacked him, within 24 hours

00:58:14.880 --> 00:58:18.570 align:middle line:10%
the Microsoft bot became
a terrible person.

00:58:18.570 --> 00:58:21.300 align:middle line:10%
They had to literally
pull Tay off the net

00:58:21.300 --> 00:58:24.720 align:middle line:10%
because he had turned
into a monster,

00:58:24.720 --> 00:58:30.470 align:middle line:10%
a misanthropic, racist, horrible
person you never want to meet.

00:58:30.470 --> 00:58:32.240 align:middle line:90%
And nobody had foreseen this.

00:58:32.240 --> 00:58:35.290 align:middle line:90%


00:58:35.290 --> 00:58:38.590 align:middle line:84%
>> The whole idea of AI is that
we are not telling it exactly

00:58:38.590 --> 00:58:42.690 align:middle line:84%
how to achieve a given
outcome or a goal.

00:58:42.690 --> 00:58:46.490 align:middle line:90%
AI develops on its own.

00:58:46.490 --> 00:58:48.970 align:middle line:84%
>> We're worried about
superintelligent AI,

00:58:48.970 --> 00:58:52.720 align:middle line:84%
the master chess player
that will outmaneuver us.

00:58:52.720 --> 00:58:54.850 align:middle line:84%
But AI won't have
to actually be that

00:58:54.850 --> 00:58:58.540 align:middle line:84%
smart to have massively
disruptive effects

00:58:58.540 --> 00:59:00.460 align:middle line:90%
on human civilization.

00:59:00.460 --> 00:59:01.930 align:middle line:84%
We've seen over
the last century,

00:59:01.930 --> 00:59:05.100 align:middle line:84%
it didn't necessarily take a
genius to knock history off

00:59:05.100 --> 00:59:06.780 align:middle line:90%
in a particular direction.

00:59:06.780 --> 00:59:10.300 align:middle line:10%
And it won't take a genius
AI to do the same thing.

00:59:10.300 --> 00:59:13.120 align:middle line:10%
>> Bogus election news stories
generated more engagement

00:59:13.120 --> 00:59:17.470 align:middle line:10%
on Facebook than
top real stories.

00:59:17.470 --> 00:59:21.010 align:middle line:84%
>> Facebook really is
the elephant in the room.

00:59:21.010 --> 00:59:23.710 align:middle line:84%
>> AI running
Facebook news feed,

00:59:23.710 --> 00:59:28.330 align:middle line:84%
the task for AI is
keeping users engaged.

00:59:28.330 --> 00:59:31.600 align:middle line:84%
But no one really
understands exactly how

00:59:31.600 --> 00:59:34.870 align:middle line:90%
this AI is achieving this goal.

00:59:34.870 --> 00:59:37.780 align:middle line:84%
>> Facebook is building an
elegant mirrored wall around

00:59:37.780 --> 00:59:41.680 align:middle line:84%
us, a mirror that we can ask
who's the fairest of them all,

00:59:41.680 --> 00:59:45.220 align:middle line:84%
and it will answer you,
you, time and again.

00:59:45.220 --> 00:59:48.100 align:middle line:84%
Slowly begin to warp
our sense of reality,

00:59:48.100 --> 00:59:51.850 align:middle line:84%
warp our sense of
politics, history,

00:59:51.850 --> 00:59:56.980 align:middle line:84%
global events, until determining
what's true and what's not true

00:59:56.980 --> 01:00:01.000 align:middle line:90%
is virtually impossible.

01:00:01.000 --> 01:00:03.960 align:middle line:84%
>> The problem is that AI
doesn't understand that.

01:00:03.960 --> 01:00:08.050 align:middle line:84%
AI just had a mission--
maximize user engagement--

01:00:08.050 --> 01:00:10.080 align:middle line:90%
and achieved that.

01:00:10.080 --> 01:00:14.330 align:middle line:84%
Nearly two billion people spend
nearly one hour on average

01:00:14.330 --> 01:00:17.970 align:middle line:84%
a day basically
interacting with AI that

01:00:17.970 --> 01:00:21.510 align:middle line:90%
is shaping their experience.

01:00:21.510 --> 01:00:24.700 align:middle line:84%
Even Facebook engineers,
they don't like fake news.

01:00:24.700 --> 01:00:26.543 align:middle line:10%
It's very bad business.

01:00:26.543 --> 01:00:27.960 align:middle line:10%
They want to get
rid of fake news.

01:00:27.960 --> 01:00:30.930 align:middle line:10%
It's just very difficult to do
because how do you recognize

01:00:30.930 --> 01:00:34.740 align:middle line:10%
news as fake if you cannot read
all of those news personally?

01:00:34.740 --> 01:00:39.590 align:middle line:10%
>> There's so much
active misinformation,

01:00:39.590 --> 01:00:43.320 align:middle line:10%
and it's packaged very well, and
it looks the same when you see

01:00:43.320 --> 01:00:47.470 align:middle line:10%
it on a Facebook page or
you turn on your television.

01:00:47.470 --> 01:00:49.380 align:middle line:84%
>> It's not terribly
sophisticated,

01:00:49.380 --> 01:00:51.660 align:middle line:90%
but it is terribly powerful.

01:00:51.660 --> 01:00:54.270 align:middle line:10%
And what it means is that
your view of the world,

01:00:54.270 --> 01:00:56.430 align:middle line:10%
which 20 years ago
was determined,

01:00:56.430 --> 01:00:59.940 align:middle line:10%
if you watched the nightly news,
by three different networks.

01:00:59.940 --> 01:01:02.460 align:middle line:84%
The three anchors who endeavored
to try to get it right

01:01:02.460 --> 01:01:04.230 align:middle line:84%
might have had a little
bias one way or the other

01:01:04.230 --> 01:01:05.688 align:middle line:84%
but, largely
speaking, we could all

01:01:05.688 --> 01:01:08.240 align:middle line:90%
agree on an objective reality.

01:01:08.240 --> 01:01:10.740 align:middle line:90%
Well, that objectivity is gone.

01:01:10.740 --> 01:01:13.570 align:middle line:84%
And Facebook has
completely annihilated it.

01:01:13.570 --> 01:01:17.093 align:middle line:90%


01:01:17.093 --> 01:01:19.260 align:middle line:84%
If most of your understanding
of how the world works

01:01:19.260 --> 01:01:21.720 align:middle line:84%
is derived from
Facebook, facilitated

01:01:21.720 --> 01:01:24.870 align:middle line:84%
by algorithmic software
that tries to show you

01:01:24.870 --> 01:01:28.810 align:middle line:84%
the news you want to see, that's
a terribly dangerous thing.

01:01:28.810 --> 01:01:33.060 align:middle line:84%
And the idea that we have
not only set that in motion,

01:01:33.060 --> 01:01:37.490 align:middle line:84%
but allowed bad faith actors
access to that information,

01:01:37.490 --> 01:01:38.970 align:middle line:90%
this is a recipe for disaster.

01:01:38.970 --> 01:01:43.263 align:middle line:90%


01:01:43.263 --> 01:01:45.430 align:middle line:84%
>> I think that there will
definitely be lots of bad

01:01:45.430 --> 01:01:48.850 align:middle line:84%
actors trying to manipulate
the world with AI.

01:01:48.850 --> 01:01:52.540 align:middle line:10%
2016 was a perfect example
of an election where

01:01:52.540 --> 01:01:55.030 align:middle line:10%
there was lots of AI
producing lots of fake news

01:01:55.030 --> 01:01:58.210 align:middle line:10%
and distributing it for
a purpose, for a result.

01:01:58.210 --> 01:01:59.800 align:middle line:90%
[APPLAUSE]

01:01:59.800 --> 01:02:02.260 align:middle line:10%
>> Ladies and gentlemen,
honorable colleagues,

01:02:02.260 --> 01:02:05.740 align:middle line:10%
it's my privilege to speak
to you today about the power

01:02:05.740 --> 01:02:09.740 align:middle line:10%
of big data and psychographics
in the electoral process.

01:02:09.740 --> 01:02:12.160 align:middle line:84%
And specifically to
talk about the work

01:02:12.160 --> 01:02:14.500 align:middle line:84%
that we contributed
to Senator Cruz's

01:02:14.500 --> 01:02:16.510 align:middle line:90%
presidential primary campaign.

01:02:16.510 --> 01:02:20.110 align:middle line:84%
>> Cambridge Analytica emerged
quietly as a company that,

01:02:20.110 --> 01:02:24.040 align:middle line:84%
according to its own hype,
has the ability to use this

01:02:24.040 --> 01:02:28.590 align:middle line:84%
tremendous amount of data
in order to effect societal

01:02:28.590 --> 01:02:30.090 align:middle line:90%
change.

01:02:30.090 --> 01:02:33.540 align:middle line:84%
In 2016, they had
three major clients.

01:02:33.540 --> 01:02:35.160 align:middle line:90%
Ted Cruz was one of them.

01:02:35.160 --> 01:02:39.540 align:middle line:84%
>> It's easy to forget that only
18 months ago Senator Cruz was

01:02:39.540 --> 01:02:42.690 align:middle line:84%
one of the less popular
candidates seeking nomination.

01:02:42.690 --> 01:02:47.400 align:middle line:10%
>> So what was not possible
maybe 10 or 15 years ago was

01:02:47.400 --> 01:02:51.030 align:middle line:10%
that you can send fake news
to exactly the people that you

01:02:51.030 --> 01:02:52.440 align:middle line:10%
want to send it to.

01:02:52.440 --> 01:02:55.560 align:middle line:84%
And then you can actually
see how he or she reacts

01:02:55.560 --> 01:02:59.520 align:middle line:84%
on Facebook, and then adjust
that information according

01:02:59.520 --> 01:03:01.750 align:middle line:90%
to the feedback that you got.

01:03:01.750 --> 01:03:04.530 align:middle line:84%
So you can start developing
kind of a real time

01:03:04.530 --> 01:03:06.780 align:middle line:90%
management of a population.

01:03:06.780 --> 01:03:09.540 align:middle line:84%
>> In this case, we've zoned
in on a group we've called

01:03:09.540 --> 01:03:10.650 align:middle line:90%
Persuasion.

01:03:10.650 --> 01:03:12.750 align:middle line:84%
These are people
who are definitely

01:03:12.750 --> 01:03:14.910 align:middle line:84%
going to vote to
caucus, but they

01:03:14.910 --> 01:03:17.310 align:middle line:84%
need moving from the
center a little bit more

01:03:17.310 --> 01:03:19.710 align:middle line:84%
towards the right in
order to support Cruz.

01:03:19.710 --> 01:03:22.050 align:middle line:90%
They need a persuasion message.

01:03:22.050 --> 01:03:23.760 align:middle line:90%
Gun rights, I've selected.

01:03:23.760 --> 01:03:25.770 align:middle line:84%
That narrows the
field slightly more.

01:03:25.770 --> 01:03:29.070 align:middle line:84%
And now we know that we need
a message on gun rights.

01:03:29.070 --> 01:03:31.110 align:middle line:84%
It needs to be a
persuasion message.

01:03:31.110 --> 01:03:33.000 align:middle line:84%
And it needs to be
nuanced according

01:03:33.000 --> 01:03:35.880 align:middle line:84%
to the certain personality
that we're interested in.

01:03:35.880 --> 01:03:39.270 align:middle line:84%
>> Through social media,
there's an infinite amount

01:03:39.270 --> 01:03:42.510 align:middle line:84%
of information that you
can gather about a person.

01:03:42.510 --> 01:03:45.810 align:middle line:84%
>> We have somewhere close
to 4,000 or 5,000 data points

01:03:45.810 --> 01:03:48.540 align:middle line:84%
on every adult in
the United States.

01:03:48.540 --> 01:03:52.030 align:middle line:84%
>> It's about targeting
the individual.

01:03:52.030 --> 01:03:54.300 align:middle line:84%
It's like a weapon
which can be used

01:03:54.300 --> 01:03:55.950 align:middle line:90%
in the totally wrong direction.

01:03:55.950 --> 01:03:58.150 align:middle line:84%
>> That's the problem
with all of this data.

01:03:58.150 --> 01:04:02.130 align:middle line:84%
It's almost as if we built the
bullet before we built the gun.

01:04:02.130 --> 01:04:06.330 align:middle line:84%
>> Ted Cruz employed our
data, our behavioral insights.

01:04:06.330 --> 01:04:09.510 align:middle line:84%
He started from a
base of less than 5%

01:04:09.510 --> 01:04:16.140 align:middle line:84%
and had a very slow and steady
but firm rise to above 35%,

01:04:16.140 --> 01:04:18.810 align:middle line:84%
making him, obviously, the
second most threatening

01:04:18.810 --> 01:04:20.430 align:middle line:90%
contender in the race.

01:04:20.430 --> 01:04:23.220 align:middle line:84%
Now, clearly, the Cruz
campaign is over now.

01:04:23.220 --> 01:04:27.120 align:middle line:84%
But what I can tell you is
that, of the two candidates left

01:04:27.120 --> 01:04:32.376 align:middle line:84%
in this election, one of them
is using these technologies.

01:04:32.376 --> 01:04:36.660 align:middle line:10%
>> I, Donald John Trump, do
solemnly swear that I will

01:04:36.660 --> 01:04:41.070 align:middle line:10%
faithfully execute the office of
President of the United States.

01:04:41.070 --> 01:04:48.450 align:middle line:90%


01:04:48.450 --> 01:04:50.280 align:middle line:84%
>> Elections are a
marginal exercise.

01:04:50.280 --> 01:04:53.250 align:middle line:10%
It doesn't take a
very sophisticated AI

01:04:53.250 --> 01:04:58.950 align:middle line:10%
in order to have a
disproportionate impact.

01:04:58.950 --> 01:05:02.520 align:middle line:10%
Before Trump, Brexit was
another supposed client.

01:05:02.520 --> 01:05:06.300 align:middle line:10%
>> Well, at 20 minutes to 5:00,
we can now say the decision

01:05:06.300 --> 01:05:11.400 align:middle line:10%
taken in 1975 by this country to
join the common market has been

01:05:11.400 --> 01:05:17.130 align:middle line:10%
reversed by this
referendum to leave the EU.

01:05:17.130 --> 01:05:18.990 align:middle line:10%
>> Cambridge
Analytica, allegedly,

01:05:18.990 --> 01:05:23.250 align:middle line:10%
uses AI to push through two of
the most ground-shaking pieces

01:05:23.250 --> 01:05:27.900 align:middle line:10%
of political change
in the last 50 years.

01:05:27.900 --> 01:05:29.130 align:middle line:90%
These are epochal events.

01:05:29.130 --> 01:05:31.920 align:middle line:84%
And if we believe the hype,
they are connected directly

01:05:31.920 --> 01:05:35.130 align:middle line:84%
to a piece of software
essentially created

01:05:35.130 --> 01:05:36.690 align:middle line:90%
by a professor at Stanford.

01:05:36.690 --> 01:05:41.330 align:middle line:90%


01:05:41.330 --> 01:05:44.750 align:middle line:84%
>> Back in 2013, I described
that what they are doing is

01:05:44.750 --> 01:05:49.340 align:middle line:84%
possible and warned against
this happening in the future.

01:05:49.340 --> 01:05:52.910 align:middle line:84%
>> At the time, Michal Kosinski
was a young Polish researcher

01:05:52.910 --> 01:05:54.920 align:middle line:84%
working at the
Psychometric Center.

01:05:54.920 --> 01:05:57.560 align:middle line:84%
So what Michal had
done was to gather

01:05:57.560 --> 01:06:03.500 align:middle line:84%
the largest ever data set of
how people behave on Facebook.

01:06:03.500 --> 01:06:07.820 align:middle line:84%
>> Psychometrics is trying to
measure psychological traits

01:06:07.820 --> 01:06:10.940 align:middle line:84%
such as personality,
intelligence, political views,

01:06:10.940 --> 01:06:12.060 align:middle line:90%
and so on.

01:06:12.060 --> 01:06:15.050 align:middle line:10%
Now, traditionally those
traits were measured

01:06:15.050 --> 01:06:17.427 align:middle line:10%
using tests and questionnaires.

01:06:17.427 --> 01:06:20.010 align:middle line:84%
>> Personality tests, the most
benign thing you could possibly

01:06:20.010 --> 01:06:20.610 align:middle line:90%
think of.

01:06:20.610 --> 01:06:22.027 align:middle line:84%
Something that
doesn't necessarily

01:06:22.027 --> 01:06:24.180 align:middle line:90%
have a lot of utility, right?

01:06:24.180 --> 01:06:27.360 align:middle line:10%
>> Our idea was that, instead
of tests and questionnaires,

01:06:27.360 --> 01:06:30.030 align:middle line:10%
we could simply look at the
digital footprints of behaviors

01:06:30.030 --> 01:06:34.830 align:middle line:10%
that we are all living behind
to understand openness,

01:06:34.830 --> 01:06:37.740 align:middle line:10%
conscientiousness, eroticism.

01:06:37.740 --> 01:06:41.170 align:middle line:84%
>> You can easily buy personal
data such as where you live,

01:06:41.170 --> 01:06:44.910 align:middle line:84%
what club memberships you've
joined, which gym you go to.

01:06:44.910 --> 01:06:48.203 align:middle line:84%
There are actually
marketplaces for personal data.

01:06:48.203 --> 01:06:50.370 align:middle line:84%
>> Turns out we can discover
an awful lot about what

01:06:50.370 --> 01:06:52.120 align:middle line:84%
you're going to do
based on a very,

01:06:52.120 --> 01:06:55.740 align:middle line:90%
very tiny set of information.

01:06:55.740 --> 01:07:00.150 align:middle line:84%
>> We are training deep learning
networks to infer intimate

01:07:00.150 --> 01:07:01.260 align:middle line:90%
traits--

01:07:01.260 --> 01:07:05.820 align:middle line:84%
people's political views,
personality, intelligence,

01:07:05.820 --> 01:07:10.020 align:middle line:84%
sexual orientation, just from
an image of someone's face.

01:07:10.020 --> 01:07:16.940 align:middle line:90%


01:07:16.940 --> 01:07:18.620 align:middle line:84%
Now think about
countries which are not

01:07:18.620 --> 01:07:20.600 align:middle line:90%
so free and open-minded.

01:07:20.600 --> 01:07:23.270 align:middle line:84%
If you can reveal
people's religious views,

01:07:23.270 --> 01:07:25.880 align:middle line:84%
or political views,
or sexual orientation

01:07:25.880 --> 01:07:28.970 align:middle line:84%
based on only profile
pictures, this

01:07:28.970 --> 01:07:33.156 align:middle line:84%
could be literally an
issue of life and death.

01:07:33.156 --> 01:07:36.940 align:middle line:90%


01:07:36.940 --> 01:07:38.340 align:middle line:90%
I think there's no going back.

01:07:38.340 --> 01:07:42.100 align:middle line:90%


01:07:42.100 --> 01:07:45.490 align:middle line:84%
>> Do you know what
the Turing Test is?

01:07:45.490 --> 01:07:48.658 align:middle line:10%
>> It's when a human
interacts with a computer.

01:07:48.658 --> 01:07:50.200 align:middle line:84%
And if the human
doesn't know they're

01:07:50.200 --> 01:07:54.540 align:middle line:84%
interacting with a computer,
the test is passed.

01:07:54.540 --> 01:07:58.150 align:middle line:84%
>> And over the next few days,
you're going to be the human

01:07:58.150 --> 01:07:59.680 align:middle line:90%
component in a Turing Test.

01:07:59.680 --> 01:08:00.520 align:middle line:90%
>> Holy shit.

01:08:00.520 --> 01:08:02.170 align:middle line:90%
>> Yeah, that's right, Caleb.

01:08:02.170 --> 01:08:04.000 align:middle line:90%
You got it.

01:08:04.000 --> 01:08:08.140 align:middle line:84%
Because if that
test is passed, you

01:08:08.140 --> 01:08:10.810 align:middle line:84%
are dead center of the
greatest scientific event

01:08:10.810 --> 01:08:12.852 align:middle line:90%
in the history of man.

01:08:12.852 --> 01:08:14.560 align:middle line:84%
>> If you've created
a conscious machine,

01:08:14.560 --> 01:08:17.520 align:middle line:90%
it's not the history of man.

01:08:17.520 --> 01:08:18.645 align:middle line:90%
That's the history of gods.

01:08:18.645 --> 01:08:26.514 align:middle line:90%


01:08:26.514 --> 01:08:28.889 align:middle line:84%
>> It's almost like technology
is a god in and of itself.

01:08:28.889 --> 01:08:33.200 align:middle line:90%


01:08:33.200 --> 01:08:35.300 align:middle line:84%
Like the weather,
we can't impact it.

01:08:35.300 --> 01:08:36.500 align:middle line:90%
We can't slow it down.

01:08:36.500 --> 01:08:39.560 align:middle line:90%
We can't stop it.

01:08:39.560 --> 01:08:43.569 align:middle line:90%
We feel powerless.

01:08:43.569 --> 01:08:45.920 align:middle line:84%
>> If we think of God
as an unlimited amount

01:08:45.920 --> 01:08:49.729 align:middle line:84%
of intelligence, the closest we
can get to that is by evolving

01:08:49.729 --> 01:08:52.670 align:middle line:84%
our own intelligence by
merging with the artificial

01:08:52.670 --> 01:08:55.300 align:middle line:90%
intelligence we're creating.

01:08:55.300 --> 01:08:59.600 align:middle line:84%
>> Today, our computers, phones,
applications give us superhuman

01:08:59.600 --> 01:09:01.593 align:middle line:90%
capability.

01:09:01.593 --> 01:09:04.010 align:middle line:84%
So as the old maxim says, if
you can't beat 'em, join 'em.

01:09:04.010 --> 01:09:06.880 align:middle line:90%


01:09:06.880 --> 01:09:09.939 align:middle line:84%
>> It's about a
human-machine partnership.

01:09:09.939 --> 01:09:13.210 align:middle line:84%
I mean, we already see how
our phones, for example,

01:09:13.210 --> 01:09:14.890 align:middle line:90%
act as memory prosthesis, right?

01:09:14.890 --> 01:09:17.109 align:middle line:84%
I don't have to remember
your phone number anymore

01:09:17.109 --> 01:09:19.120 align:middle line:90%
because it's on my phone.

01:09:19.120 --> 01:09:22.510 align:middle line:84%
It's about machines augmenting
our human abilities as opposed

01:09:22.510 --> 01:09:24.955 align:middle line:90%
to completely displacing them.

01:09:24.955 --> 01:09:27.330 align:middle line:84%
>> If you look at all the
objects that have made the leap

01:09:27.330 --> 01:09:31.970 align:middle line:84%
from analog to digital over
the last 20 years, it's a lot.

01:09:31.970 --> 01:09:35.338 align:middle line:84%
We're the last analog object
in a digital universe.

01:09:35.338 --> 01:09:36.880 align:middle line:84%
And the problem with
that, of course,

01:09:36.880 --> 01:09:40.360 align:middle line:84%
is that the data
input-output is very limited.

01:09:40.360 --> 01:09:41.160 align:middle line:90%
It's this.

01:09:41.160 --> 01:09:43.652 align:middle line:90%
It's these.

01:09:43.652 --> 01:09:44.819 align:middle line:90%
>> Our eyes are pretty good.

01:09:44.819 --> 01:09:48.960 align:middle line:84%
We're able to take in a
lot of visual information.

01:09:48.960 --> 01:09:52.590 align:middle line:84%
But our information output
is very, very, very low.

01:09:52.590 --> 01:09:56.390 align:middle line:84%
The reason this is important,
if we envision a scenario where

01:09:56.390 --> 01:09:59.540 align:middle line:84%
AI is playing a more
prominent role in societies,

01:09:59.540 --> 01:10:02.000 align:middle line:84%
we want good ways to
interact with this technology

01:10:02.000 --> 01:10:04.360 align:middle line:84%
so that it ends
up augmenting us.

01:10:04.360 --> 01:10:08.470 align:middle line:90%


01:10:08.470 --> 01:10:12.140 align:middle line:84%
>> I think it's incredibly
important that AI not be other.

01:10:12.140 --> 01:10:14.800 align:middle line:90%
It must be us.

01:10:14.800 --> 01:10:19.090 align:middle line:84%
And I could be wrong
about what I'm saying.

01:10:19.090 --> 01:10:21.010 align:middle line:84%
I'm certainly open to
ideas if anybody can

01:10:21.010 --> 01:10:24.370 align:middle line:90%
suggest a path that's better.

01:10:24.370 --> 01:10:27.230 align:middle line:84%
But I think we're really going
to have to either merge with AI

01:10:27.230 --> 01:10:28.070 align:middle line:90%
or be left behind.

01:10:28.070 --> 01:10:36.340 align:middle line:90%


01:10:36.340 --> 01:10:38.710 align:middle line:84%
>> It's hard to think
of unplugging a system

01:10:38.710 --> 01:10:41.530 align:middle line:84%
that's distributed
everywhere on the planet,

01:10:41.530 --> 01:10:45.890 align:middle line:84%
that's distributed now
across the solar system.

01:10:45.890 --> 01:10:49.630 align:middle line:90%
You can't just shut that off.

01:10:49.630 --> 01:10:50.980 align:middle line:90%
>> We've opened Pandora's Box.

01:10:50.980 --> 01:10:55.450 align:middle line:84%
We've unleashed forces that we
can't control, we can't stop.

01:10:55.450 --> 01:10:57.200 align:middle line:84%
We're in the midst of
essentially creating

01:10:57.200 --> 01:10:58.730 align:middle line:90%
a new lifeform on Earth.

01:10:58.730 --> 01:11:05.970 align:middle line:90%


01:11:05.970 --> 01:11:07.650 align:middle line:84%
>> We don't know
what happens next.

01:11:07.650 --> 01:11:10.380 align:middle line:84%
We don't know what shape
the intellect of a machine

01:11:10.380 --> 01:11:14.520 align:middle line:84%
will be when that intellect is
far beyond human capabilities.

01:11:14.520 --> 01:11:16.410 align:middle line:84%
It's just not something
that's possible.

01:11:16.410 --> 01:11:24.710 align:middle line:90%


01:11:24.710 --> 01:11:27.410 align:middle line:84%
>> The least scary future I can
think of is one where we have

01:11:27.410 --> 01:11:31.490 align:middle line:90%
at least democratized AI.

01:11:31.490 --> 01:11:34.130 align:middle line:84%
Because if one company
or small group of people

01:11:34.130 --> 01:11:37.010 align:middle line:84%
manages to develop godlike
digital superintelligence,

01:11:37.010 --> 01:11:40.260 align:middle line:90%
they could take over the world.

01:11:40.260 --> 01:11:42.150 align:middle line:84%
At least when there's
an evil dictator,

01:11:42.150 --> 01:11:44.310 align:middle line:90%
that human is going to die.

01:11:44.310 --> 01:11:47.070 align:middle line:84%
But for an AI, there
would be no death.

01:11:47.070 --> 01:11:49.400 align:middle line:90%
It would live forever.

01:11:49.400 --> 01:11:51.860 align:middle line:84%
And then you'd have
an immortal dictator

01:11:51.860 --> 01:11:53.560 align:middle line:90%
from which we can never escape.

01:11:53.560 --> 01:11:55.660 align:middle line:90%
[DRAMATIC MUSIC]

01:11:55.660 --> 01:12:50.260 align:middle line:90%


01:12:50.260 --> 01:12:56.560 align:middle line:90%
[COFFEE SHOP SOUNDS]

01:12:56.560 --> 01:13:35.260 align:middle line:84%
[NUMEROUS DIGITAL NOTIFICATION
 SOUNDS]

01:13:35.260 --> 01:13:38.610 align:middle line:90%
[MUSIC PLAYING]

01:13:38.610 --> 01:18:04.000 align:middle line:90%